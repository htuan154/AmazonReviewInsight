<!DOCTYPE html>
<html lang="vi">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Day 3 V2 - Visual Analysis & Final Report</title>
  <style>
    body{font-family:'Segoe UI',Tahoma,Arial,sans-serif;background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#222;margin:0;padding:24px}
    .container{max-width:1600px;margin:0 auto}
    .hero{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;border-radius:16px;padding:40px;margin-bottom:24px;box-shadow:0 20px 60px rgba(0,0,0,0.3)}
    .hero h1{margin:0 0 12px;font-size:2.5em;font-weight:700}
    .hero .subtitle{font-size:1.1em;opacity:0.9;margin:0}
    .grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(350px,1fr));gap:20px;margin-bottom:20px}
    .card{background:#fff;border-radius:12px;padding:24px;box-shadow:0 8px 30px rgba(0,0,0,0.12);transition:transform 0.2s}
    .card:hover{transform:translateY(-4px);box-shadow:0 12px 40px rgba(0,0,0,0.15)}
    .card-full{grid-column:1/-1}
    h2{color:#667eea;margin:0 0 16px;font-size:1.5em;border-bottom:3px solid #667eea;padding-bottom:8px}
    h3{color:#333;margin:20px 0 12px;font-size:1.2em}
    .metric-grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(200px,1fr));gap:16px;margin:16px 0}
    .metric-box{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;padding:20px;border-radius:10px;text-align:center}
    .metric-box .label{font-size:0.9em;opacity:0.9;margin-bottom:8px}
    .metric-box .value{font-size:2em;font-weight:700}
    .code-block{background:#0d1117;color:#c9d1d9;padding:16px;border-radius:8px;overflow-x:auto;margin:12px 0;border-left:4px solid #667eea}
    .code-block pre{margin:0;font-family:'Consolas','Monaco','Courier New',monospace;font-size:0.9em}
    table{width:100%;border-collapse:collapse;margin:16px 0}
    th,td{padding:12px;text-align:left;border-bottom:1px solid #e5e7eb}
    th{background:#f3f4f6;color:#667eea;font-weight:600}
    tr:hover{background:#f9fafb}
    .badge{display:inline-block;padding:6px 12px;border-radius:20px;font-size:0.85em;font-weight:600;margin:4px}
    .badge-success{background:#d1fae5;color:#065f46}
    .badge-info{background:#dbeafe;color:#1e40af}
    .badge-warning{background:#fef3c7;color:#92400e}
    .badge-danger{background:#fee2e2;color:#991b1b}
    .success-box{background:#d1fae5;border-left:4px solid #10b981;padding:12px 16px;border-radius:4px;margin:12px 0}
    .warning-box{background:#fff3cd;border-left:4px solid #ffc107;padding:12px 16px;border-radius:4px;margin:12px 0}
    .info-box{background:#dbeafe;border-left:4px solid #3b82f6;padding:12px 16px;border-radius:4px;margin:12px 0}
    .danger-box{background:#fee2e2;border-left:4px solid #ef4444;padding:12px 16px;border-radius:4px;margin:12px 0}
    .compare-grid{display:grid;grid-template-columns:1fr 1fr;gap:20px;margin:20px 0}
    .vs-badge{background:linear-gradient(135deg,#f093fb 0%,#f5576c 100%);color:#fff;padding:8px 16px;border-radius:20px;font-weight:700;display:inline-block;margin:12px 0}
    .winner-card{border:4px solid #10b981;background:#f0fdf4}
    .runner-card{border:4px solid #3b82f6;background:#eff6ff}
    .chart-container{text-align:center;margin:20px 0;padding:16px;background:#f9fafb;border-radius:8px}
    .chart-img{max-width:100%;height:auto;border-radius:8px;box-shadow:0 4px 20px rgba(0,0,0,0.1)}
    .highlight{background:#fef3c7;padding:2px 6px;border-radius:3px;font-weight:600}
  </style>
</head>
<body>
  <div class="container">
    <div class="hero">
      <h1>üìä Day 3 V2 ‚Äî Visual Analysis & Final Report</h1>
      <p class="subtitle">V7 Baseline vs V7 Auto-tune ‚Äî Complete Statistical & Visual Comparison</p>
      <div style="margin-top:16px">
        <span class="badge badge-info">Date: November 1, 2025 @ 20:00</span>
        <span class="badge badge-success">Analysis Complete ‚úì</span>
        <span class="badge badge-info">Charts Generated ‚úì</span>
      </div>
    </div>

    <!-- Critical Alert -->
    <div class="card card-full">
      <h2>üö® CRITICAL: Duplicate Review IDs Warning</h2>
      <div class="danger-box">
        <h3 style="margin-top:0;color:#991b1b">‚ö†Ô∏è 83% Duplicate Review IDs in Both Submissions!</h3>
        
        <table>
          <thead>
            <tr><th>Metric</th><th>V7 Baseline</th><th>V7 Auto-tune</th><th>Status</th></tr>
          </thead>
          <tbody>
            <tr>
              <td><strong>Total Rows</strong></td>
              <td>1,735,280</td>
              <td>1,735,280</td>
              <td><span class="badge badge-info">Same</span></td>
            </tr>
            <tr>
              <td><strong>Unique IDs</strong></td>
              <td>294,010</td>
              <td>294,010</td>
              <td><span class="badge badge-info">Same</span></td>
            </tr>
            <tr>
              <td><strong>Duplicates</strong></td>
              <td>1,441,270</td>
              <td>1,441,270</td>
              <td><span class="badge badge-danger">83.06%</span></td>
            </tr>
            <tr>
              <td><strong>Avg Repeats/ID</strong></td>
              <td>~5.9 times</td>
              <td>~5.9 times</td>
              <td><span class="badge badge-warning">High</span></td>
            </tr>
          </tbody>
        </table>

        <h4>Root Cause:</h4>
        <ul>
          <li>Test data source (<code>features_test_v4</code> on HDFS) contains duplicate review_ids</li>
          <li>Prediction pipeline processes all rows ‚Üí generates duplicate predictions</li>
          <li>Same pattern in both models confirms it's a data issue, not model issue</li>
        </ul>

        <h4>‚ö° Action Required:</h4>
        <div class="warning-box">
          <strong>Before Submission:</strong><br>
          1. <strong>Check competition rules:</strong> Does it require 1 prediction per review_id?<br>
          2. <strong>If YES (unique required):</strong> Clean duplicates using <code>keep='first'</code> ‚Üí 1.73M ‚Üí 294K rows<br>
          3. <strong>If NO (duplicates OK):</strong> Submit as-is (1.73M rows)<br><br>
          
          <strong>Recommended cleaning script:</strong><br>
          <code>df.drop_duplicates(subset='review_id', keep='first').to_csv('submission_clean.csv', index=False)</code>
        </div>
      </div>
    </div>

    <!-- Executive Summary -->
    <div class="card card-full">
      <h2>üìà Executive Summary</h2>
      
      <div class="metric-grid">
        <div class="metric-box">
          <div class="label">Winner Model</div>
          <div class="value">V7 Baseline</div>
        </div>
        <div class="metric-box">
          <div class="label">Best AUC-PR</div>
          <div class="value">0.6327</div>
        </div>
        <div class="metric-box">
          <div class="label">Predictions Each</div>
          <div class="value">1.73M</div>
        </div>
        <div class="metric-box">
          <div class="label">Feature Dimension</div>
          <div class="value">10,017</div>
        </div>
      </div>

      <div class="success-box">
        <strong>‚úÖ Analysis Complete!</strong><br>
        - Both models trained successfully with 5M training samples<br>
        - Predictions generated for all 1.73M test rows<br>
        - Statistical analysis & visualization charts created<br>
        - <strong>Recommendation: Submit V7 Baseline FIRST</strong> (higher validation AUC-PR)
      </div>
    </div>

    <!-- Detailed Comparison -->
    <div class="card card-full">
      <h2>‚öñÔ∏è V7 Baseline vs V7 Auto-tune ‚Äî Detailed Comparison</h2>
      
      <div style="text-align:center;margin:20px 0">
        <span class="vs-badge">V7 BASELINE (WINNER) vs V7 AUTO-TUNE</span>
      </div>

      <h3>üèÜ Model Performance Metrics</h3>
      <table>
        <thead>
          <tr>
            <th>Metric</th>
            <th>V7 Baseline</th>
            <th>V7 Auto-tune</th>
            <th>Difference</th>
            <th>Winner</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>Validation AUC-PR</strong></td>
            <td><span class="badge badge-success">0.6327</span></td>
            <td><span class="badge badge-warning">0.6315</span></td>
            <td>+0.0012 (+0.19%)</td>
            <td>üèÜ Baseline</td>
          </tr>
          <tr>
            <td><strong>Validation AUC-ROC</strong></td>
            <td><span class="badge badge-success">0.8392</span></td>
            <td><span class="badge badge-warning">0.8376</span></td>
            <td>+0.0016 (+0.19%)</td>
            <td>üèÜ Baseline</td>
          </tr>
          <tr>
            <td><strong>Training Time</strong></td>
            <td><span class="badge badge-success">2.5 hours</span></td>
            <td><span class="badge badge-info">2.7 hours</span></td>
            <td>-0.2h faster</td>
            <td>üèÜ Baseline</td>
          </tr>
          <tr>
            <td><strong>numLeaves</strong></td>
            <td>120</td>
            <td>100</td>
            <td>+20 (higher capacity)</td>
            <td>‚Äî</td>
          </tr>
          <tr>
            <td><strong>learningRate</strong></td>
            <td>0.03</td>
            <td>0.15</td>
            <td>-0.12 (slower learning)</td>
            <td>‚Äî</td>
          </tr>
        </tbody>
      </table>

      <h3>üìä Prediction Statistics</h3>
      <table>
        <thead>
          <tr>
            <th>Statistic</th>
            <th>V7 Baseline</th>
            <th>V7 Auto-tune</th>
            <th>Observation</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>Mean Probability</strong></td>
            <td>0.5627</td>
            <td>0.5729</td>
            <td>Auto-tune predicts higher on average (+1.8%)</td>
          </tr>
          <tr>
            <td><strong>Median</strong></td>
            <td>0.5746</td>
            <td>0.5851</td>
            <td>Auto-tune more optimistic (+1.8%)</td>
          </tr>
          <tr>
            <td><strong>Std Dev</strong></td>
            <td>0.0670</td>
            <td>0.0773</td>
            <td>Auto-tune more spread out (+15.4%)</td>
          </tr>
          <tr>
            <td><strong>Min Probability</strong></td>
            <td>0.3669</td>
            <td>0.3467</td>
            <td>Auto-tune goes lower (-5.5%)</td>
          </tr>
          <tr>
            <td><strong>Max Probability</strong></td>
            <td>0.6543</td>
            <td>0.6792</td>
            <td>Auto-tune goes higher (+3.8%)</td>
          </tr>
          <tr>
            <td><strong>Range</strong></td>
            <td>0.2874</td>
            <td>0.3325</td>
            <td>Auto-tune wider range (+15.7%)</td>
          </tr>
          <tr>
            <td><strong>Q1 (25%)</strong></td>
            <td>0.5156</td>
            <td>0.5164</td>
            <td>Very similar (+0.2%)</td>
          </tr>
          <tr>
            <td><strong>Q3 (75%)</strong></td>
            <td>0.6192</td>
            <td>0.6405</td>
            <td>Auto-tune higher upper quartile (+3.4%)</td>
          </tr>
        </tbody>
      </table>

      <div class="info-box">
        <strong>üìä Key Observations:</strong><br>
        <ul>
          <li><strong>V7 Baseline:</strong> More conservative predictions (narrower range: 0.367-0.654), lower std (0.067)</li>
          <li><strong>V7 Auto-tune:</strong> More confident predictions (wider range: 0.347-0.679), higher std (0.077)</li>
          <li><strong>Correlation:</strong> Both models agree on relative ordering (high correlation expected)</li>
          <li><strong>Calibration:</strong> Baseline slightly under-predicts, Auto-tune slightly over-predicts</li>
        </ul>
      </div>
    </div>

    <!-- Visual Analysis Charts -->
    <div class="card card-full">
      <h2>üìä Visual Analysis ‚Äî Distribution Charts</h2>
      
      <div class="warning-box">
        <strong>‚ö†Ô∏è Note on Charts:</strong> Charts were generated sequentially, so the last generated charts (V7 Auto-tune) 
        are currently saved in <code>output_final/analysis/</code>. Both models have identical filenames, 
        so only V7 Auto-tune charts are preserved. To view V7 Baseline charts, re-run the analysis script 
        with V7 Baseline path.
      </div>

      <h3>üéØ V7 Auto-tune ‚Äî Distribution Analysis</h3>
      
      <div class="chart-container">
        <h4>4-Panel Distribution Chart</h4>
        <p><em>Histogram, Pie Chart, Boxplot, and Cumulative Distribution Function</em></p>
        <img src="../output_final/analysis/submission_distribution.png" alt="V7 Auto-tune Distribution" class="chart-img">
        
        <div class="info-box" style="text-align:left;margin-top:16px">
          <strong>Chart Insights:</strong><br>
          <ul>
            <li><strong>Histogram (Top-Left):</strong> Bell-shaped distribution centered around 0.57, slight left skew</li>
            <li><strong>Pie Chart (Top-Right):</strong> 4 bins showing probability ranges (Low/Med-Low/Med-High/High)</li>
            <li><strong>Boxplot (Bottom-Left):</strong> Shows median, quartiles, and outliers - very compact distribution</li>
            <li><strong>CDF (Bottom-Right):</strong> Cumulative distribution with key percentiles marked (25%, 50%, 75%, 90%)</li>
          </ul>
        </div>
      </div>

      <div class="chart-container">
        <h4>Simple Pie Chart ‚Äî Binary Classification</h4>
        <p><em>Not Helpful (prob < 0.5) vs Helpful (prob ‚â• 0.5)</em></p>
        <img src="../output_final/analysis/submission_pie_chart.png" alt="V7 Auto-tune Pie Chart" class="chart-img">
        
        <div class="info-box" style="text-align:left;margin-top:16px">
          <strong>Classification Split:</strong><br>
          Based on threshold 0.5, majority of predictions fall into "Helpful" category (prob ‚â• 0.5).
          This indicates model predicts most reviews as helpful, which aligns with the mean probability of ~0.57.
        </div>
      </div>

      <h3>üìÅ Generated Files</h3>
      <div class="code-block">
        <pre>output_final/analysis/
‚îú‚îÄ‚îÄ submission_distribution.png  (4-panel chart)
‚îú‚îÄ‚îÄ submission_pie_chart.png     (simple pie chart)
‚îî‚îÄ‚îÄ submission_statistics.json   (numeric stats)</pre>
      </div>

      <div class="success-box">
        <strong>‚úÖ Analysis Files Ready!</strong><br>
        All statistical analysis and visualization charts have been generated and saved to 
        <code>output_final/analysis/</code> directory.
      </div>
    </div>

    <!-- Model Selection Recommendation -->
    <div class="card card-full">
      <h2>üéØ Final Recommendation ‚Äî Which Model to Submit?</h2>
      
      <div class="compare-grid">
        <!-- V7 Baseline -->
        <div class="winner-card" style="padding:20px;border-radius:12px">
          <h3 style="color:#10b981;margin-top:0">üèÜ V7 Baseline (RECOMMENDED)</h3>
          
          <div class="metric-box" style="margin:12px 0">
            <div class="label">Validation AUC-PR</div>
            <div class="value">0.6327</div>
          </div>

          <h4>‚úÖ Advantages:</h4>
          <ul>
            <li><strong>Highest validation score</strong> (AUC-PR 0.6327, AUC-ROC 0.8392)</li>
            <li><strong>Faster training</strong> (2.5 hours vs 2.7 hours)</li>
            <li><strong>Manual tuning success</strong> (proven config from experience)</li>
            <li><strong>Conservative predictions</strong> (narrower range, less risky)</li>
            <li><strong>Lower std dev</strong> (0.067 vs 0.077, more stable)</li>
          </ul>

          <h4>üìÅ File to Submit:</h4>
          <div class="code-block">
            <pre>output_final/submission_v7.csv
Size: 53.77 MB
Rows: 1,735,280 (with duplicates)
Unique IDs: 294,010</pre>
          </div>

          <div class="success-box">
            <strong>üéØ Priority: SUBMIT FIRST</strong><br>
            V7 Baseline has the highest validation AUC-PR and most stable predictions.
          </div>
        </div>

        <!-- V7 Auto-tune -->
        <div class="runner-card" style="padding:20px;border-radius:12px">
          <h3 style="color:#3b82f6;margin-top:0">ü•à V7 Auto-tune (BACKUP)</h3>
          
          <div class="metric-box" style="margin:12px 0;background:linear-gradient(135deg,#3b82f6 0%,#1e40af 100%)">
            <div class="label">Validation AUC-PR</div>
            <div class="value">0.6315</div>
          </div>

          <h4>‚úÖ Advantages:</h4>
          <ul>
            <li><strong>More robust</strong> (from 27 CV runs, 3-fold √ó 9 configs)</li>
            <li><strong>Grid-searched params</strong> (systematic optimization)</li>
            <li><strong>Best CV mean</strong> (0.6417 across folds)</li>
            <li><strong>Wider prediction range</strong> (0.347-0.679, more confident)</li>
            <li><strong>Higher std dev</strong> (0.077, more discriminative)</li>
          </ul>

          <h4>üìÅ File to Submit:</h4>
          <div class="code-block">
            <pre>output_final/submission_v7_auto.csv
Size: 53.74 MB
Rows: 1,735,280 (with duplicates)
Unique IDs: 294,010</pre>
          </div>

          <div class="info-box">
            <strong>üîÑ Backup Plan</strong><br>
            If V7 Baseline doesn't perform well on hidden test, submit V7 Auto-tune next.
            Only 0.19% worse on validation, but more robust from CV.
          </div>
        </div>
      </div>

      <h3>üìä Performance Gap Analysis</h3>
      <table>
        <thead>
          <tr><th>Aspect</th><th>Gap</th><th>Significance</th><th>Interpretation</th></tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>AUC-PR Difference</strong></td>
            <td>+0.0012</td>
            <td><span class="badge badge-warning">Very Small (0.19%)</span></td>
            <td>Statistically negligible, could reverse on different test set</td>
          </tr>
          <tr>
            <td><strong>AUC-ROC Difference</strong></td>
            <td>+0.0016</td>
            <td><span class="badge badge-warning">Very Small (0.19%)</span></td>
            <td>Both models essentially equivalent in ranking ability</td>
          </tr>
          <tr>
            <td><strong>Prediction Style</strong></td>
            <td>Conservative vs Confident</td>
            <td><span class="badge badge-info">Different</span></td>
            <td>Baseline safer, Auto-tune more decisive</td>
          </tr>
        </tbody>
      </table>

      <div class="warning-box">
        <strong>‚ö†Ô∏è Important Consideration:</strong><br><br>
        
        The 0.19% difference between models is <strong>very small</strong> and within noise margin. 
        Performance on hidden test could go either way. Strategy:<br><br>
        
        1. <strong>Submit V7 Baseline FIRST</strong> (higher validation score)<br>
        2. <strong>Monitor leaderboard score</strong><br>
        3. <strong>If not satisfied ‚Üí submit V7 Auto-tune</strong> (more robust from CV)<br>
        4. <strong>Compare results</strong> to determine which works better on hidden test distribution
      </div>
    </div>

    <!-- Action Plan -->
    <div class="card card-full">
      <h2>üöÄ Action Plan ‚Äî Next Steps</h2>
      
      <h3>Step 1: Check Competition Rules ‚ö†Ô∏è</h3>
      <div class="danger-box">
        <strong>CRITICAL: Must check before submission!</strong><br><br>
        
        <strong>Question:</strong> Does competition require 1 prediction per review_id?<br><br>
        
        <strong>Scenario A ‚Äî Unique IDs Required:</strong><br>
        <div class="code-block">
          <pre># Clean duplicates (keep first occurrence)
import pandas as pd
df = pd.read_csv('output_final/submission_v7.csv')
df_clean = df.drop_duplicates(subset='review_id', keep='first')
df_clean.to_csv('output_final/submission_v7_clean.csv', index=False)
print(f"Reduced from {len(df):,} to {len(df_clean):,} rows")</pre>
        </div>
        Expected result: 1,735,280 ‚Üí 294,010 rows (~9.2 MB file)<br><br>
        
        <strong>Scenario B ‚Äî Duplicates Allowed:</strong><br>
        Submit as-is (1.73M rows, 53.77 MB file)
      </div>

      <h3>Step 2: Submit V7 Baseline</h3>
      <div class="success-box">
        <strong>‚úÖ Primary Submission</strong><br>
        File: <code>output_final/submission_v7.csv</code> (or <code>submission_v7_clean.csv</code> if cleaned)<br>
        Expected AUC-PR: ~0.63 (based on validation)<br>
        Reasoning: Highest validation score, most stable predictions
      </div>

      <h3>Step 3: Monitor Results</h3>
      <div class="info-box">
        <ul>
          <li>Check leaderboard score after submission</li>
          <li>Compare with validation AUC-PR (0.6327)</li>
          <li>If score drops significantly ‚Üí distribution shift between validation and test</li>
          <li>If score is acceptable ‚Üí done! üéâ</li>
        </ul>
      </div>

      <h3>Step 4: Backup Submission (If Needed)</h3>
      <div class="warning-box">
        <strong>If V7 Baseline underperforms:</strong><br>
        File: <code>output_final/submission_v7_auto.csv</code> (or cleaned version)<br>
        Expected AUC-PR: ~0.63 (based on validation)<br>
        Reasoning: More robust from CV, wider prediction range, only 0.19% worse on validation
      </div>

      <h3>üìã Submission Checklist</h3>
      <table>
        <thead>
          <tr><th>Task</th><th>Status</th><th>Notes</th></tr>
        </thead>
        <tbody>
          <tr>
            <td>‚úÖ Training Complete</td>
            <td><span class="badge badge-success">Done</span></td>
            <td>V7 Baseline & V7 Auto-tune both trained</td>
          </tr>
          <tr>
            <td>‚úÖ Predictions Generated</td>
            <td><span class="badge badge-success">Done</span></td>
            <td>Both models predicted on 1.73M test samples</td>
          </tr>
          <tr>
            <td>‚úÖ Files Copied to Local</td>
            <td><span class="badge badge-success">Done</span></td>
            <td>Files in output_final/ directory</td>
          </tr>
          <tr>
            <td>‚úÖ Statistical Analysis</td>
            <td><span class="badge badge-success">Done</span></td>
            <td>Stats, charts, and reports generated</td>
          </tr>
          <tr>
            <td>‚ö†Ô∏è Check Competition Rules</td>
            <td><span class="badge badge-warning">TODO</span></td>
            <td>Unique IDs required or duplicates OK?</td>
          </tr>
          <tr>
            <td>‚ö†Ô∏è Clean Duplicates (If Required)</td>
            <td><span class="badge badge-warning">TODO</span></td>
            <td>Run drop_duplicates if needed</td>
          </tr>
          <tr>
            <td>‚ö†Ô∏è Submit to Competition</td>
            <td><span class="badge badge-warning">TODO</span></td>
            <td>Upload submission_v7.csv (or cleaned)</td>
          </tr>
        </tbody>
      </table>
    </div>

    <!-- Technical Summary -->
    <div class="card card-full">
      <h2>üî¨ Technical Summary</h2>
      
      <h3>Training Configuration</h3>
      <div class="compare-grid">
        <div>
          <h4>V7 Baseline (Manual Tuning)</h4>
          <div class="code-block">
            <pre>numLeaves: 120
learningRate: 0.03
minDataInLeaf: 50
numIterations: 1500
earlyStoppingRound: 200
featureFraction: 0.75
baggingFraction: 0.75
lambdaL1: 0.1
lambdaL2: 0.1

Training samples: 5,000,000
Training time: 2.5 hours
Validation AUC-PR: 0.6327</pre>
          </div>
        </div>
        <div>
          <h4>V7 Auto-tune (Grid Search)</h4>
          <div class="code-block">
            <pre>Best params (from 27 CV runs):
numLeaves: 100
learningRate: 0.15

Fixed params:
minDataInLeaf: 50
numIterations: 1500
earlyStoppingRound: 200
featureFraction: 0.75
baggingFraction: 0.75
lambdaL1: 0.1
lambdaL2: 0.1

Training samples: 5,000,000
Training time: 2.7 hours
CV mean AUC-PR: 0.6417
Validation AUC-PR: 0.6315</pre>
          </div>
        </div>
      </div>

      <h3>Feature Engineering</h3>
      <ul>
        <li><strong>Text Features:</strong> TF-IDF with 10,000 vocabulary (review_body + review_headline)</li>
        <li><strong>Numeric Features:</strong> 17 engineered features (vine, verified_purchase, vote_ratio, etc.)</li>
        <li><strong>Total Dimension:</strong> 10,017 features</li>
        <li><strong>NULL Handling:</strong> Imputation with 0 for TF-IDF, mean for numeric</li>
        <li><strong>Data Split:</strong> 5M train (32% of 15.6M), 1.73M test, 100% coverage</li>
      </ul>

      <h3>Model Architecture</h3>
      <ul>
        <li><strong>Algorithm:</strong> LightGBM (Gradient Boosting Decision Trees)</li>
        <li><strong>Library:</strong> SynapseML LightGBM (Spark-distributed)</li>
        <li><strong>Objective:</strong> Binary classification (is_helpful_vote)</li>
        <li><strong>Metric:</strong> AUC-PR (Area Under Precision-Recall Curve)</li>
        <li><strong>Hardware:</strong> Local Spark cluster (11GB driver + 11GB executor)</li>
      </ul>

      <h3>Data Quality Issues</h3>
      <div class="danger-box">
        <strong>üö® Duplicate Review IDs:</strong><br>
        - 83% of test data contains duplicate review_ids<br>
        - Root cause: Test data preprocessing created duplicates<br>
        - Impact: Both models predict on all rows ‚Üí duplicate predictions<br>
        - Solution: Must check competition rules and clean if required
      </div>
    </div>

    <!-- Conclusion -->
    <div class="card card-full">
      <h2>üéØ Conclusion & Recommendations</h2>
      
      <div class="success-box">
        <h3 style="margin-top:0">‚úÖ Primary Recommendation: Submit V7 Baseline</h3>
        
        <strong>Reasons:</strong>
        <ol>
          <li><strong>Highest validation AUC-PR:</strong> 0.6327 (vs 0.6315 for Auto-tune)</li>
          <li><strong>Most stable predictions:</strong> Lower std dev (0.067 vs 0.077)</li>
          <li><strong>Conservative approach:</strong> Narrower range reduces risk of extreme errors</li>
          <li><strong>Faster training:</strong> 2.5 hours (vs 2.7 hours for Auto-tune)</li>
          <li><strong>Proven configuration:</strong> Manual tuning based on V4-V6 experience</li>
        </ol>

        <strong>File to submit:</strong><br>
        <code>output_final/submission_v7.csv</code> (or cleaned version if required)<br>
        Size: 53.77 MB (1.73M rows) or ~9.2 MB (294K rows after cleaning)
      </div>

      <div class="info-box">
        <h3 style="margin-top:0">üîÑ Backup Option: V7 Auto-tune</h3>
        
        If V7 Baseline doesn't perform well on hidden test:<br>
        - Submit <code>output_final/submission_v7_auto.csv</code> (or cleaned)<br>
        - More robust from 27 CV runs (3-fold √ó 9 configs)<br>
        - Wider prediction range (0.347-0.679) may capture more diversity<br>
        - Only 0.19% worse on validation ‚Üí could perform better on different distribution
      </div>

      <div class="warning-box">
        <h3 style="margin-top:0">‚ö†Ô∏è Critical Action Required</h3>
        
        <strong>Before submitting:</strong><br>
        1. <strong>Check competition submission format requirements</strong><br>
        2. <strong>If unique IDs required ‚Üí clean duplicates first</strong><br>
        3. <strong>Verify file format matches sample submission</strong><br>
        4. <strong>Test upload with small sample if possible</strong>
      </div>

      <h3>üéì Key Learnings</h3>
      <ul>
        <li><strong>More data ‚â† always better:</strong> V7 (5M samples, AUC-PR 0.63) didn't beat V6 (1M samples, AUC-PR 0.64)</li>
        <li><strong>Manual tuning competitive:</strong> Experience-based params matched grid search results</li>
        <li><strong>Data quality matters:</strong> 83% duplicates is a critical issue that needs attention</li>
        <li><strong>Validation may not generalize:</strong> 0.19% gap could reverse on hidden test</li>
        <li><strong>Always check submission rules:</strong> Format requirements can invalidate submissions</li>
      </ul>

      <div style="text-align:center;margin:32px 0">
        <span class="badge badge-success" style="font-size:1.2em;padding:12px 24px">
          üéØ Ready to Submit! Good Luck! üöÄ
        </span>
      </div>
    </div>

    <!-- Footer -->
    <div style="text-align:center;color:#fff;margin-top:32px;padding:20px;background:rgba(0,0,0,0.2);border-radius:8px">
      <p><strong>Amazon Review Helpfulness Prediction ‚Äî HK7 Project</strong></p>
      <p>Authors: V√µ Th·ªã Di·ªÖm Thanh & L√™ ƒêƒÉng Ho√†ng Tu·∫•n</p>
      <p>Date: November 1, 2025 @ 20:00</p>
    </div>
  </div>
</body>
</html>
