<!DOCTYPE html>
<html lang="vi">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Day 3 V2 ‚Äî Consolidated Report</title>
  <style>
/* --- Consolidated wrapper styles --- */
:root { --bg1:#667eea; --bg2:#764ba2; --accent:#667eea; }
body{font-family:'Segoe UI',Tahoma,Arial,sans-serif;background:linear-gradient(135deg,var(--bg1) 0%,var(--bg2) 100%);color:#222;margin:0;padding:24px}
.container{max-width:1600px;margin:0 auto}
.header{background:linear-gradient(135deg,var(--bg1) 0%,var(--bg2) 100%);color:#fff;border-radius:16px;padding:40px;margin-bottom:24px;box-shadow:0 20px 60px rgba(0,0,0,0.3)}
.header h1{margin:0 0 8px;font-size:2.4em}
.toc{background:#fff;border-radius:12px;box-shadow:0 8px 30px rgba(0,0,0,0.12);padding:20px;margin-bottom:24px}
.toc a{display:block;padding:8px 0;color:#1f4ed8;text-decoration:none}
.toc a:hover{text-decoration:underline}
.section{background:#fff;border-radius:12px;box-shadow:0 8px 30px rgba(0,0,0,0.12);padding:24px;margin:24px 0}
.section h2{margin-top:0;color:#1f4ed8}
.backtop{margin-top:16px;display:inline-block;color:#1f4ed8}
.footer{color:#fff;opacity:.9;margin-top:24px;text-align:center}
</style>
  <style>/* From source 1 */

    body{font-family:'Segoe UI',Tahoma,Arial,sans-serif;background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#222;margin:0;padding:24px}
    .container{max-width:1400px;margin:0 auto}
    .hero{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;border-radius:16px;padding:40px;margin-bottom:24px;box-shadow:0 20px 60px rgba(0,0,0,0.3)}
    .hero h1{margin:0 0 12px;font-size:2.5em;font-weight:700}
    .hero .subtitle{font-size:1.1em;opacity:0.9;margin:0}
    .grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(350px,1fr));gap:20px;margin-bottom:20px}
    .card{background:#fff;border-radius:12px;padding:24px;box-shadow:0 8px 30px rgba(0,0,0,0.12);transition:transform 0.2s}
    .card:hover{transform:translateY(-4px);box-shadow:0 12px 40px rgba(0,0,0,0.15)}
    .card-full{grid-column:1/-1}
    h2{color:#667eea;margin:0 0 16px;font-size:1.5em;border-bottom:3px solid #667eea;padding-bottom:8px}
    h3{color:#333;margin:20px 0 12px;font-size:1.2em}
    .metric-grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(200px,1fr));gap:16px;margin:16px 0}
    .metric-box{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;padding:20px;border-radius:10px;text-align:center}
    .metric-box .label{font-size:0.9em;opacity:0.9;margin-bottom:8px}
    .metric-box .value{font-size:2em;font-weight:700}
    .code-block{background:#0d1117;color:#c9d1d9;padding:16px;border-radius:8px;overflow-x:auto;margin:12px 0;border-left:4px solid #667eea}
    .code-block pre{margin:0;font-family:'Consolas','Monaco','Courier New',monospace;font-size:0.9em}
    table{width:100%;border-collapse:collapse;margin:16px 0}
    th,td{padding:12px;text-align:left;border-bottom:1px solid #e5e7eb}
    th{background:#f3f4f6;color:#667eea;font-weight:600}
    tr:hover{background:#f9fafb}
    .badge{display:inline-block;padding:6px 12px;border-radius:20px;font-size:0.85em;font-weight:600;margin:4px}
    .badge-success{background:#d1fae5;color:#065f46}
    .badge-info{background:#dbeafe;color:#1e40af}
    .badge-warning{background:#fef3c7;color:#92400e}
    .badge-danger{background:#fee2e2;color:#991b1b}
    .success-box{background:#d1fae5;border-left:4px solid #10b981;padding:12px 16px;border-radius:4px;margin:12px 0}
    .warning-box{background:#fff3cd;border-left:4px solid #ffc107;padding:12px 16px;border-radius:4px;margin:12px 0}
    .info-box{background:#dbeafe;border-left:4px solid #3b82f6;padding:12px 16px;border-radius:4px;margin:12px 0}
    .compare-grid{display:grid;grid-template-columns:1fr 1fr;gap:20px;margin:20px 0}
  
/* From source 2 */

    body{font-family:'Segoe UI',Tahoma,Arial,sans-serif;background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#222;margin:0;padding:24px}
    .container{max-width:1400px;margin:0 auto}
    .hero{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;border-radius:16px;padding:40px;margin-bottom:24px;box-shadow:0 20px 60px rgba(0,0,0,0.3)}
    .hero h1{margin:0 0 12px;font-size:2.5em;font-weight:700}
    .hero .subtitle{font-size:1.1em;opacity:0.9;margin:0}
    .grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(350px,1fr));gap:20px;margin-bottom:20px}
    .card{background:#fff;border-radius:12px;padding:24px;box-shadow:0 8px 30px rgba(0,0,0,0.12);transition:transform 0.2s}
    .card:hover{transform:translateY(-4px);box-shadow:0 12px 40px rgba(0,0,0,0.15)}
    .card-full{grid-column:1/-1}
    h2{color:#667eea;margin:0 0 16px;font-size:1.5em;border-bottom:3px solid #667eea;padding-bottom:8px}
    h3{color:#333;margin:20px 0 12px;font-size:1.2em}
    .metric-grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(200px,1fr));gap:16px;margin:16px 0}
    .metric-box{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;padding:20px;border-radius:10px;text-align:center}
    .metric-box .label{font-size:0.9em;opacity:0.9;margin-bottom:8px}
    .metric-box .value{font-size:2em;font-weight:700}
    .code-block{background:#0d1117;color:#c9d1d9;padding:16px;border-radius:8px;overflow-x:auto;margin:12px 0;border-left:4px solid #667eea}
    .code-block pre{margin:0;font-family:'Consolas','Monaco','Courier New',monospace;font-size:0.9em}
    table{width:100%;border-collapse:collapse;margin:16px 0}
    th,td{padding:12px;text-align:left;border-bottom:1px solid #e5e7eb}
    th{background:#f3f4f6;color:#667eea;font-weight:600}
    tr:hover{background:#f9fafb}
    .badge{display:inline-block;padding:6px 12px;border-radius:20px;font-size:0.85em;font-weight:600;margin:4px}
    .badge-success{background:#d1fae5;color:#065f46}
    .badge-info{background:#dbeafe;color:#1e40af}
    .badge-warning{background:#fef3c7;color:#92400e}
    .badge-danger{background:#fee2e2;color:#991b1b}
    .success-box{background:#d1fae5;border-left:4px solid #10b981;padding:12px 16px;border-radius:4px;margin:12px 0}
    .warning-box{background:#fff3cd;border-left:4px solid #ffc107;padding:12px 16px;border-radius:4px;margin:12px 0}
    .info-box{background:#dbeafe;border-left:4px solid #3b82f6;padding:12px 16px;border-radius:4px;margin:12px 0}
    .compare-grid{display:grid;grid-template-columns:1fr 1fr;gap:20px;margin:20px 0}
    .vs-badge{background:linear-gradient(135deg,#f093fb 0%,#f5576c 100%);color:#fff;padding:8px 16px;border-radius:20px;font-weight:700;display:inline-block;margin:12px 0}
  
/* From source 3 */

    body{font-family:'Segoe UI',Tahoma,Arial,sans-serif;background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#222;margin:0;padding:24px}
    .container{max-width:1400px;margin:0 auto}
    .hero{background:linear-gradient(135deg,#1e3c72 0%,#2a5298 100%);color:#fff;border-radius:16px;padding:40px;margin-bottom:24px;box-shadow:0 20px 60px rgba(0,0,0,0.3)}
    .hero h1{margin:0 0 12px;font-size:2.5em;font-weight:700}
    .hero .subtitle{font-size:1.1em;opacity:0.9;margin:0}
    .grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(350px,1fr));gap:20px;margin-bottom:20px}
    .card{background:#fff;border-radius:12px;padding:24px;box-shadow:0 8px 30px rgba(0,0,0,0.12);transition:transform 0.2s}
    .card:hover{transform:translateY(-4px);box-shadow:0 12px 40px rgba(0,0,0,0.15)}
    .card-full{grid-column:1/-1}
    h2{color:#1f4ed8;margin:0 0 16px;font-size:1.5em;border-bottom:3px solid #1f4ed8;padding-bottom:8px}
    h3{color:#333;margin:20px 0 12px;font-size:1.2em}
    .metric-grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(200px,1fr));gap:16px;margin:16px 0}
    .metric-box{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;padding:20px;border-radius:10px;text-align:center}
    .metric-box .label{font-size:0.9em;opacity:0.9;margin-bottom:8px}
    .metric-box .value{font-size:2em;font-weight:700}
    .code-block{background:#0d1117;color:#c9d1d9;padding:16px;border-radius:8px;overflow-x:auto;margin:12px 0;border-left:4px solid #58a6ff}
    .code-block pre{margin:0;font-family:'Consolas','Monaco','Courier New',monospace;font-size:0.9em}
    table{width:100%;border-collapse:collapse;margin:16px 0}
    th,td{padding:12px;text-align:left;border-bottom:1px solid #e5e7eb}
    th{background:#f3f4f6;color:#1f4ed8;font-weight:600}
    tr:hover{background:#f9fafb}
    .badge{display:inline-block;padding:6px 12px;border-radius:20px;font-size:0.85em;font-weight:600;margin:4px}
    .badge-success{background:#d1fae5;color:#065f46}
    .badge-info{background:#dbeafe;color:#1e40af}
    .badge-warning{background:#fef3c7;color:#92400e}
    .muted{color:#6b7280;font-size:0.95em}
    .feature-list{background:#f9fafb;padding:16px;border-radius:8px;border-left:4px solid #10b981}
    .section{margin-bottom:24px}
    .note{background:#fff3cd;border-left:4px solid #ffc107;padding:12px 16px;border-radius:4px;margin:12px 0}
    .success-box{background:#d1fae5;border-left:4px solid #10b981;padding:12px 16px;border-radius:4px;margin:12px 0}
    img{max-width:100%;border-radius:8px;box-shadow:0 4px 12px rgba(0,0,0,0.1);margin:12px 0}
    .workflow{background:#f3f4f6;padding:20px;border-radius:8px;margin:16px 0}
    .workflow-step{background:#fff;padding:16px;margin:12px 0;border-radius:8px;border-left:4px solid #667eea;box-shadow:0 2px 8px rgba(0,0,0,0.05)}
    .workflow-step h4{margin:0 0 8px;color:#667eea}
  
/* From source 4 */

    body{font-family:'Segoe UI',Tahoma,Arial,sans-serif;background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#222;margin:0;padding:24px}
    .container{max-width:1600px;margin:0 auto}
    .hero{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;border-radius:16px;padding:40px;margin-bottom:24px;box-shadow:0 20px 60px rgba(0,0,0,0.3)}
    .hero h1{margin:0 0 12px;font-size:2.5em;font-weight:700}
    .hero .subtitle{font-size:1.1em;opacity:0.9;margin:0}
    .grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(350px,1fr));gap:20px;margin-bottom:20px}
    .card{background:#fff;border-radius:12px;padding:24px;box-shadow:0 8px 30px rgba(0,0,0,0.12);transition:transform 0.2s}
    .card:hover{transform:translateY(-4px);box-shadow:0 12px 40px rgba(0,0,0,0.15)}
    .card-full{grid-column:1/-1}
    h2{color:#667eea;margin:0 0 16px;font-size:1.5em;border-bottom:3px solid #667eea;padding-bottom:8px}
    h3{color:#333;margin:20px 0 12px;font-size:1.2em}
    .metric-grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(200px,1fr));gap:16px;margin:16px 0}
    .metric-box{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;padding:20px;border-radius:10px;text-align:center}
    .metric-box .label{font-size:0.9em;opacity:0.9;margin-bottom:8px}
    .metric-box .value{font-size:2em;font-weight:700}
    .code-block{background:#0d1117;color:#c9d1d9;padding:16px;border-radius:8px;overflow-x:auto;margin:12px 0;border-left:4px solid #667eea}
    .code-block pre{margin:0;font-family:'Consolas','Monaco','Courier New',monospace;font-size:0.9em}
    table{width:100%;border-collapse:collapse;margin:16px 0}
    th,td{padding:12px;text-align:left;border-bottom:1px solid #e5e7eb}
    th{background:#f3f4f6;color:#667eea;font-weight:600}
    tr:hover{background:#f9fafb}
    .badge{display:inline-block;padding:6px 12px;border-radius:20px;font-size:0.85em;font-weight:600;margin:4px}
    .badge-success{background:#d1fae5;color:#065f46}
    .badge-info{background:#dbeafe;color:#1e40af}
    .badge-warning{background:#fef3c7;color:#92400e}
    .badge-danger{background:#fee2e2;color:#991b1b}
    .success-box{background:#d1fae5;border-left:4px solid #10b981;padding:12px 16px;border-radius:4px;margin:12px 0}
    .warning-box{background:#fff3cd;border-left:4px solid #ffc107;padding:12px 16px;border-radius:4px;margin:12px 0}
    .info-box{background:#dbeafe;border-left:4px solid #3b82f6;padding:12px 16px;border-radius:4px;margin:12px 0}
    .danger-box{background:#fee2e2;border-left:4px solid #ef4444;padding:12px 16px;border-radius:4px;margin:12px 0}
    .compare-grid{display:grid;grid-template-columns:1fr 1fr;gap:20px;margin:20px 0}
    .vs-badge{background:linear-gradient(135deg,#f093fb 0%,#f5576c 100%);color:#fff;padding:8px 16px;border-radius:20px;font-weight:700;display:inline-block;margin:12px 0}
    .winner-card{border:4px solid #10b981;background:#f0fdf4}
    .runner-card{border:4px solid #3b82f6;background:#eff6ff}
    .chart-container{text-align:center;margin:20px 0;padding:16px;background:#f9fafb;border-radius:8px}
    .chart-img{max-width:100%;height:auto;border-radius:8px;box-shadow:0 4px 20px rgba(0,0,0,0.1)}
    .highlight{background:#fef3c7;padding:2px 6px;border-radius:3px;font-weight:600}
  </style>
</head>
<body id="top">
  <div class="container">
    <div class="header">
      <h1>üìö Day 3 V2 ‚Äî Consolidated Report</h1>
      <p>T·ªïng h·ª£p ƒë·∫ßy ƒë·ªß c√°c ph·∫ßn: Final, Prediction, Training (Auto-tune), v√† Visual Analysis.</p>
    </div>
    <nav class="toc">
      <h3>M·ª•c l·ª•c</h3>
      <a href="#sec1">Section 1: Day 3 V2 - Final Submission Report</a>
<a href="#sec2">Section 2: Day 3 V2 - Prediction & Comparison Report</a>
<a href="#sec3">Section 3: Day 3 V2 - Auto-Tuning Training Report (November 1, 2025)</a>
<a href="#sec4">Section 4: Day 3 V2 - Visual Analysis & Final Report</a>
    </nav>
    
    <section id="sec1" class="section">
      <h2>Section 1: Day 3 V2 - Final Submission Report</h2>
      <div class="source-note">Source file: day3_v2_final_report.html</div>
      <hr/>
      <div class="embedded-report">
        <div class="container">
    <div class="hero">
      <h1>üéâ Day 3 V2 ‚Äî Final Submission Report</h1>
      <p class="subtitle">Auto-Tuning Complete + Predictions Generated</p>
      <div style="margin-top:16px">
        <span class="badge badge-info">Date: November 1, 2025</span>
        <span class="badge badge-success">Status: Ready for Submission ‚úì</span>
        <span class="badge badge-info">Models: V7 Baseline + V7 Auto-tune</span>
      </div>
    </div>

    <!-- Executive Summary -->
    <div class="card card-full">
      <h2>üìä Executive Summary</h2>
      <div class="success-box">
        <strong>üéØ Mission Complete!</strong><br>
        ƒê√£ ho√†n th√†nh training, auto-tuning, v√† prediction cho c·∫£ 2 models. S·∫µn s√†ng 2 submission files ƒë·ªÉ so s√°nh v√† ch·ªçn best model!
      </div>
      
      <div class="metric-grid">
        <div class="metric-box">
          <div class="label">Test Predictions</div>
          <div class="value">1.73M</div>
        </div>
        <div class="metric-box">
          <div class="label">Feature Dimension</div>
          <div class="value">10,017</div>
        </div>
        <div class="metric-box">
          <div class="label">Models Trained</div>
          <div class="value">2</div>
        </div>
        <div class="metric-box">
          <div class="label">Total Runtime</div>
          <div class="value">~6 hrs</div>
        </div>
      </div>
    </div>

    <!-- Model Comparison -->
    <div class="card card-full">
      <h2>‚öñÔ∏è V7 Baseline vs V7 Auto-tune Comparison</h2>
      
      <div class="compare-grid">
        <div>
          <h3>üìå V7 Baseline</h3>
          <div class="info-box">
            <strong>Manual Hyperparameters</strong><br>
            - numLeaves: 120<br>
            - learningRate: 0.03<br>
            - minDataInLeaf: 50
          </div>
          <table>
            <thead>
              <tr><th>Metric</th><th>Value</th></tr>
            </thead>
            <tbody>
              <tr><td><strong>Validation AUC-PR</strong></td><td><span class="badge badge-success">0.6327</span></td></tr>
              <tr><td>Validation AUC-ROC</td><td>0.8392</td></tr>
              <tr><td>Precision</td><td>81.59%</td></tr>
              <tr><td>Recall</td><td>57.67%</td></tr>
              <tr><td>F1-Score</td><td>67.55%</td></tr>
              <tr><td>Training Time</td><td>~2.5 hours</td></tr>
              <tr><td>File Size</td><td>53.77 MB</td></tr>
              <tr><td>Rows</td><td>1,735,281</td></tr>
            </tbody>
          </table>
        </div>

        <div>
          <h3>üîß V7 Auto-tune</h3>
          <div class="info-box">
            <strong>Best Auto-tuned Params</strong><br>
            - numLeaves: 100<br>
            - learningRate: 0.15<br>
            - minDataInLeaf: 50
          </div>
          <table>
            <thead>
              <tr><th>Metric</th><th>Value</th></tr>
            </thead>
            <tbody>
              <tr><td><strong>Validation AUC-PR</strong></td><td><span class="badge badge-warning">0.6315</span></td></tr>
              <tr><td>Validation AUC-ROC</td><td>0.8376</td></tr>
              <tr><td>Precision</td><td>80.79%</td></tr>
              <tr><td>Recall</td><td>56.84%</td></tr>
              <tr><td>F1-Score</td><td>66.76%</td></tr>
              <tr><td>Training Time</td><td>~2.7 hours (27 runs)</td></tr>
              <tr><td>File Size</td><td>53.74 MB</td></tr>
              <tr><td>Rows</td><td>1,735,281</td></tr>
            </tbody>
          </table>
        </div>
      </div>

      <div class="warning-box" style="margin-top:20px">
        <strong>‚ö†Ô∏è Analysis:</strong><br>
        - <strong>V7 Baseline WON</strong> by a narrow margin (+0.0012 AUC-PR)<br>
        - Baseline: 0.6327 vs Auto-tune: 0.6315 (difference: 0.19%)<br>
        - Manual hyperparameters (numLeaves=120, lr=0.03) slightly better than auto-tuned (numLeaves=100, lr=0.15)<br>
        - Both models very close in performance ‚Üí recommend testing both on leaderboard
      </div>
    </div>

    <!-- Prediction Statistics -->
    <div class="card card-full">
      <h2>üìà V7 Auto-tune Prediction Statistics</h2>
      
      <table>
        <thead>
          <tr><th>Metric</th><th>Value</th><th>Description</th></tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>Test Samples</strong></td>
            <td>1,735,280</td>
            <td>Total predictions generated</td>
          </tr>
          <tr>
            <td><strong>Feature Dimension</strong></td>
            <td>10,017</td>
            <td>10K TF-IDF + 17 numeric features</td>
          </tr>
          <tr>
            <td><strong>Min Probability</strong></td>
            <td>0.3467</td>
            <td>Lowest helpful probability</td>
          </tr>
          <tr>
            <td><strong>Max Probability</strong></td>
            <td>0.6792</td>
            <td>Highest helpful probability</td>
          </tr>
          <tr>
            <td><strong>Mean Probability</strong></td>
            <td>0.5729</td>
            <td>Average helpful probability</td>
          </tr>
          <tr>
            <td><strong>Std Deviation</strong></td>
            <td>0.0773</td>
            <td>Low variance (tight distribution)</td>
          </tr>
          <tr>
            <td><strong>Probability Range</strong></td>
            <td>[0.35, 0.68]</td>
            <td>Narrow range (conservative predictions)</td>
          </tr>
        </tbody>
      </table>

      <div class="info-box" style="margin-top:16px">
        <strong>üìä Distribution Insight:</strong><br>
        - Mean = 0.573 ‚Üí balanced predictions (slightly more helpful than unhelpful)<br>
        - Narrow range [0.35, 0.68] ‚Üí model conservative (no extreme probabilities)<br>
        - Low std = 0.077 ‚Üí consistent predictions (not bimodal like V2 old project)<br>
        - No probabilities near 0 or 1 ‚Üí calibrated model (no overconfident predictions)
      </div>
    </div>

    <!-- Training Journey -->
    <div class="card card-full">
      <h2>üöÄ Complete Training Journey</h2>
      
      <table>
        <thead>
          <tr><th>Phase</th><th>Model</th><th>Time</th><th>AUC-PR</th><th>Status</th></tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>Phase 1</strong></td>
            <td>V7 Baseline (Manual params)</td>
            <td>2.5 hours</td>
            <td><strong>0.6327</strong></td>
            <td><span class="badge badge-success">‚úì Best</span></td>
          </tr>
          <tr>
            <td><strong>Phase 2</strong></td>
            <td>V7 Auto-tune (Quick preset)</td>
            <td>2.7 hours</td>
            <td>0.6315</td>
            <td><span class="badge badge-info">‚úì Close 2nd</span></td>
          </tr>
          <tr>
            <td><strong>Phase 3</strong></td>
            <td>V7 Prediction (Baseline)</td>
            <td>~10 mins</td>
            <td>‚Äî</td>
            <td><span class="badge badge-success">‚úì Done</span></td>
          </tr>
          <tr>
            <td><strong>Phase 4</strong></td>
            <td>V7_auto Prediction</td>
            <td>~10 mins</td>
            <td>‚Äî</td>
            <td><span class="badge badge-success">‚úì Done</span></td>
          </tr>
        </tbody>
      </table>

      <h3>Timeline Breakdown</h3>
      <ul>
        <li><strong>14:00-16:30:</strong> V7 Baseline training (numLeaves=120, lr=0.03)</li>
        <li><strong>16:30-16:40:</strong> V7 Baseline prediction ‚Üí submission_v7.csv</li>
        <li><strong>16:04-18:50:</strong> V7 Auto-tune training (9 combos √ó 3-fold CV)</li>
        <li><strong>19:12-19:22:</strong> V7_auto prediction ‚Üí submission_v7_auto.csv</li>
      </ul>
    </div>

    <!-- Auto-tuning Results -->
    <div class="card card-full">
      <h2>üîß Auto-tuning Detailed Results</h2>
      
      <h3>Grid Search Configuration</h3>
      <div class="code-block">
        <pre># Quick preset - 9 combinations
numLeaves: [50, 100, 150]
learningRate: [0.05, 0.10, 0.15]
minDataInLeaf: [50]  # fixed

Total runs: 9 combos √ó 3 folds = 27 training runs
Total time: 2.7 hours (6 mins/run average)</pre>
      </div>

      <h3>Top 5 Configurations (by mean AUC-PR)</h3>
      <table>
        <thead>
          <tr><th>Rank</th><th>numLeaves</th><th>learningRate</th><th>Mean AUC-PR</th><th>Std</th></tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>ü•á 1st</strong></td>
            <td>100</td>
            <td>0.15</td>
            <td><strong>0.6315</strong></td>
            <td>0.0012</td>
          </tr>
          <tr>
            <td>ü•à 2nd</td>
            <td>150</td>
            <td>0.15</td>
            <td>0.6312</td>
            <td>0.0011</td>
          </tr>
          <tr>
            <td>ü•â 3rd</td>
            <td>100</td>
            <td>0.10</td>
            <td>0.6309</td>
            <td>0.0013</td>
          </tr>
          <tr>
            <td>4th</td>
            <td>50</td>
            <td>0.15</td>
            <td>0.6305</td>
            <td>0.0014</td>
          </tr>
          <tr>
            <td>5th</td>
            <td>150</td>
            <td>0.10</td>
            <td>0.6301</td>
            <td>0.0012</td>
          </tr>
        </tbody>
      </table>

      <div class="info-box" style="margin-top:16px">
        <strong>üéØ Key Finding:</strong><br>
        - <strong>learningRate=0.15</strong> dominates top positions (1st, 2nd, 4th)<br>
        - <strong>numLeaves=100-150</strong> optimal range (not too simple, not too complex)<br>
        - Low std (0.0011-0.0014) ‚Üí stable across folds ‚Üí generalizes well<br>
        - BUT manual params (numLeaves=120, lr=0.03) STILL better by 0.19%!
      </div>
    </div>

    <!-- Prediction Command Log -->
    <div class="card card-full">
      <h2>üíª Prediction Commands</h2>
      
      <h3>V7 Auto-tune Prediction</h3>
      <div class="code-block">
        <pre>$env:PYSPARK_PYTHON = "C:\Users\LeDangHoangTuan\AppData\Local\Programs\Python\Python311\python.exe"
$env:PYSPARK_DRIVER_PYTHON = "C:\Users\LeDangHoangTuan\AppData\Local\Programs\Python\Python311\python.exe"

& "$env:SPARK_HOME\bin\spark-submit.cmd" `
  --master local[*] `
  --deploy-mode client `
  --packages com.microsoft.azure:synapseml-lightgbm_2.12:1.0.7 `
  --driver-memory 11g `
  --executor-memory 11g `
  --conf spark.driver.maxResultSize=4g `
  --conf spark.sql.shuffle.partitions=64 `
  --conf spark.sql.adaptive.enabled=true `
  "D:\HK7\AmazonReviewInsight\code_v2\models\predict_pipeline_v2.py" `
  --model_path "hdfs://localhost:9000/output_v2/models/lightgbm_v7_auto" `
  --test "hdfs://localhost:9000/output_v2/features_test_v4" `
  --out "hdfs://localhost:9000/output_v2/predictions_v7_auto" `
  --debug_samples 100</pre>
      </div>

      <h3>Parameters Explanation</h3>
      <table>
        <thead>
          <tr><th>Parameter</th><th>Value</th><th>Purpose</th></tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>model_path</strong></td>
            <td>lightgbm_v7_auto</td>
            <td>Best auto-tuned model (numLeaves=100, lr=0.15)</td>
          </tr>
          <tr>
            <td><strong>test</strong></td>
            <td>features_test_v4</td>
            <td>Test features (1.73M √ó 10,017 dims)</td>
          </tr>
          <tr>
            <td><strong>out</strong></td>
            <td>predictions_v7_auto</td>
            <td>Output directory for submission CSV</td>
          </tr>
          <tr>
            <td><strong>debug_samples</strong></td>
            <td>100</td>
            <td>Save first 100 predictions for inspection</td>
          </tr>
          <tr>
            <td><strong>driver-memory</strong></td>
            <td>11g</td>
            <td>Driver memory for large dataset handling</td>
          </tr>
          <tr>
            <td><strong>shuffle.partitions</strong></td>
            <td>64</td>
            <td>Parallelism for join/shuffle operations</td>
          </tr>
        </tbody>
      </table>
    </div>

    <!-- Output Files -->
    <div class="card card-full">
      <h2>üìÅ Generated Output Files</h2>
      
      <table>
        <thead>
          <tr><th>File</th><th>Location</th><th>Size</th><th>Description</th></tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>submission_v7.csv</strong></td>
            <td>output_final/</td>
            <td>53.77 MB</td>
            <td>V7 Baseline predictions (AUC-PR 0.6327) ‚úÖ</td>
          </tr>
          <tr>
            <td><strong>submission_v7_auto.csv</strong></td>
            <td>output_final/</td>
            <td>53.74 MB</td>
            <td>V7 Auto-tune predictions (AUC-PR 0.6315)</td>
          </tr>
          <tr>
            <td><strong>debug_v7_auto.csv</strong></td>
            <td>output_final/</td>
            <td>3.1 KB</td>
            <td>First 100 predictions for debugging</td>
          </tr>
          <tr>
            <td><strong>stats.json</strong></td>
            <td>tmp/predict_logs/</td>
            <td>~1 KB</td>
            <td>Prediction statistics (min, max, mean, std)</td>
          </tr>
          <tr>
            <td><strong>params.txt</strong></td>
            <td>tmp/predict_logs/</td>
            <td>~1 KB</td>
            <td>Prediction parameters log</td>
          </tr>
          <tr>
            <td><strong>day3_v2_training_report.html</strong></td>
            <td>docs_v2/</td>
            <td>~180 KB</td>
            <td>Training report (algorithms + hyperparameters)</td>
          </tr>
          <tr>
            <td><strong>day3_v2_final_report.html</strong></td>
            <td>docs_v2/</td>
            <td>~45 KB</td>
            <td>This final summary report</td>
          </tr>
        </tbody>
      </table>
    </div>

    <!-- Recommendations -->
    <div class="card card-full">
      <h2>üéØ Recommendations & Next Steps</h2>
      
      <div class="success-box">
        <strong>‚úÖ Recommendation: Submit V7 Baseline FIRST</strong><br><br>
        
        <strong>Reasons:</strong><br>
        1. <strong>Higher validation AUC-PR:</strong> 0.6327 vs 0.6315 (+0.19%)<br>
        2. <strong>Better all-around metrics:</strong> Precision 81.59%, Recall 57.67%, F1 67.55%<br>
        3. <strong>Conservative but effective:</strong> Manual hyperparameters well-tuned<br>
        4. <strong>Proven stable:</strong> Single training run, no overfitting signs<br><br>

        <strong>Backup Plan:</strong><br>
        - If V7 Baseline doesn't perform well on leaderboard ‚Üí submit V7 Auto-tune<br>
        - Auto-tune model only 0.19% worse ‚Üí very close alternative<br>
        - Both models have similar prediction distributions ‚Üí minimal risk
      </div>

      <h3>üìä Decision Matrix</h3>
      <table>
        <thead>
          <tr><th>Criterion</th><th>V7 Baseline</th><th>V7 Auto-tune</th><th>Winner</th></tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>Validation AUC-PR</strong></td>
            <td>0.6327</td>
            <td>0.6315</td>
            <td><span class="badge badge-success">Baseline</span></td>
          </tr>
          <tr>
            <td><strong>Training Time</strong></td>
            <td>2.5 hours</td>
            <td>2.7 hours</td>
            <td><span class="badge badge-success">Baseline</span></td>
          </tr>
          <tr>
            <td><strong>Robustness (CV)</strong></td>
            <td>Single run</td>
            <td>3-fold CV</td>
            <td><span class="badge badge-warning">Auto-tune</span></td>
          </tr>
          <tr>
            <td><strong>Hyperparameters</strong></td>
            <td>Manual tuned</td>
            <td>Grid searched</td>
            <td><span class="badge badge-info">Tie</span></td>
          </tr>
          <tr>
            <td><strong>Simplicity</strong></td>
            <td>Simple</td>
            <td>Complex</td>
            <td><span class="badge badge-success">Baseline</span></td>
          </tr>
        </tbody>
      </table>

      <div class="info-box" style="margin-top:20px">
        <strong>üí° Next Actions:</strong><br>
        1. ‚úÖ Copy <code>submission_v7.csv</code> to submission folder<br>
        2. ‚úÖ Upload to competition platform<br>
        3. ‚è≥ Wait for leaderboard score<br>
        4. üìä If score < expected ‚Üí try <code>submission_v7_auto.csv</code><br>
        5. üìù Document final leaderboard results
      </div>
    </div>

    <!-- Lessons Learned -->
    <div class="card card-full">
      <h2>üí° Lessons Learned</h2>
      
      <h3>‚úÖ What Went Well</h3>
      <ul>
        <li><strong>Hyperparameter Tuning:</strong> Auto-tune workflow worked perfectly (9 combos √ó 3-fold CV in 2.7 hrs)</li>
        <li><strong>Prediction Pipeline:</strong> Clean, robust predict_pipeline_v2.py with comprehensive validation</li>
        <li><strong>Feature Engineering:</strong> 10,017 features (10K TF-IDF + 17 numeric) working well</li>
        <li><strong>Memory Management:</strong> 11GB driver memory sufficient for 1.73M predictions</li>
        <li><strong>Documentation:</strong> Comprehensive HTML reports with all details</li>
      </ul>

      <h3>‚ö†Ô∏è Surprises & Insights</h3>
      <ul>
        <li><strong>Manual > Auto:</strong> Manual hyperparameters outperformed auto-tuned by 0.19% (unexpected!)</li>
        <li><strong>Learning Rate:</strong> Higher lr=0.15 dominated auto-tune, but manual lr=0.03 still best</li>
        <li><strong>numLeaves:</strong> Sweet spot 100-120 (not too complex, not too simple)</li>
        <li><strong>Narrow Probability Range:</strong> [0.35, 0.68] ‚Üí model conservative (good calibration)</li>
        <li><strong>Low Variance:</strong> std=0.077 ‚Üí consistent predictions across test set</li>
      </ul>

      <h3>üîß What Could Be Improved</h3>
      <ul>
        <li><strong>Thorough Auto-tune:</strong> Try "thorough" preset (27 combos) instead of "quick" (9 combos)</li>
        <li><strong>Learning Rate Range:</strong> Expand grid to include 0.01-0.05 (current best=0.03)</li>
        <li><strong>Ensemble:</strong> Combine V7 + V7_auto predictions (weighted average)</li>
        <li><strong>Post-processing:</strong> Calibration (Platt scaling) to improve probability estimates</li>
        <li><strong>Feature Selection:</strong> Investigate which features contribute most (SHAP values)</li>
      </ul>
    </div>

    <!-- Technical Summary -->
    <div class="card card-full">
      <h2>üî¨ Technical Summary</h2>
      
      <div style="display:grid;grid-template-columns:1fr 1fr;gap:20px">
        <div>
          <h3>Training Configuration</h3>
          <ul>
            <li><strong>Dataset:</strong> 5M samples (32% of 15.6M)</li>
            <li><strong>Features:</strong> 10,017 dims (10K TF-IDF + 17 numeric)</li>
            <li><strong>Class Ratio:</strong> 1:3 (1.1M helpful vs 3.4M unhelpful)</li>
            <li><strong>Class Weight:</strong> 3.054 for positive class</li>
            <li><strong>Max Depth:</strong> -1 (unlimited, leaf-wise growth)</li>
            <li><strong>Early Stopping:</strong> 200 rounds</li>
            <li><strong>Max Iterations:</strong> 1500 trees</li>
          </ul>
        </div>

        <div>
          <h3>Prediction Configuration</h3>
          <ul>
            <li><strong>Test Samples:</strong> 1,735,280 rows</li>
            <li><strong>Model Type:</strong> LightGBMClassificationModel</li>
            <li><strong>Probability Extraction:</strong> vector_to_array + getItem(1)</li>
            <li><strong>Validation:</strong> Range check [0, 1], no NULLs</li>
            <li><strong>Output Format:</strong> CSV (review_id, probability_helpful)</li>
            <li><strong>File Size:</strong> ~54 MB (1.73M rows)</li>
          </ul>
        </div>
      </div>

      <h3>Infrastructure</h3>
      <ul>
        <li><strong>Spark Version:</strong> 3.4.1 with SynapseML 1.0.7</li>
        <li><strong>Python:</strong> 3.11</li>
        <li><strong>Memory:</strong> 11GB driver + 11GB executor</li>
        <li><strong>Parallelism:</strong> local[*] (all CPU cores)</li>
        <li><strong>Storage:</strong> HDFS (localhost:9000) + local output_final/</li>
        <li><strong>Shuffle Partitions:</strong> 64 (optimized for dataset size)</li>
      </ul>
    </div>

    <!-- Footer -->
    <div style="text-align:center;padding:24px;color:#fff;font-size:0.9em">
      <p><strong>Day 3 V2 Final Report</strong> ‚Äî Generated on November 1, 2025 at 19:30</p>
      <p>Complete journey: Training ‚Üí Auto-tuning ‚Üí Prediction ‚Üí Submission Ready!</p>
      <p style="margin-top:8px">
        <span class="badge badge-success">V7 Baseline: AUC-PR 0.6327 ‚úÖ</span>
        <span class="badge badge-info">V7 Auto-tune: AUC-PR 0.6315</span>
        <span class="badge badge-success">Both submissions ready!</span>
      </p>
      <div style="margin-top:16px;padding:16px;background:rgba(255,255,255,0.1);border-radius:8px">
        <strong>üéâ READY FOR SUBMISSION!</strong><br>
        Recommend: <code>submission_v7.csv</code> (V7 Baseline) first<br>
        Backup: <code>submission_v7_auto.csv</code> (V7 Auto-tune) if needed
      </div>
    </div>
  </div>
      </div>
      <a class="backtop" href="#top">‚Üë Back to top</a>
    </section>
    
    <section id="sec2" class="section">
      <h2>Section 2: Day 3 V2 - Prediction & Comparison Report</h2>
      <div class="source-note">Source file: day3_v2_prediction_report.html</div>
      <hr/>
      <div class="embedded-report">
        <div class="container">
    <div class="hero">
      <h1>üéØ Day 3 V2 ‚Äî Prediction & Comparison Report</h1>
      <p class="subtitle">V7 Baseline vs V7 Auto-tune ‚Äî Complete Analysis</p>
      <div style="margin-top:16px">
        <span class="badge badge-info">Date: November 1, 2025 @ 19:30</span>
        <span class="badge badge-success">Both Models Ready ‚úì</span>
        <span class="badge badge-info">1.73M Predictions Each</span>
      </div>
    </div>

    <!-- Critical Issue Alert -->
    <div class="card card-full">
      <h2>‚ö†Ô∏è DUPLICATE REVIEW_IDs DETECTED</h2>
      <div class="warning-box">
        <strong>üîç Data Quality Issue Found:</strong><br><br>
        
        C·∫£ 2 submission files ƒë·ªÅu c√≥ <strong>83% duplicate review_ids</strong>!<br><br>
        
        <table>
          <thead>
            <tr><th>File</th><th>Total Rows</th><th>Unique IDs</th><th>Duplicates</th><th>Duplicate %</th></tr>
          </thead>
          <tbody>
            <tr>
              <td><strong>submission_v7.csv</strong></td>
              <td>1,735,281</td>
              <td>294,010</td>
              <td>1,441,270</td>
              <td><span class="badge badge-danger">83.06%</span></td>
            </tr>
            <tr>
              <td><strong>submission_v7_auto.csv</strong></td>
              <td>1,735,281</td>
              <td>294,010</td>
              <td>1,441,270</td>
              <td><span class="badge badge-danger">83.06%</span></td>
            </tr>
          </tbody>
        </table>

        <strong>üìä Root Cause Analysis:</strong><br>
        - Test data c√≥ duplicates trong source parquet (features_test_v4)<br>
        - M·ªói review_id xu·∫•t hi·ªán trung b√¨nh <strong>~5.9 l·∫ßn</strong> (1,735,281 / 294,010)<br>
        - Prediction pipeline x·ª≠ l√Ω t·∫•t c·∫£ rows ‚Üí duplicate predictions<br><br>

        <strong>üí° Recommendation:</strong><br>
        N·∫øu competition y√™u c·∫ßu 1 prediction per review_id ‚Üí c·∫ßn clean duplicates (keep first/last/average)<br>
        N·∫øu submission format ch·∫•p nh·∫≠n duplicates ‚Üí c√≥ th·ªÉ submit as-is
      </div>
    </div>

    <!-- Executive Summary -->
    <div class="card card-full">
      <h2>üìä Executive Summary</h2>
      
      <div class="metric-grid">
        <div class="metric-box">
          <div class="label">Total Predictions</div>
          <div class="value">1.73M</div>
        </div>
        <div class="metric-box">
          <div class="label">Unique Review IDs</div>
          <div class="value">294K</div>
        </div>
        <div class="metric-box">
          <div class="label">Feature Dimension</div>
          <div class="value">10,017</div>
        </div>
        <div class="metric-box">
          <div class="label">Models Compared</div>
          <div class="value">2</div>
        </div>
      </div>

      <div class="success-box">
        <strong>‚úÖ Both Models Completed Successfully!</strong><br>
        - V7 Baseline: Manual hyperparameters (numLeaves=120, lr=0.03)<br>
        - V7 Auto-tune: Grid-searched params (numLeaves=100, lr=0.15)<br>
        - Both ready in <code>output_final/</code> folder
      </div>
    </div>

    <!-- Model Comparison -->
    <div class="card card-full">
      <h2>‚öñÔ∏è V7 Baseline vs V7 Auto-tune Comparison</h2>
      <div style="text-align:center;margin:20px 0">
        <span class="vs-badge">V7 BASELINE vs V7 AUTO-TUNE</span>
      </div>

      <div class="compare-grid">
        <!-- V7 Baseline -->
        <div style="border:3px solid #10b981;border-radius:12px;padding:20px;background:#f0fdf4">
          <h3 style="color:#10b981;margin-top:0">üèÜ V7 Baseline (WINNER)</h3>
          
          <h4>Hyperparameters (Manual)</h4>
          <div class="code-block">
            <pre>numLeaves: 120
learningRate: 0.03
minDataInLeaf: 50
numIterations: 1500
earlyStoppingRound: 200
featureFraction: 0.75
baggingFraction: 0.75
lambdaL1: 0.1
lambdaL2: 0.1</pre>
          </div>

          <h4>Validation Metrics</h4>
          <table>
            <tr><td><strong>AUC-PR</strong></td><td><span class="badge badge-success">0.6327</span></td></tr>
            <tr><td>AUC-ROC</td><td>0.8392</td></tr>
            <tr><td>Precision</td><td>81.59%</td></tr>
            <tr><td>Recall</td><td>57.67%</td></tr>
            <tr><td>F1-Score</td><td>67.55%</td></tr>
          </table>

          <h4>Training Stats</h4>
          <ul>
            <li>Time: 2.5 hours</li>
            <li>Samples: 5M (32% of 15.6M)</li>
            <li>CV: Single run (no cross-validation)</li>
          </ul>

          <h4>Prediction Stats (KH√îNG CLEAN)</h4>
          <table>
            <tr><td>Total Rows</td><td>1,735,281</td></tr>
            <tr><td>Unique IDs</td><td>294,010</td></tr>
            <tr><td>Duplicates</td><td>1,441,270 (83%)</td></tr>
            <tr><td>File Size</td><td>53.77 MB</td></tr>
          </table>
        </div>

        <!-- V7 Auto-tune -->
        <div style="border:3px solid #3b82f6;border-radius:12px;padding:20px;background:#eff6ff">
          <h3 style="color:#3b82f6;margin-top:0">üîß V7 Auto-tune (Close 2nd)</h3>
          
          <h4>Hyperparameters (Auto-tuned)</h4>
          <div class="code-block">
            <pre>numLeaves: 100
learningRate: 0.15
minDataInLeaf: 50
numIterations: 1500
earlyStoppingRound: 200
featureFraction: 0.75
baggingFraction: 0.75
lambdaL1: 0.1
lambdaL2: 0.1</pre>
          </div>

          <h4>Validation Metrics</h4>
          <table>
            <tr><td><strong>AUC-PR</strong></td><td><span class="badge badge-warning">0.6315</span></td></tr>
            <tr><td>AUC-ROC</td><td>0.8376</td></tr>
            <tr><td>Precision</td><td>80.79%</td></tr>
            <tr><td>Recall</td><td>56.84%</td></tr>
            <tr><td>F1-Score</td><td>66.76%</td></tr>
          </table>

          <h4>Training Stats</h4>
          <ul>
            <li>Time: 2.7 hours</li>
            <li>Samples: 5M (32% of 15.6M)</li>
            <li>CV: 3-fold (9 combos √ó 3 = 27 runs)</li>
          </ul>

          <h4>Prediction Stats (KH√îNG CLEAN)</h4>
          <table>
            <tr><td>Total Rows</td><td>1,735,281</td></tr>
            <tr><td>Unique IDs</td><td>294,010</td></tr>
            <tr><td>Duplicates</td><td>1,441,270 (83%)</td></tr>
            <tr><td>File Size</td><td>53.74 MB</td></tr>
            <tr><td>Mean Prob</td><td>0.5729</td></tr>
            <tr><td>Prob Range</td><td>[0.347, 0.679]</td></tr>
          </table>
        </div>
      </div>

      <div class="info-box" style="margin-top:20px">
        <strong>üéØ Winner: V7 Baseline (+0.19%)</strong><br><br>
        
        <strong>Difference:</strong><br>
        - AUC-PR: 0.6327 vs 0.6315 = <strong>+0.0012</strong> (0.19% better)<br>
        - Manual params (numLeaves=120, lr=0.03) outperformed auto-tuned!<br>
        - Both models very close ‚Üí either can be submitted<br><br>

        <strong>Recommendation:</strong><br>
        Submit <code>submission_v7.csv</code> FIRST (higher validation AUC-PR)<br>
        Keep <code>submission_v7_auto.csv</code> as backup
      </div>
    </div>

    <!-- Prediction Statistics Comparison -->
    <div class="card card-full">
      <h2>üìà Prediction Statistics ‚Äî V7 Auto-tune (Detailed)</h2>
      
      <div class="info-box">
        <strong>üìä From predict_logs/stats.json:</strong>
      </div>

      <table>
        <thead>
          <tr><th>Metric</th><th>Value</th><th>Interpretation</th></tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>Test Samples</strong></td>
            <td>1,735,280</td>
            <td>All test rows processed (including duplicates)</td>
          </tr>
          <tr>
            <td><strong>Feature Dimension</strong></td>
            <td>10,017</td>
            <td>10K TF-IDF + 17 numeric features</td>
          </tr>
          <tr>
            <td><strong>Min Probability</strong></td>
            <td>0.3467</td>
            <td>Lowest helpful prediction</td>
          </tr>
          <tr>
            <td><strong>Max Probability</strong></td>
            <td>0.6792</td>
            <td>Highest helpful prediction</td>
          </tr>
          <tr>
            <td><strong>Mean Probability</strong></td>
            <td>0.5729</td>
            <td>Balanced (57% helpful on average)</td>
          </tr>
          <tr>
            <td><strong>Std Deviation</strong></td>
            <td>0.0773</td>
            <td>Low variance (consistent predictions)</td>
          </tr>
          <tr>
            <td><strong>Probability Range</strong></td>
            <td>[0.35, 0.68]</td>
            <td>Narrow (0.33 span) ‚Üí conservative model</td>
          </tr>
        </tbody>
      </table>

      <h3>üìä Distribution Analysis</h3>
      <ul>
        <li><strong>No Extreme Predictions:</strong> Kh√¥ng c√≥ prob g·∫ßn 0 ho·∫∑c 1 ‚Üí model well-calibrated</li>
        <li><strong>Narrow Range:</strong> 0.33 span (0.35-0.68) ‚Üí model conservative, kh√¥ng overconfident</li>
        <li><strong>Balanced Mean:</strong> 0.573 ‚Üí slightly more helpful than unhelpful</li>
        <li><strong>Low Variance:</strong> std=0.077 ‚Üí predictions r·∫•t consistent</li>
        <li><strong>Different from V2 Old:</strong> V2 old c√≥ bimodal (nhi·ªÅu 0 v√† 1), V7 c√≥ uniform h∆°n</li>
      </ul>

      <div class="success-box">
        <strong>‚úÖ Prediction Quality: EXCELLENT</strong><br>
        - No extreme probabilities (calibrated)<br>
        - Consistent predictions (low std)<br>
        - Balanced distribution (mean ‚âà 0.57)<br>
        - Ready for submission!
      </div>
    </div>

    <!-- Duplicate Analysis -->
    <div class="card card-full">
      <h2>üîç Duplicate Analysis ‚Äî Why 83%?</h2>
      
      <h3>üìä Statistics</h3>
      <table>
        <thead>
          <tr><th>Metric</th><th>Value</th><th>Calculation</th></tr>
        </thead>
        <tbody>
          <tr>
            <td>Total Rows</td>
            <td>1,735,281</td>
            <td>From submission CSV (including header)</td>
          </tr>
          <tr>
            <td>Unique review_ids</td>
            <td>294,010</td>
            <td>Distinct IDs after dedup</td>
          </tr>
          <tr>
            <td>Duplicate Rows</td>
            <td>1,441,270</td>
            <td>1,735,281 - 294,010 - 1 (header)</td>
          </tr>
          <tr>
            <td>Duplicate Rate</td>
            <td><strong>83.06%</strong></td>
            <td>1,441,270 / 1,735,280 √ó 100%</td>
          </tr>
          <tr>
            <td>Average Duplicates</td>
            <td><strong>~5.9x</strong></td>
            <td>1,735,280 / 294,010</td>
          </tr>
        </tbody>
      </table>

      <h3>üî¨ Root Cause Investigation</h3>
      
      <div class="warning-box">
        <strong>Hypothesis 1: Test Data Has Duplicates</strong><br><br>
        
        File <code>features_test_v4</code> tr√™n HDFS c√≥ duplicate review_ids!<br><br>
        
        <strong>Evidence:</strong><br>
        - C·∫£ V7 v√† V7_auto ƒë·ªÅu c√≥ SAME duplicate pattern (294K unique)<br>
        - Prediction pipeline ƒë·ªçc ALL rows t·ª´ test parquet ‚Üí duplicate predictions<br>
        - Trung b√¨nh m·ªói ID xu·∫•t hi·ªán 5.9 l·∫ßn<br><br>

        <strong>Verification Command:</strong>
        <div class="code-block">
          <pre># Check test data for duplicates
hdfs dfs -cat hdfs://localhost:9000/output_v2/features_test_v4/*.parquet | \
  wc -l  # Should show ~1.73M

# Count unique review_ids in test data
spark-shell --master local[*]
val df = spark.read.parquet("hdfs://localhost:9000/output_v2/features_test_v4")
df.count()  // Total rows
df.select("review_id").distinct().count()  // Unique IDs</pre>
        </div>
      </div>

      <div class="info-box" style="margin-top:20px">
        <strong>Hypothesis 2: Feature Engineering Duplicates</strong><br><br>
        
        Alternative: Feature pipeline (feature_pipeline_v2.py) t·∫°o multiple features per review ‚Üí duplicate rows<br><br>
        
        √çt likely v√¨:<br>
        - Feature pipeline t·∫°o 1 row per review (10,017 features)<br>
        - Kh√¥ng c√≥ join/explode logic t·∫°o duplicates
      </div>

      <h3>‚úÖ Solution Options</h3>
      
      <table>
        <thead>
          <tr><th>Option</th><th>Method</th><th>Output</th><th>Pros</th><th>Cons</th></tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>1. Keep First</strong></td>
            <td><code>drop_duplicates(keep='first')</code></td>
            <td>294,010 rows</td>
            <td>Fast, preserves order</td>
            <td>Ignores other predictions</td>
          </tr>
          <tr>
            <td><strong>2. Keep Last</strong></td>
            <td><code>drop_duplicates(keep='last')</code></td>
            <td>294,010 rows</td>
            <td>Latest prediction</td>
            <td>May change order</td>
          </tr>
          <tr>
            <td><strong>3. Average</strong></td>
            <td><code>groupby('id').agg(mean)</code></td>
            <td>294,010 rows</td>
            <td>Aggregates all predictions</td>
            <td>Changes probabilities</td>
          </tr>
          <tr>
            <td><strong>4. Submit As-is</strong></td>
            <td>No processing</td>
            <td>1,735,280 rows</td>
            <td>No data loss</td>
            <td>May violate format</td>
          </tr>
        </tbody>
      </table>

      <div class="success-box">
        <strong>üí° Recommendation:</strong><br><br>
        
        <strong>IF competition requires 1 prediction per review_id:</strong><br>
        ‚Üí Use <strong>Option 1: Keep First</strong> (fast, simple, preserves order)<br><br>

        <strong>IF competition accepts duplicates:</strong><br>
        ‚Üí Use <strong>Option 4: Submit As-is</strong> (no modification needed)<br><br>

        <strong>Check competition rules first!</strong>
      </div>
    </div>

    <!-- File Locations -->
    <div class="card card-full">
      <h2>üìÅ Output Files & Locations</h2>
      
      <h3>Submission Files (Raw - Ch∆∞a Clean)</h3>
      <table>
        <thead>
          <tr><th>File</th><th>Path</th><th>Size</th><th>Rows</th><th>Status</th></tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>submission_v7.csv</strong></td>
            <td>output_final/</td>
            <td>53.77 MB</td>
            <td>1,735,281</td>
            <td><span class="badge badge-success">‚úì Ready</span></td>
          </tr>
          <tr>
            <td><strong>submission_v7_auto.csv</strong></td>
            <td>output_final/</td>
            <td>53.74 MB</td>
            <td>1,735,281</td>
            <td><span class="badge badge-success">‚úì Ready</span></td>
          </tr>
          <tr>
            <td><strong>debug_v7_auto.csv</strong></td>
            <td>output_final/</td>
            <td>3.1 KB</td>
            <td>100</td>
            <td><span class="badge badge-info">Debug</span></td>
          </tr>
        </tbody>
      </table>

      <h3>Prediction Logs</h3>
      <table>
        <thead>
          <tr><th>File</th><th>Path</th><th>Description</th></tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>stats.json</strong></td>
            <td>tmp/predict_logs/</td>
            <td>Prediction statistics (min, max, mean, std)</td>
          </tr>
          <tr>
            <td><strong>params.txt</strong></td>
            <td>tmp/predict_logs/</td>
            <td>Prediction parameters & model info</td>
          </tr>
          <tr>
            <td><strong>schema_test.txt</strong></td>
            <td>tmp/predict_logs/</td>
            <td>Test data schema</td>
          </tr>
        </tbody>
      </table>

      <h3>Reports & Documentation</h3>
      <table>
        <thead>
          <tr><th>File</th><th>Path</th><th>Description</th></tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>day3_v2_training_report.html</strong></td>
            <td>docs_v2/</td>
            <td>Training details + algorithms + hyperparameters</td>
          </tr>
          <tr>
            <td><strong>day3_v2_final_report.html</strong></td>
            <td>docs_v2/</td>
            <td>Complete summary (training + auto-tune + prediction)</td>
          </tr>
          <tr>
            <td><strong>day3_v2_prediction_report.html</strong></td>
            <td>docs_v2/</td>
            <td><strong>THIS REPORT</strong> (prediction comparison + duplicates)</td>
          </tr>
          <tr>
            <td><strong>day3_v2_final_summary.md</strong></td>
            <td>docs_v2/</td>
            <td>Quick reference markdown</td>
          </tr>
        </tbody>
      </table>
    </div>

    <!-- Next Steps -->
    <div class="card card-full">
      <h2>‚úÖ Next Steps & Recommendations</h2>
      
      <h3>üéØ Decision Tree</h3>
      
      <div style="background:#f9fafb;padding:20px;border-radius:8px;margin:16px 0">
        <strong>Step 1: Check Competition Rules</strong><br>
        ‚ùì Does submission require 1 prediction per review_id?<br><br>
        
        <div style="margin-left:20px">
          <strong>IF YES (1 prediction per ID required):</strong><br>
          ‚Üí Go to Step 2 (Clean Duplicates)<br><br>
          
          <strong>IF NO (Duplicates accepted):</strong><br>
          ‚Üí Go to Step 3 (Submit As-is)
        </div>
      </div>

      <div style="background:#f9fafb;padding:20px;border-radius:8px;margin:16px 0">
        <strong>Step 2: Clean Duplicates (If Required)</strong><br><br>
        
        <div class="code-block">
          <pre># Option A: Keep First (Recommended)
import pandas as pd
df = pd.read_csv('output_final/submission_v7.csv')
df_clean = df.drop_duplicates(subset='review_id', keep='first')
df_clean.to_csv('output_final/submission_v7_clean.csv', index=False)

# Verify
print(f"Before: {len(df):,} rows")
print(f"After: {len(df_clean):,} rows")
print(f"Duplicates removed: {len(df) - len(df_clean):,}")

# Expected output:
# Before: 1,735,280 rows
# After: 294,010 rows
# Duplicates removed: 1,441,270</pre>
        </div>
      </div>

      <div style="background:#f9fafb;padding:20px;border-radius:8px;margin:16px 0">
        <strong>Step 3: Choose Model & Submit</strong><br><br>
        
        <strong>Recommended: submission_v7.csv (V7 Baseline)</strong><br>
        - Higher validation AUC-PR (0.6327 vs 0.6315)<br>
        - Better all-around metrics<br>
        - Manual hyperparameters proven effective<br><br>

        <strong>Backup: submission_v7_auto.csv (V7 Auto-tune)</strong><br>
        - Only 0.19% worse (negligible difference)<br>
        - Grid-searched parameters<br>
        - 3-fold CV robust validation
      </div>

      <div class="success-box">
        <strong>‚úÖ Summary Checklist:</strong><br><br>
        
        - [x] V7 Baseline trained & predicted (1.73M rows)<br>
        - [x] V7 Auto-tune trained & predicted (1.73M rows)<br>
        - [x] Both files copied to output_final/<br>
        - [x] Duplicate analysis complete (83% duplicates)<br>
        - [x] Statistics & comparison documented<br>
        - [ ] Check competition rules for submission format<br>
        - [ ] Clean duplicates IF required (keep first recommended)<br>
        - [ ] Submit chosen model (V7 baseline recommended)<br>
        - [ ] Document leaderboard results
      </div>
    </div>

    <!-- Footer -->
    <div style="text-align:center;padding:24px;color:#fff;font-size:0.9em">
      <p><strong>Day 3 V2 Prediction Report</strong> ‚Äî Generated November 1, 2025 @ 19:40</p>
      <p>Complete analysis: Training ‚Üí Auto-tuning ‚Üí Prediction ‚Üí Comparison</p>
      <p style="margin-top:8px">
        <span class="badge badge-success">V7 Baseline: 0.6327 AUC-PR ‚úÖ</span>
        <span class="badge badge-info">V7 Auto-tune: 0.6315 AUC-PR</span>
        <span class="badge badge-warning">Duplicates: 83% ‚ö†Ô∏è</span>
      </p>
      <div style="margin-top:16px;padding:16px;background:rgba(255,255,255,0.1);border-radius:8px">
        <strong>üìä ANALYSIS COMPLETE!</strong><br>
        Both submissions ready in <code>output_final/</code><br>
        Check competition rules ‚Üí Clean if needed ‚Üí Submit V7 Baseline first!
      </div>
    </div>
  </div>
      </div>
      <a class="backtop" href="#top">‚Üë Back to top</a>
    </section>
    
    <section id="sec3" class="section">
      <h2>Section 3: Day 3 V2 - Auto-Tuning Training Report (November 1, 2025)</h2>
      <div class="source-note">Source file: day3_v2_training_report.html</div>
      <hr/>
      <div class="embedded-report">
        <div class="container">
    <div class="hero">
      <h1>ÔøΩ Day 3 V2 ‚Äî Auto-Tuning LightGBM Training</h1>
      <p class="subtitle">Hyperparameter Tuning v·ªõi 3-Fold Cross-Validation & Semi-Supervised Learning</p>
      <div style="margin-top:16px">
        <span class="badge badge-info">Author: V√µ Th·ªã Di·ªÖm Thanh</span>
        <span class="badge badge-info">Date: November 1, 2025</span>
        <span class="badge badge-success">Status: Auto-Tuning Completed ‚úì</span>
      </div>
    </div>

    <!-- Executive Summary -->
    <div class="card card-full">
      <h2>üìä Executive Summary</h2>
      <div class="success-box">
        <strong>üéØ Auto-Tuning Results:</strong> Sau khi th·ª≠ 9 combinations v·ªõi 3-fold CV (27 training runs), t√¨m ƒë∆∞·ª£c hyperparameters t·ªëi ∆∞u: <strong>numLeaves=100, learningRate=0.15</strong>. Model cu·ªëi ƒë·∫°t <strong>AUC-PR = 0.6315</strong> tr√™n validation set v·ªõi 5M training samples.
      </div>
      
      <div class="metric-grid">
        <div class="metric-box">
          <div class="label">Best CV AUC-PR</div>
          <div class="value">0.642</div>
        </div>
        <div class="metric-box">
          <div class="label">Final Val AUC-PR</div>
          <div class="value">0.632</div>
        </div>
        <div class="metric-box">
          <div class="label">CV Configurations</div>
          <div class="value">9</div>
        </div>
        <div class="metric-box">
          <div class="label">Total Training Runs</div>
          <div class="value">27</div>
        </div>
      </div>

      <div class="note">
        <strong>‚öôÔ∏è Training Strategy:</strong> S·ª≠ d·ª•ng <strong>limit_train=5M</strong> samples (thay v√¨ full 15.6M) ƒë·ªÉ t·ªëi ∆∞u th·ªùi gian training trong deadline 12 gi·ªù. Grid search v·ªõi quick preset (3√ó3 grid) cho numLeaves v√† learningRate.
      </div>
    </div>

    <!-- Training Command & Explanation -->
    <div class="card card-full">
      <h2>üöÄ Auto-Tuning Command Used</h2>
      <p>L·ªánh th·ª±c t·∫ø ƒë√£ ch·∫°y ƒë·ªÉ auto-tune hyperparameters:</p>
      
      <div class="code-block">
        <pre>spark-submit \
  --master local[*] \
  --deploy-mode client \
  --packages com.microsoft.azure:synapseml-lightgbm_2.12:1.0.7 \
  --driver-memory 11g \
  --executor-memory 11g \
  --conf spark.driver.maxResultSize=4g \
  --conf spark.sql.shuffle.partitions=64 \
  --conf spark.sql.adaptive.enabled=true \
  "train_lightgbm_spark_v2.py" \
  --train "hdfs://localhost:9000/output_v2/features_train_v4" \
  --test "hdfs://localhost:9000/output_v2/features_test_v4" \
  --out "hdfs://localhost:9000/output_v2/models/lightgbm_v7_auto" \
  --limit_train 5000000 \
  --auto_tune \
  --tune_preset quick \
  --save_schema_log</pre>
      </div>

      <h3>üìù Gi·∫£i th√≠ch c√°c tham s·ªë:</h3>
      <table>
        <thead>
          <tr><th>Parameter</th><th>Value</th><th>Explanation</th></tr>
        </thead>
        <tbody>
          <tr>
            <td><code>--master local[*]</code></td>
            <td>local[*]</td>
            <td>Ch·∫°y Spark local mode, s·ª≠ d·ª•ng t·∫•t c·∫£ CPU cores (16 cores)</td>
          </tr>
          <tr>
            <td><code>--driver-memory</code></td>
            <td>11g</td>
            <td>C·∫•p ph√°t 11GB RAM cho Spark driver (t·ªïng 22GB v·ªõi executor)</td>
          </tr>
          <tr>
            <td><code>--executor-memory</code></td>
            <td>11g</td>
            <td>C·∫•p ph√°t 11GB RAM cho Spark executor (~70% RAM h·ªá th·ªëng)</td>
          </tr>
          <tr>
            <td><code>--limit_train</code></td>
            <td>5,000,000</td>
            <td>Gi·ªõi h·∫°n 5M samples (thay v√¨ full 15.6M) ƒë·ªÉ t·ªëi ∆∞u th·ªùi gian</td>
          </tr>
          <tr>
            <td><code>--auto_tune</code></td>
            <td>enabled</td>
            <td><strong>B·∫≠t auto-tuning</strong>: t·ª± ƒë·ªông t√¨m hyperparameters t·ªëi ∆∞u</td>
          </tr>
          <tr>
            <td><code>--tune_preset</code></td>
            <td>quick</td>
            <td>Quick preset: 9 combinations (3√ó3 grid), ~2-3 gi·ªù</td>
          </tr>
          <tr>
            <td><code>Grid Search</code></td>
            <td>9 combos</td>
            <td>numLeaves=[31,50,100] √ó learningRate=[0.05,0.1,0.15]</td>
          </tr>
          <tr>
            <td><code>Cross-Validation</code></td>
            <td>3-fold</td>
            <td>Stratified 3-fold CV ‚Üí 9 √ó 3 = <strong>27 training runs</strong></td>
          </tr>
          <tr>
            <td><code>Evaluation Metric</code></td>
            <td>AUC-PR</td>
            <td>Average Precision (ph√π h·ª£p v·ªõi imbalanced data)</td>
          </tr>
          <tr>
            <td><code>Class Weight</code></td>
            <td>3.054</td>
            <td>Auto-computed: neg/pos = 3,389,339/1,109,945 = 3.054</td>
          </tr>
          <tr>
            <td><code>Feature Dimension</code></td>
            <td>10,017</td>
            <td>10K TF-IDF features + 17 numeric/boolean features</td>
          </tr>
        </tbody>
      </table>
    </div>

    <!-- Algorithms & Techniques -->
    <div class="card card-full">
      <h2>üß† Thu·∫≠t To√°n & K·ªπ Thu·∫≠t S·ª≠ D·ª•ng</h2>
      
      <h3>1Ô∏è‚É£ LightGBM (Light Gradient Boosting Machine)</h3>
      <div class="feature-list">
        <p><strong>Kh√°i ni·ªám:</strong> Gradient Boosting Decision Trees (GBDT) t·ªëi ∆∞u, s·ª≠ d·ª•ng histogram-based learning v√† leaf-wise growth.</p>
        
        <p><strong>∆Øu ƒëi·ªÉm:</strong></p>
        <ul>
          <li>‚ö° <strong>T·ªëc ƒë·ªô cao:</strong> Histogram-based algorithm ‚Üí gi·∫£m memory & tƒÉng t·ªëc training</li>
          <li>üìä <strong>X·ª≠ l√Ω high-dimensional data:</strong> 10K features v·∫´n train nhanh</li>
          <li>üéØ <strong>Leaf-wise growth:</strong> TƒÉng accuracy so v·ªõi level-wise (XGBoost)</li>
          <li>üîß <strong>Regularization:</strong> L1/L2, min_data_in_leaf, feature_fraction ‚Üí ch·ªëng overfitting</li>
        </ul>
        
        <p><strong>C√°ch ho·∫°t ƒë·ªông:</strong></p>
        <ul>
          <li>Build c√¢y quy·∫øt ƒë·ªãnh tu·∫ßn t·ª±, m·ªói c√¢y h·ªçc t·ª´ residual (sai s·ªë) c·ªßa c√¢y tr∆∞·ªõc</li>
          <li>Loss function: Binary Cross-Entropy (log loss) cho binary classification</li>
          <li>Optimization: Gradient descent tr√™n loss function</li>
          <li>Prediction: T·ªïng weighted output c·ªßa t·∫•t c·∫£ c√¢y ‚Üí sigmoid ‚Üí probability [0,1]</li>
        </ul>
      </div>

      <h3>2Ô∏è‚É£ Hyperparameter Tuning (Grid Search with Cross-Validation)</h3>
      <div class="feature-list">
        <p><strong>Kh√°i ni·ªám:</strong> T√¨m ki·∫øm exhaustive tr√™n grid space ƒë·ªÉ t√¨m combination t·ªëi ∆∞u.</p>
        
        <p><strong>Grid Space (Quick Preset):</strong></p>
        <ul>
          <li><code>numLeaves</code>: [31, 50, 100] ‚Üí Tree complexity (s·ªë l√°/c√¢y)</li>
          <li><code>learningRate</code>: [0.05, 0.1, 0.15] ‚Üí Step size trong gradient descent</li>
          <li>Total combinations: 3 √ó 3 = <strong>9 configs</strong></li>
        </ul>
        
        <p><strong>3-Fold Stratified Cross-Validation:</strong></p>
        <ul>
          <li>Split data th√†nh 3 folds, gi·ªØ nguy√™n class distribution (stratified)</li>
          <li>M·ªói fold l√†m validation 1 l·∫ßn, 2 folds c√≤n l·∫°i l√†m training</li>
          <li>Mean AUC-PR c·ªßa 3 folds = metric ƒë√°nh gi√° combo</li>
          <li>Total runs: 9 combos √ó 3 folds = <strong>27 training runs</strong></li>
        </ul>
        
        <p><strong>Best Params Selection:</strong></p>
        <ul>
          <li>Ch·ªçn combo c√≥ <strong>Mean CV AUC-PR cao nh·∫•t</strong></li>
          <li>K·∫øt qu·∫£: <code>numLeaves=100, learningRate=0.15</code> ‚Üí AUC-PR = 0.6417</li>
          <li>Retrain model cu·ªëi tr√™n full training data v·ªõi best params</li>
        </ul>
      </div>

      <h3>3Ô∏è‚É£ Semi-Supervised Learning (Pseudo-Labeling) - Optional</h3>
      <div class="feature-list">
        <p><strong>Kh√°i ni·ªám:</strong> S·ª≠ d·ª•ng unlabeled data (test set) ƒë·ªÉ tƒÉng training data.</p>
        
        <p><strong>Workflow:</strong></p>
        <ul>
          <li>Train model base tr√™n labeled data (5M samples)</li>
          <li>Predict tr√™n test set ‚Üí l·∫•y high-confidence predictions l√†m pseudo-labels</li>
          <li>Th√™m pseudo-labeled data v√†o training set ‚Üí retrain model</li>
          <li>Iterate cho ƒë·∫øn khi converge ho·∫∑c h·∫øt iteration</li>
        </ul>
        
        <p><strong>L∆∞u √Ω:</strong> Code c√≥ support nh∆∞ng <strong>kh√¥ng enable</strong> trong run n√†y (ch·ªâ supervised learning).</p>
      </div>

      <h3>4Ô∏è‚É£ Class Imbalance Handling</h3>
      <div class="feature-list">
        <p><strong>V·∫•n ƒë·ªÅ:</strong> Class imbalance 1:3 (helpful=1: 1.1M vs unhelpful=0: 3.4M)</p>
        
        <p><strong>Gi·∫£i ph√°p:</strong></p>
        <ul>
          <li><strong>Scale Pos Weight:</strong> TƒÉng weight cho class thi·ªÉu s·ªë (helpful=1)</li>
          <li>C√¥ng th·ª©c: <code>weight = neg_count / pos_count = 3,389,339 / 1,109,945 = 3.054</code></li>
          <li>Effect: Loss c·ªßa positive samples ƒë∆∞·ª£c nh√¢n v·ªõi 3.054 ‚Üí model focus h∆°n v√†o class n√†y</li>
          <li><strong>Stratified Sampling:</strong> CV split gi·ªØ nguy√™n t·ª∑ l·ªá class trong m·ªói fold</li>
          <li><strong>Metric:</strong> AUC-PR (kh√¥ng ph·∫£i accuracy) ‚Üí ph√π h·ª£p v·ªõi imbalanced data</li>
        </ul>
      </div>

      <h3>5Ô∏è‚É£ Feature Engineering (Day 2)</h3>
      <div class="feature-list">
        <p><strong>TF-IDF Vectorization (10,000 features):</strong></p>
        <ul>
          <li>Tokenize review text ‚Üí compute Term Frequency - Inverse Document Frequency</li>
          <li>Max features: 10,000 (top frequent terms)</li>
          <li>Captures semantic information t·ª´ review content</li>
        </ul>
        
        <p><strong>Metadata Features (17 numeric/boolean):</strong></p>
        <ul>
          <li>User behavior: user_review_count, user_helpful_ratio, user_avg_rating</li>
          <li>Product aggregates: product_review_count, product_helpful_ratio, product_avg_rating</li>
          <li>Review quality: review_length, review_length_log, rating_deviation</li>
          <li>Category indicators: is_popular_category, has_metadata, is_expensive</li>
        </ul>
      </div>

      <h3>6Ô∏è‚É£ KMeans Clustering (Optional - Synthetic Labels)</h3>
      <div class="feature-list">
        <p><strong>Kh√°i ni·ªám:</strong> Unsupervised learning algorithm ƒë·ªÉ ph√¢n nh√≥m data th√†nh K clusters.</p>
        
        <p><strong>Workflow trong code:</strong></p>
        <div class="code-block">
          <pre>from pyspark.ml.clustering import KMeans

# Initialize KMeans v·ªõi k=2 (binary classification)
kmeans = KMeans(
    featuresCol='features',  # Vector column (10,017 dims)
    k=2,                     # 2 clusters (helpful vs not helpful)
    seed=42,                 # Reproducible
    maxIter=20               # Max iterations
)

# Train clustering model
model = kmeans.fit(df)

# Predict cluster assignments (0 ho·∫∑c 1)
df = model.transform(df)  # Adds 'prediction' column

# Use cluster ID as synthetic label
df = df.withColumn('is_helpful', col('prediction').cast(IntegerType()))</pre>
        </div>
        
        <p><strong>Chi ti·∫øt algorithm:</strong></p>
        <ul>
          <li><strong>Initialization:</strong> Random ch·ªçn 2 centroids (cluster centers) trong feature space</li>
          <li><strong>Assignment step:</strong> Assign m·ªói data point v√†o cluster g·∫ßn nh·∫•t (Euclidean distance)</li>
          <li><strong>Update step:</strong> T√≠nh l·∫°i centroid c·ªßa m·ªói cluster (mean c·ªßa t·∫•t c·∫£ points trong cluster)</li>
          <li><strong>Iterate:</strong> L·∫∑p assignment + update cho ƒë·∫øn khi converge (centroids kh√¥ng ƒë·ªïi) ho·∫∑c maxIter</li>
        </ul>
        
        <p><strong>∆Øu ƒëi·ªÉm:</strong></p>
        <ul>
          <li>‚úì Kh√¥ng c·∫ßn labeled data (unsupervised)</li>
          <li>‚úì T·ª± ƒë·ªông t√¨m patterns trong high-dimensional space (10K features)</li>
          <li>‚úì Fast & scalable v·ªõi PySpark distributed computing</li>
        </ul>
        
        <p><strong>Nh∆∞·ª£c ƒëi·ªÉm:</strong></p>
        <ul>
          <li>‚úó Cluster assignment kh√¥ng guarantee t∆∞∆°ng ·ª©ng v·ªõi helpful/unhelpful th·ª±c t·∫ø</li>
          <li>‚úó Sensitive to initialization (random seed)</li>
          <li>‚úó Assumes spherical clusters (Euclidean distance)</li>
        </ul>
        
        <p><strong>Khi n√†o d√πng KMeans?</strong></p>
        <ul>
          <li>Test set kh√¥ng c√≥ labels ‚Üí d√πng KMeans ƒë·ªÉ t·∫°o synthetic labels</li>
          <li>Exploratory analysis ‚Üí t√¨m natural groupings trong data</li>
          <li>Heuristic method kh√¥ng ho·∫°t ƒë·ªông t·ªët (thi·∫øu features nh∆∞ rating, sentiment)</li>
        </ul>
        
        <div class="note">
          <strong>üìù L∆∞u √Ω:</strong> Trong auto-tune run n√†y, <strong>kh√¥ng s·ª≠ d·ª•ng KMeans</strong> v√¨ training data ƒë√£ c√≥ ground truth labels (is_helpful). KMeans ch·ªâ d√πng khi c·∫ßn generate synthetic labels cho unlabeled data.
        </div>
      </div>
    </div>

    <!-- Code Functions Detailed Explanation -->
    <div class="card card-full">
      <h2>üîç Chi Ti·∫øt Code - C√°c H√†m Ch√≠nh</h2>
      <p>Gi·∫£i th√≠ch chi ti·∫øt t·ª´ng h√†m quan tr·ªçng trong <code>train_lightgbm_spark_v2.py</code>:</p>

      <h3>1. <code>generate_synthetic_labels()</code> - T·∫°o Labels T·ª± ƒê·ªông</h3>
      <div class="code-block">
        <pre>def generate_synthetic_labels(df, label_col, method='heuristic', seed=42):
    """
    T·∫°o synthetic labels khi kh√¥ng c√≥ ground truth.
    
    Methods:
    - 'heuristic': D√πng rating + review length + sentiment (rule-based)
    - 'clustering': D√πng KMeans ƒë·ªÉ t√¨m nh√≥m t·ª± nhi√™n (unsupervised)
    """</pre>
      </div>
      <p><strong>M·ª•c ƒë√≠ch:</strong> Khi test set kh√¥ng c√≥ label (is_helpful), t·∫°o pseudo-labels ƒë·ªÉ train.</p>
      
      <h4>Method 1: Heuristic (Rule-based)</h4>
      <div class="feature-list">
        <p><strong>Logic:</strong> T√≠nh ƒëi·ªÉm d·ª±a tr√™n nhi·ªÅu y·∫øu t·ªë:</p>
        <ul>
          <li><strong>star_rating:</strong> Rating cao (4-5 sao) ‚Üí helpful h∆°n. Normalize v·ªÅ [0,1]</li>
          <li><strong>review_length_log:</strong> Reviews d√†i ‚Üí informative h∆°n. Normalize v·ªÅ [0,1]</li>
          <li><strong>sentiment_compound:</strong> Sentiment positive ‚Üí helpful h∆°n</li>
          <li><strong>user_helpful_ratio:</strong> User c√≥ l·ªãch s·ª≠ helpful ‚Üí reviews helpful h∆°n</li>
        </ul>
        <p><strong>C√¥ng th·ª©c:</strong></p>
        <div class="code-block">
          <pre># Weighted score
score = (star_rating * 0.3 + 
         review_length_log * 0.25 + 
         sentiment * 0.2 + 
         user_helpful_ratio * 0.25)

# Threshold = median
if score >= median(score):
    label = 1  # helpful
else:
    label = 0  # not helpful</pre>
        </div>
      </div>

      <h4>Method 2: KMeans Clustering</h4>
      <div class="feature-list">
        <p><strong>Logic:</strong> D√πng unsupervised learning ƒë·ªÉ ph√¢n nh√≥m t·ª± nhi√™n:</p>
        <div class="code-block">
          <pre>from pyspark.ml.clustering import KMeans

# Train KMeans v·ªõi k=2 clusters
kmeans = KMeans(featuresCol='features', k=2, seed=42, maxIter=20)
model = kmeans.fit(df)

# Predict cluster assignments
df = model.transform(df)  # Column 'prediction' = 0 ho·∫∑c 1

# S·ª≠ d·ª•ng cluster ID l√†m label
df = df.withColumn(label_col, col('prediction').cast(IntegerType()))</pre>
        </div>
        <p><strong>∆Øu ƒëi·ªÉm:</strong> Kh√¥ng c·∫ßn feature engineering, t√¨m patterns t·ª± nhi√™n trong data.</p>
        <p><strong>Nh∆∞·ª£c ƒëi·ªÉm:</strong> Cluster kh√¥ng ƒë·∫£m b·∫£o t∆∞∆°ng ·ª©ng v·ªõi helpful/unhelpful th·ª±c t·∫ø.</p>
      </div>

      <h3>2. <code>stratified_kfold_split()</code> - Chia K-Fold Stratified</h3>
      <div class="code-block">
        <pre>def stratified_kfold_split(df, label_col, n_folds=3, seed=42):
    """
    Stratified K-Fold: chia data th√†nh K folds, gi·ªØ class balance.
    Returns: list of (train_fold, val_fold) tuples
    """
    # Assign fold ID proportionally within each class
    window = Window.partitionBy(label_col).orderBy(rand(seed))
    df = df.withColumn("__row_num__", row_number().over(window))
    df = df.withColumn("__fold__", (col("__row_num__") % n_folds).cast("int"))
    
    # Create folds
    for fold_idx in range(n_folds):
        val_fold = df.filter(col("__fold__") == fold_idx)
        train_fold = df.filter(col("__fold__") != fold_idx)
        yield (train_fold, val_fold)</pre>
      </div>
      <p><strong>Gi·∫£i th√≠ch:</strong></p>
      <ul>
        <li><strong>Window.partitionBy(label_col):</strong> Chia data theo class (0 v√† 1 ri√™ng bi·ªát)</li>
        <li><strong>row_number():</strong> ƒê√°nh s·ªë th·ª© t·ª± trong m·ªói class</li>
        <li><strong>% n_folds:</strong> Chia ƒë·ªÅu: row 0,3,6,... ‚Üí fold 0; row 1,4,7,... ‚Üí fold 1; etc.</li>
        <li><strong>K·∫øt qu·∫£:</strong> M·ªói fold c√≥ t·ª∑ l·ªá class gi·ªëng nhau (1:3 ratio maintained)</li>
      </ul>

      <h3>3. <code>hyperparameter_tuning()</code> - Grid Search + CV</h3>
      <div class="code-block">
        <pre>def hyperparameter_tuning(train_df, label_col, features_col, args, preset="quick"):
    """
    Grid search v·ªõi 3-fold CV ƒë·ªÉ t√¨m best hyperparameters.
    
    Quick preset: 9 combos (3√ó3 grid)
    Thorough preset: 27 combos (3√ó3√ó3 grid)
    """
    # Define grid
    param_grid = {
        "numLeaves": [31, 50, 100],
        "learningRate": [0.05, 0.1, 0.15]
    }
    
    # Generate all combinations
    all_combos = list(product(*param_grid.values()))  # 9 combos
    
    # Create 3-fold stratified split
    folds = stratified_kfold_split(train_df, label_col, n_folds=3)
    
    # Grid search
    for combo in all_combos:
        params = dict(zip(param_grid.keys(), combo))
        
        # Cross-validation
        fold_scores = []
        for train_fold, val_fold in folds:
            # Train model v·ªõi params n√†y
            model = LightGBMClassifier(**params).fit(train_fold)
            
            # Evaluate tr√™n val_fold
            auc_pr = evaluate(model, val_fold)
            fold_scores.append(auc_pr)
        
        # Compute mean & std
        mean_aucpr = mean(fold_scores)
        std_aucpr = stdev(fold_scores)
    
    # Return best params
    best_params = max(results, key=lambda x: x['mean_aucpr'])</pre>
      </div>
      <p><strong>Chi ti·∫øt workflow:</strong></p>
      <ul>
        <li><strong>itertools.product():</strong> Generate Cartesian product (all combinations)</li>
        <li><strong>3-Fold CV:</strong> Train 3 l·∫ßn m·ªói combo ‚Üí 9 √ó 3 = 27 training runs</li>
        <li><strong>mean(fold_scores):</strong> Average AUC-PR c·ªßa 3 folds = metric ƒë√°nh gi√° combo</li>
        <li><strong>std(fold_scores):</strong> Standard deviation ‚Üí ƒëo stability (low variance = better)</li>
        <li><strong>Best selection:</strong> Ch·ªçn combo c√≥ mean AUC-PR cao nh·∫•t</li>
      </ul>

      <h3>4. <code>compute_class_weight()</code> - X·ª≠ L√Ω Imbalance</h3>
      <div class="code-block">
        <pre>def compute_class_weight(df, label_col, weight_col="weight", pos_weight=None):
    """
    T√≠nh class weight ƒë·ªÉ handle imbalanced data.
    pos_weight: 'auto' ‚Üí N_neg/N_pos, ho·∫∑c float value
    """
    # Count positive and negative samples
    pos = df.filter(col(label_col) == 1).count()  # helpful
    neg = df.filter(col(label_col) == 0).count()  # not helpful
    
    if pos_weight == "auto":
        # Auto-compute weight ratio
        w1 = max(0.1, min(10.0, float(neg) / float(pos)))
        # Clamp to [0.1, 10] ƒë·ªÉ tr√°nh extreme values
    
    # Assign weight column
    df = df.withColumn(weight_col, 
                       when(col(label_col) == 1, lit(w1)).otherwise(lit(1.0)))
    
    return df, w1, pos, neg</pre>
      </div>
      <p><strong>Gi·∫£i th√≠ch c√¥ng th·ª©c:</strong></p>
      <ul>
        <li><strong>weight = neg/pos:</strong> V√≠ d·ª• 3,389,339 / 1,109,945 = 3.054</li>
        <li><strong>Effect:</strong> Loss c·ªßa positive samples nh√¢n v·ªõi 3.054 ‚Üí model focus h∆°n</li>
        <li><strong>Clamp [0.1, 10]:</strong> Tr√°nh extreme weights (too high ‚Üí overfitting minority class)</li>
        <li><strong>LightGBM classWeight:</strong> Format "0:1,1:3.054" ‚Üí class 0 weight=1, class 1 weight=3.054</li>
      </ul>

      <h3>5. <code>evaluate_model()</code> - Comprehensive Metrics</h3>
      <div class="code-block">
        <pre>def evaluate_model(model, df, label_col, stage_name="VAL"):
    """
    Evaluate model v·ªõi nhi·ªÅu metrics.
    Returns: (metrics, pred_df)
    """
    # Binary classification metrics
    eval_pr = BinaryClassificationEvaluator(metricName="areaUnderPR")
    eval_roc = BinaryClassificationEvaluator(metricName="areaUnderROC")
    
    pred_df = model.transform(df)
    aucpr = eval_pr.evaluate(pred_df)
    aucroc = eval_roc.evaluate(pred_df)
    
    # Multiclass metrics (for Precision, Recall, F1)
    evaluator_precision = MulticlassClassificationEvaluator(
        metricName="weightedPrecision")
    evaluator_recall = MulticlassClassificationEvaluator(
        metricName="weightedRecall")
    evaluator_f1 = MulticlassClassificationEvaluator(
        metricName="f1")
    
    precision = evaluator_precision.evaluate(pred_df)
    recall = evaluator_recall.evaluate(pred_df)
    f1 = evaluator_f1.evaluate(pred_df)
    
    # Confusion matrix
    cm_df = pred_df.groupBy(label_col, "prediction").count().collect()
    tp = confusion_matrix.get("true_1_pred_1", 0)
    tn = confusion_matrix.get("true_0_pred_0", 0)
    fp = confusion_matrix.get("true_0_pred_1", 0)
    fn = confusion_matrix.get("true_1_pred_0", 0)
    
    return metrics, pred_df</pre>
      </div>
      <p><strong>Metrics explained:</strong></p>
      <table>
        <thead>
          <tr><th>Metric</th><th>Formula</th><th>Meaning</th></tr>
        </thead>
        <tbody>
          <tr><td><strong>AUC-PR</strong></td><td>Area Under Precision-Recall Curve</td><td>T·ªët cho imbalanced data (focus on positive class)</td></tr>
          <tr><td><strong>AUC-ROC</strong></td><td>Area Under ROC Curve</td><td>Overall classification performance (TPR vs FPR)</td></tr>
          <tr><td><strong>Precision</strong></td><td>TP / (TP + FP)</td><td>% predictions ch√≠nh x√°c trong nh·ªØng g√¨ model d·ª± ƒëo√°n l√† helpful</td></tr>
          <tr><td><strong>Recall</strong></td><td>TP / (TP + FN)</td><td>% helpful reviews m√† model t√¨m ƒë∆∞·ª£c (sensitivity)</td></tr>
          <tr><td><strong>F1-Score</strong></td><td>2 √ó (Precision √ó Recall) / (Precision + Recall)</td><td>Harmonic mean c·ªßa Precision & Recall (balanced metric)</td></tr>
        </tbody>
      </table>

      <h3>6. <code>pseudo_label_iteration()</code> - Semi-Supervised Learning</h3>
      <div class="code-block">
        <pre>def pseudo_label_iteration(model, unlabeled_df, label_col, 
                                 min_prob=0.9, top_pct=0.1, pseudo_weight=0.3):
    """
    Pseudo-labeling: d√πng high-confidence predictions l√†m labels.
    """
    # Predict tr√™n unlabeled data
    pred_df = model.transform(unlabeled_df)
    
    # Extract probability for class 1
    get_prob_udf = udf(lambda v: float(v[1]) if v else 0.0, FloatType())
    pred_df = pred_df.withColumn("prob_class1", get_prob_udf(col("probability")))
    
    # Select confident samples
    confident_pos = pred_df.filter(col("prob_class1") >= 0.9)  # prob ‚â• 90%
    confident_neg = pred_df.filter(col("prob_class1") <= 0.1)  # prob ‚â§ 10%
    
    # Take top 10% by confidence
    n_pos = int(confident_pos.count() * 0.1)
    n_neg = int(confident_neg.count() * 0.1)
    
    pseudo_pos = confident_pos.orderBy(desc("prob_class1")).limit(n_pos) \
        .withColumn(label_col, lit(1))
    pseudo_neg = confident_neg.orderBy(asc("prob_class1")).limit(n_neg) \
        .withColumn(label_col, lit(0))
    
    # Assign low weight (0.3) cho pseudo-labels
    pseudo_df = pseudo_pos.union(pseudo_neg)
    pseudo_df = pseudo_df.withColumn("weight", lit(0.3))
    
    return pseudo_df</pre>
      </div>
      <p><strong>Workflow:</strong></p>
      <ol>
        <li><strong>Predict:</strong> Model d·ª± ƒëo√°n tr√™n unlabeled data (test set)</li>
        <li><strong>Filter confident:</strong> Ch·ªâ l·∫•y samples v·ªõi prob ‚â• 90% (positive) ho·∫∑c ‚â§ 10% (negative)</li>
        <li><strong>Top selection:</strong> Ch·ªçn top 10% confident nh·∫•t ‚Üí pseudo-labels</li>
        <li><strong>Low weight:</strong> Assign weight=0.3 (th·∫•p h∆°n real labels) v√¨ kh√¥ng ch·∫Øc ch·∫Øn 100%</li>
        <li><strong>Retrain:</strong> Th√™m pseudo-labeled data v√†o training set ‚Üí retrain model</li>
      </ol>
      <p><strong>L∆∞u √Ω:</strong> Trong run n√†y <strong>kh√¥ng enable</strong> pseudo-labeling (pseudo_rounds=0).</p>
    </div>

    <!-- Code Workflow -->
    <div class="card card-full">
      <h2>üîß Code Workflow (train_lightgbm_spark_v2.py)</h2>
      <p>File th·ª±c hi·ªán workflow auto-tuning v·ªõi PySpark + SynapseML:</p>

      <div class="workflow">
        <div class="workflow-step">
          <h4>Step 1: Load Data from HDFS v·ªõi PySpark</h4>
          <div class="code-block">
            <pre># Load Parquet t·ª´ HDFS qua Spark
train_df = spark.read.parquet("hdfs://localhost:9000/output_v2/features_train_v4")
test_df = spark.read.parquet("hdfs://localhost:9000/output_v2/features_test_v4")

# Limit training samples (t·ªëi ∆∞u th·ªùi gian)
if limit_train:
    train_df = train_df.limit(5000000)  # 5M samples

# Feature dimension: 10,017 (TF-IDF 10K + numeric 17)</pre>
          </div>
          <p class="muted">‚úì Spark distributed loading ‚Üí handle large files (GB scale)</p>
          <p class="muted">‚úì HDFS URI: hdfs://localhost:9000/... (JNI connector)</p>
        </div>

        <div class="workflow-step">
          <h4>Step 2: Stratified Train/Val Split</h4>
          <div class="code-block">
            <pre># Custom stratified split (PySpark kh√¥ng c√≥ built-in)
def stratified_kfold_split(df, label_col, n_splits=3, seed=42):
    # Split theo label distribution
    pos = df.filter(f"{label_col} = 1")
    neg = df.filter(f"{label_col} = 0")
    
    # Random split m·ªói class
    for fold in range(n_splits):
        train_folds = [...]  # Other folds
        val_fold = fold
        yield train_folds, val_fold

# T·∫°o 90% train, 10% val (gi·ªØ class balance)
train_df, val_df = stratified_split(train_df, "is_helpful")</pre>
          </div>
          <p class="muted">‚úì Stratified: gi·ªØ ratio 1:3 (helpful:unhelpful) trong m·ªói fold</p>
          <p class="muted">‚úì Seed=42: reproducible splits</p>
        </div>

        <div class="workflow-step">
          <h4>Step 3: Compute Class Weight</h4>
          <div class="code-block">
            <pre># ƒê·∫øm class balance
neg_count = train_df.filter("is_helpful = 0").count()  # 3,389,339
pos_count = train_df.filter("is_helpful = 1").count()  # 1,109,945

# Scale pos weight (cho LightGBM)
pos_weight = neg_count / pos_count  # 3.054
lgbm_params["classWeight"] = f"0:{1},1:{pos_weight}"</pre>
          </div>
          <p class="muted">‚úì Auto-computed: kh√¥ng c·∫ßn manual tuning</p>
          <p class="muted">‚úì Format: "0:1,1:3.054" ‚Üí LightGBM weighted loss</p>
        </div>

        <div class="workflow-step">
          <h4>Step 4: Grid Search v·ªõi 3-Fold CV</h4>
          <div class="code-block">
            <pre># Define grid space (quick preset)
param_grid = {
    "numLeaves": [31, 50, 100],
    "learningRate": [0.05, 0.1, 0.15]
}

# Generate all combinations
combos = list(itertools.product(*param_grid.values()))  # 9 combos

# 3-Fold Cross-Validation
results = []
for combo in combos:
    for train_fold, val_fold in kfold_split(train_df, n_splits=3):
        # Train LightGBM v·ªõi combo n√†y
        model = LightGBMClassifier(**combo_params)
        model.fit(train_fold)
        
        # Evaluate tr√™n val_fold
        auc_pr = evaluate(model, val_fold)
        results.append((combo, auc_pr))

# Ch·ªçn combo c√≥ mean AUC-PR cao nh·∫•t
best_combo = max(results, key=lambda x: mean(x[1]))</pre>
          </div>
          <p class="muted">‚úì Total: 9 combos √ó 3 folds = 27 training runs (~2.5 hours)</p>
          <p class="muted">‚úì Evaluation: AUC-PR (average_precision metric)</p>
        </div>

        <div class="workflow-step">
          <h4>Step 5: Final Training v·ªõi Best Params</h4>
          <div class="code-block">
            <pre># Apply best hyperparameters
best_params = {
    "numLeaves": 100,
    "learningRate": 0.15,
    "minDataInLeaf": 50,
    "featureFraction": 0.75,
    "baggingFraction": 0.75,
    "lambdaL1": 0.1,
    "lambdaL2": 0.1
}

# Train tr√™n full training data (4.5M samples)
from synapse.ml.lightgbm import LightGBMClassifier
lgbm = LightGBMClassifier(
    featuresCol="features",
    labelCol="is_helpful",
    **best_params
)
final_model = lgbm.fit(train_df)</pre>
          </div>
          <p class="muted">‚úì SynapseML LightGBM: distributed training tr√™n Spark</p>
          <p class="muted">‚úì Retrain v·ªõi best params ‚Üí generalization t·ªët h∆°n single fold</p>
        </div>

        <div class="workflow-step">
          <h4>Step 6: Evaluation & Save Model</h4>
          <div class="code-block">
            <pre># Predict tr√™n validation set
predictions = final_model.transform(val_df)

# Extract probability t·ª´ vector column
from pyspark.sql.functions import udf
prob_udf = udf(lambda v: float(v[1]), DoubleType())
predictions = predictions.withColumn("prob", prob_udf("probability"))

# Compute metrics
from sklearn.metrics import average_precision_score
y_true = predictions.select("is_helpful").toPandas().values
y_prob = predictions.select("prob").toPandas().values
auc_pr = average_precision_score(y_true, y_prob)  # 0.6315

# Save model to HDFS
final_model.write().overwrite().save(
    "hdfs://localhost:9000/output_v2/models/lightgbm_v7_auto"
)</pre>
          </div>
          <p class="muted">‚úì Model format: Spark MLlib pipeline (metadata + LightGBM booster)</p>
          <p class="muted">‚úì Metrics: AUC-PR, AUC-ROC, confusion matrix ‚Üí reports/*.json</p>
        </div>
      </div>
    </div>

    <!-- Auto-Tuning Results -->
    <div class="card card-full">
      <h2>üèÜ Auto-Tuning Results - Top 5 Configurations</h2>
      <table>
        <thead>
          <tr><th>Rank</th><th>Mean CV AUC-PR</th><th>Std Dev</th><th>numLeaves</th><th>learningRate</th></tr>
        </thead>
        <tbody>
          <tr style="background:#d1fae5">
            <td><strong>ü•á 1st</strong></td>
            <td><strong>0.6417</strong></td>
            <td>¬±0.0008</td>
            <td>100</td>
            <td>0.15</td>
          </tr>
          <tr>
            <td>ü•à 2nd</td>
            <td>0.6398</td>
            <td>¬±0.0015</td>
            <td>100</td>
            <td>0.10</td>
          </tr>
          <tr>
            <td>ü•â 3rd</td>
            <td>0.6387</td>
            <td>¬±0.0003</td>
            <td>100</td>
            <td>0.05</td>
          </tr>
          <tr>
            <td>4th</td>
            <td>0.6375</td>
            <td>¬±0.0020</td>
            <td>50</td>
            <td>0.15</td>
          </tr>
          <tr>
            <td>5th</td>
            <td>0.6374</td>
            <td>¬±0.0021</td>
            <td>50</td>
            <td>0.10</td>
          </tr>
        </tbody>
      </table>

      <div class="success-box" style="margin-top:16px">
        <strong>üí° Insight:</strong> numLeaves=100 consistently performs best across all learning rates. Higher learningRate (0.15) v·ªõi low variance (¬±0.0008) ‚Üí stable & optimal.
      </div>
    </div>

    <!-- Detailed Results -->
    <div class="grid">
      <div class="card">
        <h2>üìà Final Model Metrics (Best Params)</h2>
        <p class="muted">Trained v·ªõi numLeaves=100, learningRate=0.15:</p>
        <table>
          <thead>
            <tr><th>Metric</th><th>Value</th></tr>
          </thead>
          <tbody>
            <tr><td>Validation AUC-PR</td><td><strong>0.6315</strong></td></tr>
            <tr><td>Validation AUC-ROC</td><td><strong>0.8376</strong></td></tr>
            <tr><td>Precision</td><td><strong>80.79%</strong></td></tr>
            <tr><td>Recall</td><td><strong>56.84%</strong></td></tr>
            <tr><td>F1-Score</td><td><strong>58.70%</strong></td></tr>
          </tbody>
        </table>

        <h3>Confusion Matrix (Validation Set)</h3>
        <table>
          <thead>
            <tr><th></th><th>Predicted Neg</th><th>Predicted Pos</th></tr>
          </thead>
          <tbody>
            <tr><td><strong>Actual Neg</strong></td><td>169,012 (TN)</td><td>208,253 (FP)</td></tr>
            <tr><td><strong>Actual Pos</strong></td><td>7,881 (FN)</td><td>115,570 (TP)</td></tr>
          </tbody>
        </table>

        <div class="muted" style="margin-top:12px">
          <strong>Total Val Samples:</strong> 500,716 (10% of 5M training data)
        </div>
      </div>

      <div class="card">
        <h2>üìä Cross-Validation Analysis</h2>
        <p class="muted">Best configuration (numLeaves=100, lr=0.15) across 3 folds:</p>
        <table>
          <thead>
            <tr><th>Fold</th><th>AUC-PR</th><th>Samples</th></tr>
          </thead>
          <tbody>
            <tr><td>Fold 1/3</td><td>0.6424</td><td>~1.5M</td></tr>
            <tr><td>Fold 2/3</td><td>0.6407</td><td>~1.5M</td></tr>
            <tr><td>Fold 3/3</td><td>0.6419</td><td>~1.5M</td></tr>
            <tr style="background:#f3f4f6"><td><strong>Mean ¬± Std</strong></td><td><strong>0.6417 ¬± 0.0008</strong></td><td>4.5M total</td></tr>
          </tbody>
        </table>

        <div class="success-box" style="margin-top:16px">
          <strong>‚úì Low variance (¬±0.0008):</strong> Model stable, kh√¥ng b·ªã overfitting specific fold. Generalization t·ªët!
        </div>
      </div>
    </div>

    <!-- Training Configuration -->
    <div class="card card-full">
      <h2>‚öôÔ∏è Training Configuration Details</h2>
      <div class="grid" style="grid-template-columns:1fr 1fr">
        <div>
          <h3>Dataset Statistics</h3>
          <table>
            <tbody>
              <tr><td>Total Available</td><td>15,593,034 samples</td></tr>
              <tr><td>Training Used</td><td>5,000,000 samples (32%)</td></tr>
              <tr><td>Train Split</td><td>4,499,284 (90%)</td></tr>
              <tr><td>Val Split</td><td>500,716 (10%)</td></tr>
              <tr><td>Test Set</td><td>1,735,280 samples</td></tr>
              <tr><td>Feature Dimension</td><td>10,017 (10K TF-IDF + 17 numeric)</td></tr>
            </tbody>
          </table>
        </div>
        <div>
          <h3>LightGBM Parameters</h3>
          <table>
            <tbody>
              <tr><td>numLeaves</td><td>100 (best from tuning)</td></tr>
              <tr><td>learningRate</td><td>0.15 (best from tuning)</td></tr>
              <tr><td>minDataInLeaf</td><td>50</td></tr>
              <tr><td>featureFraction</td><td>0.75 (75% features/tree)</td></tr>
              <tr><td>baggingFraction</td><td>0.75 (75% samples/iter)</td></tr>
              <tr><td>lambdaL1 (L1 reg)</td><td>0.1</td></tr>
              <tr><td>lambdaL2 (L2 reg)</td><td>0.1</td></tr>
              <tr><td>Class Weight</td><td>3.054 (auto-computed)</td></tr>
            </tbody>
          </table>
        </div>
      </div>

      <div class="note" style="margin-top:16px">
        <strong>üí° Why limit to 5M samples?</strong><br>
        Training v·ªõi full 15.6M samples takes 8-10 hours. V·ªõi deadline 12 gi·ªù, ch·ªçn 5M samples (32%) ƒë·ªÉ:
        <ul style="margin:8px 0">
          <li>‚úì Auto-tuning ho√†n th√†nh trong 2.5 gi·ªù (9 combos √ó 3 folds)</li>
          <li>‚úì ƒê·ªß data cho model h·ªçc patterns (5M >> 1M baseline V6)</li>
          <li>‚úì C√≤n th·ªùi gian cho prediction & submission (1-2 gi·ªù)</li>
        </ul>
      </div>
    </div>

    <!-- Feature Importance -->
    <div class="card card-full">
      <h2>üéØ Feature Analysis (10,017 Features)</h2>
      <p>Model s·ª≠ d·ª•ng <strong>10,017 features</strong> t·ª´ feature_pipeline_v2:</p>
      
      <h3>Feature Breakdown:</h3>
      <table>
        <thead>
          <tr><th>Category</th><th>Count</th><th>Examples</th><th>Purpose</th></tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>TF-IDF Text Features</strong></td>
            <td>10,000</td>
            <td>tf_awesome, tf_love, tf_great, tf_waste, tf_terrible, ...</td>
            <td>Capture semantic meaning t·ª´ review text (unigrams)</td>
          </tr>
          <tr>
            <td><strong>Numeric Features</strong></td>
            <td>17</td>
            <td>review_length, star_rating, user_review_count, product_avg_rating, ...</td>
            <td>Metadata v·ªÅ user, product, review quality</td>
          </tr>
        </tbody>
      </table>

      <h3>Top 10 Numeric Features (by importance):</h3>
      <table>
        <thead>
          <tr><th>Rank</th><th>Feature</th><th>Type</th><th>Explanation</th></tr>
        </thead>
        <tbody>
          <tr>
            <td>1</td>
            <td><code>user_helpful_ratio</code></td>
            <td>User behavior</td>
            <td>% helpful reviews c·ªßa user - signal m·∫°nh nh·∫•t</td>
          </tr>
          <tr>
            <td>2</td>
            <td><code>product_helpful_ratio</code></td>
            <td>Product aggregate</td>
            <td>% helpful reviews c·ªßa product - ch·∫•t l∆∞·ª£ng product</td>
          </tr>
          <tr>
            <td>3</td>
            <td><code>review_length</code></td>
            <td>Review quality</td>
            <td>S·ªë k√Ω t·ª± - reviews d√†i th∆∞·ªùng informative h∆°n</td>
          </tr>
          <tr>
            <td>4</td>
            <td><code>star_rating</code></td>
            <td>Review quality</td>
            <td>1-5 stars - extreme ratings (1 or 5) thu h√∫t votes</td>
          </tr>
          <tr>
            <td>5</td>
            <td><code>user_review_count</code></td>
            <td>User behavior</td>
            <td>S·ªë reviews c·ªßa user - experienced reviewers ƒë√°ng tin</td>
          </tr>
          <tr>
            <td>6</td>
            <td><code>product_review_count</code></td>
            <td>Product aggregate</td>
            <td>S·ªë reviews c·ªßa product - popularity indicator</td>
          </tr>
          <tr>
            <td>7</td>
            <td><code>user_avg_rating</code></td>
            <td>User behavior</td>
            <td>Average rating c·ªßa user - harsh vs lenient reviewer</td>
          </tr>
          <tr>
            <td>8</td>
            <td><code>product_avg_rating</code></td>
            <td>Product aggregate</td>
            <td>Average rating c·ªßa product - quality signal</td>
          </tr>
          <tr>
            <td>9</td>
            <td><code>review_length_log</code></td>
            <td>Review quality</td>
            <td>log(review_length) - normalize skewed distribution</td>
          </tr>
          <tr>
            <td>10</td>
            <td><code>rating_deviation</code></td>
            <td>Review quality</td>
            <td>|user_rating - product_avg_rating| - controversial reviews</td>
          </tr>
        </tbody>
      </table>

      <div class="note" style="margin-top:16px">
        <strong>üí° Feature Importance Insight:</strong><br>
        - <strong>User behavior</strong> (helpful_ratio, review_count) l√† predictors m·∫°nh nh·∫•t<br>
        - <strong>Product aggregates</strong> (helpful_ratio, avg_rating) c≈©ng r·∫•t quan tr·ªçng<br>
        - <strong>TF-IDF features</strong> (10K terms) capture semantic meaning nh∆∞ng individual weight th·∫•p (spread across many terms)<br>
        - <strong>Review quality</strong> (length, rating) l√† moderate predictors
      </div>
    </div>

    <!-- Training Timeline -->
    <div class="card card-full">
      <h2>‚è±Ô∏è Training Timeline & Performance</h2>
      <table>
        <thead>
          <tr><th>Phase</th><th>Duration</th><th>Operations</th><th>Result</th></tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>Data Loading</strong></td>
            <td>~2 min</td>
            <td>Load 15.6M samples t·ª´ HDFS, limit to 5M, split 90/10</td>
            <td>‚úì 4.5M train, 500K val</td>
          </tr>
          <tr>
            <td><strong>Grid Search CV</strong></td>
            <td>~2.5 hours</td>
            <td>9 combos √ó 3 folds = 27 training runs</td>
            <td>‚úì Best: numLeaves=100, lr=0.15</td>
          </tr>
          <tr>
            <td><strong>Final Training</strong></td>
            <td>~5 min</td>
            <td>Retrain v·ªõi best params tr√™n 4.5M samples</td>
            <td>‚úì Val AUC-PR: 0.6315</td>
          </tr>
          <tr>
            <td><strong>Model Saving</strong></td>
            <td>~30 sec</td>
            <td>Save to HDFS + generate reports (JSON/CSV/TXT)</td>
            <td>‚úì Model at lightgbm_v7_auto</td>
          </tr>
          <tr style="background:#f3f4f6">
            <td><strong>Total</strong></td>
            <td><strong>~2h 40m</strong></td>
            <td>Start: 16:04, End: 18:50</td>
            <td><strong>‚úì Auto-tuning Complete</strong></td>
          </tr>
        </tbody>
      </table>

      <div class="success-box" style="margin-top:16px">
        <strong>üéØ Efficiency:</strong> Auto-tuning ho√†n th√†nh trong 2.7 gi·ªù (thay v√¨ 8-10 gi·ªù v·ªõi full data). ƒê·ªß th·ªùi gian cho prediction & submission trong deadline 12 gi·ªù!
      </div>
    </div>

    <!-- Key Learnings -->
    <div class="card card-full">
      <h2>ÔøΩ Key Learnings & Insights</h2>
      
      <h3>1Ô∏è‚É£ Auto-Tuning Effectiveness</h3>
      <div class="feature-list">
        <p><strong>‚úì Systematic Search:</strong> Grid search t√¨m ƒë∆∞·ª£c optimal params (numLeaves=100, lr=0.15) m√† manual tuning kh√≥ ph√°t hi·ªán.</p>
        <p><strong>‚úì Cross-Validation:</strong> 3-fold CV v·ªõi low variance (¬±0.0008) ‚Üí model stable, kh√¥ng b·ªã overfitting.</p>
        <p><strong>‚úì numLeaves Impact:</strong> 100 leaves consistently outperforms 31 & 50 ‚Üí model c·∫ßn complexity ƒë·ªÉ h·ªçc 10K features.</p>
        <p><strong>‚úì Learning Rate:</strong> 0.15 (highest tested) cho best result ‚Üí faster convergence v·ªõi regularization ƒë·ªß m·∫°nh.</p>
      </div>

      <h3>2Ô∏è‚É£ Data Strategy</h3>
      <div class="feature-list">
        <p><strong>‚ö†Ô∏è More Data ‚â† Always Better:</strong> V7 v·ªõi 5M samples (AUC-PR 0.6315) kh√¥ng t·ªët h∆°n V6 v·ªõi 1M samples (AUC-PR 0.6444).</p>
        <p><strong>Hypothesis:</strong> 5M samples c√≥ nhi·ªÅu noise h∆°n ‚Üí model h·ªçc c·∫£ patterns l·∫´n noise.</p>
        <p><strong>Trade-off:</strong> 5M samples ƒë·ªß cho auto-tuning nhanh (2.7h) nh∆∞ng performance kh√¥ng optimal.</p>
        <p><strong>Next Step:</strong> Th·ª≠ 2-3M samples v·ªõi best params ‚Üí balance gi·ªØa data quality & quantity.</p>
      </div>

      <h3>3Ô∏è‚É£ Feature Engineering Impact</h3>
      <div class="feature-list">
        <p><strong>TF-IDF Dominance:</strong> 10K text features chi·∫øm 99.8% feature space ‚Üí capture semantic meaning t·ªët.</p>
        <p><strong>Metadata Power:</strong> 17 numeric features (0.2%) nh∆∞ng c√≥ predictive power cao (user_helpful_ratio, product_helpful_ratio).</p>
        <p><strong>Combination Effect:</strong> Text + metadata synergy ‚Üí model h·ªçc c·∫£ content l·∫´n context.</p>
      </div>

      <h3>4Ô∏è‚É£ Class Imbalance Handling</h3>
      <div class="feature-list">
        <p><strong>Effective Weight:</strong> Scale pos weight = 3.054 ‚Üí balance loss function cho imbalanced data.</p>
        <p><strong>Stratified CV:</strong> Gi·ªØ ratio 1:3 trong m·ªçi fold ‚Üí reliable validation metrics.</p>
        <p><strong>Metric Choice:</strong> AUC-PR (kh√¥ng ph·∫£i accuracy) ‚Üí ph√π h·ª£p v·ªõi imbalanced classification.</p>
      </div>
    </div>

    <!-- Model Comparison -->
    <div class="card card-full">
      <h2>üìä Model Version Comparison</h2>
      <table>
        <thead>
          <tr><th>Version</th><th>Training Samples</th><th>numLeaves</th><th>learningRate</th><th>Val AUC-PR</th><th>Notes</th></tr>
        </thead>
        <tbody>
          <tr>
            <td>V4</td>
            <td>1M</td>
            <td>128</td>
            <td>0.035</td>
            <td><strong>0.6448</strong></td>
            <td>Manual tuning, small data</td>
          </tr>
          <tr>
            <td>V5</td>
            <td>1M</td>
            <td>50</td>
            <td>0.05</td>
            <td>0.6363</td>
            <td>Underfit (too simple)</td>
          </tr>
          <tr>
            <td>V6</td>
            <td>1M</td>
            <td>100</td>
            <td>0.03</td>
            <td><strong>0.6444</strong></td>
            <td>Balanced complexity</td>
          </tr>
          <tr>
            <td>V7 Manual</td>
            <td>5M</td>
            <td>120</td>
            <td>0.03</td>
            <td>0.6327</td>
            <td>‚ùå More data but worse (noise)</td>
          </tr>
          <tr style="background:#d1fae5">
            <td><strong>V7 Auto-Tune</strong></td>
            <td><strong>5M</strong></td>
            <td><strong>100</strong></td>
            <td><strong>0.15</strong></td>
            <td><strong>0.6315</strong></td>
            <td><strong>‚úì Best params from CV</strong></td>
          </tr>
        </tbody>
      </table>

      <div class="note" style="margin-top:16px">
        <strong>‚ö†Ô∏è Performance Paradox:</strong><br>
        - V4/V6 (1M samples) ƒë·∫°t 0.644-0.645 AUC-PR<br>
        - V7 (5M samples) ch·ªâ ƒë·∫°t 0.631-0.633 AUC-PR<br>
        - <strong>Conclusion:</strong> More data kh√¥ng ƒë·∫£m b·∫£o better model. Data quality > data quantity. 5M samples c√≥ nhi·ªÅu noise/outliers ‚Üí model h·ªçc patterns + noise.
      </div>

      <h3>Best Configuration (from Auto-Tuning):</h3>
      <div class="feature-list">
        <ul>
          <li><code>numLeaves = 100</code> ‚Üí Optimal complexity cho 10K features</li>
          <li><code>learningRate = 0.15</code> ‚Üí Fast convergence v·ªõi regularization</li>
          <li><code>minDataInLeaf = 50</code> ‚Üí Prevent overfitting</li>
          <li><code>featureFraction = 0.75</code> ‚Üí Random feature selection</li>
          <li><code>baggingFraction = 0.75</code> ‚Üí Bagging for stability</li>
          <li><code>lambdaL1 = 0.1, lambdaL2 = 0.1</code> ‚Üí Regularization</li>
        </ul>
      </div>
    </div>

    <!-- LightGBM Parameters Detailed -->
    <div class="card card-full">
      <h2>‚öôÔ∏è LightGBM Hyperparameters - Chi Ti·∫øt</h2>
      <p>Gi·∫£i th√≠ch t·ª´ng hyperparameter trong LightGBMClassifier:</p>

      <table>
        <thead>
          <tr><th>Parameter</th><th>Value</th><th>√ù Nghƒ©a</th><th>Trade-off</th></tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>numLeaves</strong></td>
            <td>100</td>
            <td>S·ªë l√° t·ªëi ƒëa m·ªói c√¢y. C√†ng cao ‚Üí c√¢y ph·ª©c t·∫°p h∆°n ‚Üí h·ªçc patterns chi ti·∫øt h∆°n</td>
            <td>High: overfitting risk | Low: underfitting (too simple)</td>
          </tr>
          <tr>
            <td><strong>learningRate</strong></td>
            <td>0.15</td>
            <td>Step size trong gradient descent. M·ªói c√¢y ƒë√≥ng g√≥p learningRate √ó prediction v√†o t·ªïng</td>
            <td>High: fast convergence, overfitting | Low: slow, better generalization</td>
          </tr>
          <tr>
            <td><strong>numIterations</strong></td>
            <td>1500</td>
            <td>S·ªë c√¢y t·ªëi ƒëa (boosting rounds). C√†ng nhi·ªÅu ‚Üí model m·∫°nh h∆°n nh∆∞ng risk overfit</td>
            <td>High: powerful but overfit | Low: underfit (not enough trees)</td>
          </tr>
          <tr>
            <td><strong>earlyStoppingRound</strong></td>
            <td>200</td>
            <td>D·ª´ng training n·∫øu validation metric kh√¥ng c·∫£i thi·ªán sau 200 rounds ‚Üí prevent overfit</td>
            <td>High: more patient (may overfit) | Low: stop too early (underfit)</td>
          </tr>
          <tr>
            <td><strong>minDataInLeaf</strong></td>
            <td>50</td>
            <td>S·ªë samples t·ªëi thi·ªÉu m·ªói l√°. C√†ng cao ‚Üí c√¢y t·ªïng qu√°t h∆°n, √≠t overfit h∆°n</td>
            <td>High: generalization, underfit | Low: specific, overfit</td>
          </tr>
          <tr>
            <td><strong>featureFraction</strong></td>
            <td>0.75</td>
            <td>Random ch·ªçn 75% features m·ªói iteration. Gi·∫£m correlation gi·ªØa c√°c c√¢y ‚Üí ensemble t·ªët h∆°n</td>
            <td>High: use more features | Low: more randomness, prevent overfit</td>
          </tr>
          <tr>
            <td><strong>baggingFraction</strong></td>
            <td>0.75</td>
            <td>Random sample 75% data m·ªói iteration (bagging). TƒÉng diversity ‚Üí robust model</td>
            <td>High: use more data | Low: more bagging, prevent overfit</td>
          </tr>
          <tr>
            <td><strong>maxDepth</strong></td>
            <td>-1</td>
            <td>Max tree depth (-1 = unlimited). Limit depth ‚Üí simpler trees ‚Üí less overfit</td>
            <td>High/unlimited: complex trees | Low: simple, generalization</td>
          </tr>
          <tr>
            <td><strong>lambdaL1</strong></td>
            <td>0.1</td>
            <td>L1 regularization (Lasso). Penalize sum of absolute weights ‚Üí feature selection (sparse)</td>
            <td>High: strong penalty, sparse | Low: weak penalty, use all features</td>
          </tr>
          <tr>
            <td><strong>lambdaL2</strong></td>
            <td>0.1</td>
            <td>L2 regularization (Ridge). Penalize sum of squared weights ‚Üí weight decay (smooth)</td>
            <td>High: strong penalty, smooth | Low: weak penalty, large weights OK</td>
          </tr>
          <tr>
            <td><strong>objective</strong></td>
            <td>binary</td>
            <td>Binary classification v·ªõi log loss (binary cross-entropy)</td>
            <td>N/A (task-specific)</td>
          </tr>
          <tr>
            <td><strong>isUnbalance</strong></td>
            <td>True</td>
            <td>Enable imbalance handling (auto adjust positive weight d·ª±a tr√™n class distribution)</td>
            <td>True: handle imbalance | False: treat equally</td>
          </tr>
          <tr>
            <td><strong>classWeight</strong></td>
            <td>"0:1,1:3.054"</td>
            <td>Manual class weights. Class 0 (neg) weight=1, Class 1 (pos) weight=3.054</td>
            <td>High pos weight: focus on minority | Equal: no imbalance handling</td>
          </tr>
        </tbody>
      </table>

      <h3>C√¥ng Th·ª©c Loss Function (Binary Cross-Entropy)</h3>
      <div class="code-block">
        <pre># Binary Cross-Entropy v·ªõi class weights
Loss = - (1/N) √ó Œ£ [w_i √ó y_i √ó log(p_i) + (1 - y_i) √ó log(1 - p_i)]

where:
  N = s·ªë samples
  y_i = ground truth label (0 ho·∫∑c 1)
  p_i = predicted probability (sigmoid output)
  w_i = sample weight (pos: 3.054, neg: 1.0)

# Effect c·ªßa class weight:
- Positive samples (y=1): loss nh√¢n v·ªõi 3.054 ‚Üí model focus h∆°n
- Negative samples (y=0): loss nh√¢n v·ªõi 1.0 ‚Üí ·∫£nh h∆∞·ªüng b√¨nh th∆∞·ªùng</pre>
      </div>

      <h3>Regularization Explained</h3>
      <div class="feature-list">
        <p><strong>L1 Regularization (Lasso):</strong></p>
        <div class="code-block">
          <pre>Loss_total = Loss + Œª‚ÇÅ √ó Œ£|w_i|

Effect: 
- Penalize absolute values c·ªßa weights
- Force weights v·ªÅ 0 ‚Üí feature selection (sparse model)
- Useful khi c√≥ nhi·ªÅu features kh√¥ng quan tr·ªçng (10K TF-IDF)</pre>
        </div>

        <p><strong>L2 Regularization (Ridge):</strong></p>
        <div class="code-block">
          <pre>Loss_total = Loss + Œª‚ÇÇ √ó Œ£(w_i)¬≤

Effect:
- Penalize squared values c·ªßa weights
- Shrink weights v·ªÅ g·∫ßn 0 (kh√¥ng v·ªÅ ƒë√∫ng 0)
- Prefer small, distributed weights ‚Üí smooth model</pre>
        </div>

        <p><strong>Combined (Elastic Net):</strong></p>
        <div class="code-block">
          <pre>Loss_total = Loss + Œª‚ÇÅ √ó Œ£|w_i| + Œª‚ÇÇ √ó Œ£(w_i)¬≤

lambdaL1=0.1, lambdaL2=0.1 ‚Üí mild regularization
- Balance gi·ªØa feature selection (L1) v√† weight smoothing (L2)
- Prevent overfitting v·ªõi 10K features</pre>
        </div>
      </div>

      <h3>Gradient Descent trong LightGBM</h3>
      <div class="code-block">
        <pre># Additive model (boosting)
F_m(x) = F_(m-1)(x) + Œ∑ √ó h_m(x)

where:
  F_m(x) = prediction sau m trees
  Œ∑ = learningRate (0.15)
  h_m(x) = c√¢y th·ª© m (h·ªçc t·ª´ residual/gradient)

# Training process:
1. Initialize: F_0(x) = log(pos/neg) = log(1,109,945 / 3,389,339)
2. For m = 1 to numIterations (1500):
     a. Compute gradient: g_i = ‚àÇLoss/‚àÇF_(m-1)(x_i)
     b. Build tree h_m(x) to fit gradient g
     c. Update: F_m(x) = F_(m-1)(x) + 0.15 √ó h_m(x)
     d. Check early stopping (AUC-PR kh√¥ng tƒÉng sau 200 rounds)
3. Final prediction: p = sigmoid(F_1500(x))</pre>
      </div>

      <div class="success-box" style="margin-top:16px">
        <strong>üéØ T√≥m t·∫Øt Best Config:</strong><br>
        - <strong>numLeaves=100, lr=0.15:</strong> Balance gi·ªØa complexity v√† generalization<br>
        - <strong>featureFraction=0.75, baggingFraction=0.75:</strong> Random subsampling ‚Üí ensemble diversity<br>
        - <strong>lambdaL1=0.1, lambdaL2=0.1:</strong> Mild regularization ‚Üí prevent overfit 10K features<br>
        - <strong>classWeight=3.054:</strong> Handle 1:3 imbalance ‚Üí focus on minority class<br>
        - <strong>earlyStoppingRound=200:</strong> Automatic stop khi validation plateau
      </div>
    </div>

    <!-- Output Files -->
    <div class="card card-full">
      <h2>üìÅ Output Artifacts</h2>
      
      <h3>Model Files (HDFS)</h3>
      <table>
        <thead>
          <tr><th>Path</th><th>Description</th><th>Usage</th></tr>
        </thead>
        <tbody>
          <tr>
            <td><code>hdfs://.../lightgbm_v7_auto/</code></td>
            <td>Spark MLlib LightGBM model (directory)</td>
            <td>Load trong predict_pipeline_v2.py</td>
          </tr>
          <tr>
            <td><code>hdfs://.../lightgbm_v7_auto/metadata/</code></td>
            <td>Model metadata (schema, params)</td>
            <td>Spark pipeline metadata</td>
          </tr>
          <tr>
            <td><code>hdfs://.../lightgbm_v7_auto/stages/</code></td>
            <td>Pipeline stages (LightGBM booster)</td>
            <td>Actual model weights & trees</td>
          </tr>
        </tbody>
      </table>

      <h3>Training Reports (Local)</h3>
      <table>
        <thead>
          <tr><th>File</th><th>Description</th><th>Content</th></tr>
        </thead>
        <tbody>
          <tr>
            <td><code>training_report_20251101_185019.json</code></td>
            <td>Comprehensive training report (JSON)</td>
            <td>Hyperparameters, metrics, confusion matrix, CV results</td>
          </tr>
          <tr>
            <td><code>training_report_20251101_185019_metrics.csv</code></td>
            <td>Metrics table (CSV format)</td>
            <td>AUC-PR, AUC-ROC, Precision, Recall, F1 per fold</td>
          </tr>
          <tr>
            <td><code>training_report_20251101_185019_summary.txt</code></td>
            <td>Human-readable summary (TXT)</td>
            <td>Executive summary, best params, top configs</td>
          </tr>
        </tbody>
      </table>

      <div class="note" style="margin-top:16px">
        <strong>üìù Report Contents:</strong><br>
        - <strong>CV Results:</strong> 9 configs √ó 3 folds = 27 AUC-PR scores<br>
        - <strong>Best Params:</strong> numLeaves=100, learningRate=0.15<br>
        - <strong>Final Metrics:</strong> Val AUC-PR=0.6315, Precision=80.79%, Recall=56.84%<br>
        - <strong>Confusion Matrix:</strong> TP=115,570, TN=169,012, FP=208,253, FN=7,881<br>
        - <strong>Training Info:</strong> 5M samples, 10,017 features, class weight=3.054
      </div>
    </div>

    <!-- Next Steps -->
    <div class="card card-full">
      <h2>üöÄ Next Steps - Prediction & Submission</h2>
      
      <div class="success-box">
        <strong>‚úÖ Auto-Tuning HO√ÄN TH√ÄNH!</strong><br>
        Best hyperparameters ƒë√£ t√¨m ƒë∆∞·ª£c: <strong>numLeaves=100, learningRate=0.15</strong>. Model trained v√† saved t·∫°i HDFS. B∆∞·ªõc ti·∫øp theo: Run prediction tr√™n test set ‚Üí Generate submission.csv ‚Üí Submit!
      </div>

      <h3>üìã Step 1: Run Prediction Pipeline</h3>
      
      <div class="code-block">
        <pre># Ch·∫°y inference v·ªõi model V7 auto-tuned
spark-submit \
  --master local[*] \
  --packages com.microsoft.azure:synapseml-lightgbm_2.12:1.0.7 \
  --driver-memory 11g \
  --executor-memory 11g \
  "code_v2/models/predict_pipeline_v2.py" \
  --model_path "hdfs://localhost:9000/output_v2/models/lightgbm_v7_auto" \
  --test "hdfs://localhost:9000/output_v2/features_test_v4" \
  --out "hdfs://localhost:9000/output_v2/predictions_v7_auto" \
  --debug_samples 100

# Download submission.csv t·ª´ HDFS
hdfs dfs -get hdfs://localhost:9000/output_v2/predictions_v7_auto/submission.csv \
  output/submission_v7_auto.csv</pre>
      </div>

      <h3>‚úÖ Validation Checklist</h3>
      <ul>
        <li>‚úì <strong>Row Count:</strong> 1,735,280 rows (= test set size)</li>
        <li>‚úì <strong>Columns:</strong> review_id (string), probability_helpful (double)</li>
        <li>‚úì <strong>No NULLs:</strong> All predictions valid</li>
        <li>‚úì <strong>Probability Range:</strong> [0.0, 1.0] (sigmoid output)</li>
        <li>‚úì <strong>Format:</strong> CSV with header</li>
      </ul>

      <h3>üìä Decision: V7 Baseline vs V7 Auto-Tune</h3>
      <table>
        <thead>
          <tr><th>Model</th><th>Val AUC-PR</th><th>Params</th><th>Status</th></tr>
        </thead>
        <tbody>
          <tr>
            <td>V7 Baseline (Manual)</td>
            <td>0.6327</td>
            <td>numLeaves=120, lr=0.03</td>
            <td>‚úì Submission ready</td>
          </tr>
          <tr style="background:#d1fae5">
            <td><strong>V7 Auto-Tune</strong></td>
            <td><strong>0.6315</strong></td>
            <td><strong>numLeaves=100, lr=0.15</strong></td>
            <td><strong>‚è≥ Need prediction</strong></td>
          </tr>
        </tbody>
      </table>

      <div class="note" style="margin-top:16px">
        <strong>ü§î Which to Submit?</strong><br>
        - V7 Baseline: AUC-PR 0.6327 (slightly better validation)<br>
        - V7 Auto-Tune: AUC-PR 0.6315 (from CV, more robust)<br>
        <br>
        <strong>Recommendation:</strong> Submit <strong>V7 Baseline (submission_v7.csv)</strong> v√¨:
        <ul>
          <li>‚úì Higher validation AUC-PR (0.6327 > 0.6315)</li>
          <li>‚úì Already generated & validated</li>
          <li>‚úì Save time (auto-tune prediction takes ~10 min)</li>
        </ul>
        <br>
        <strong>Alternative:</strong> Generate V7 Auto-Tune prediction ‚Üí compare probability distributions ‚Üí submit better one.
      </div>

      <h3>üéØ Expected Timeline</h3>
      <table>
        <thead>
          <tr><th>Task</th><th>Duration</th><th>Action</th></tr>
        </thead>
        <tbody>
          <tr><td>Run Prediction (V7 Auto)</td><td>~10 min</td><td>spark-submit predict_pipeline_v2.py</td></tr>
          <tr><td>Download CSV</td><td>~1 min</td><td>hdfs dfs -get submission.csv</td></tr>
          <tr><td>Compare Models</td><td>~5 min</td><td>Check prob distributions, stats</td></tr>
          <tr><td>Final Submission</td><td>~5 min</td><td>Upload to competition platform</td></tr>
          <tr style="background:#f3f4f6"><td><strong>Total</strong></td><td><strong>~20 min</strong></td><td><strong>Complete pipeline</strong></td></tr>
        </tbody>
      </table>
    </div>

    <!-- Summary Box -->
    <div class="card card-full" style="background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff">
      <h2 style="color:#fff;border-bottom:3px solid #fff">üéì T√≥m T·∫Øt K·ªπ Thu·∫≠t</h2>
      
      <div style="display:grid;grid-template-columns:1fr 1fr;gap:20px;margin-top:16px">
        <div>
          <h3 style="color:#fff">Thu·∫≠t To√°n</h3>
          <ul style="color:#fff">
            <li><strong>LightGBM:</strong> Gradient Boosting v·ªõi histogram-based learning</li>
            <li><strong>Grid Search:</strong> 9 combinations (3√ó3 grid)</li>
            <li><strong>3-Fold CV:</strong> Stratified cross-validation (27 runs)</li>
            <li><strong>Loss Function:</strong> Binary cross-entropy</li>
            <li><strong>Optimization:</strong> Gradient descent with L1/L2 regularization</li>
          </ul>
        </div>
        <div>
          <h3 style="color:#fff">K·ªπ Thu·∫≠t</h3>
          <ul style="color:#fff">
            <li><strong>Distributed Training:</strong> PySpark + SynapseML LightGBM</li>
            <li><strong>Class Imbalance:</strong> Scale pos weight = 3.054</li>
            <li><strong>Feature Engineering:</strong> TF-IDF (10K) + Metadata (17)</li>
            <li><strong>Data Strategy:</strong> Limit 5M/15.6M samples (time optimization)</li>
            <li><strong>Evaluation:</strong> AUC-PR (ph√π h·ª£p v·ªõi imbalanced data)</li>
          </ul>
        </div>
      </div>

      <div style="margin-top:20px;padding:16px;background:rgba(255,255,255,0.2);border-radius:8px">
        <strong style="font-size:1.1em">üèÜ Best Hyperparameters:</strong><br>
        <code style="color:#fff">numLeaves=100 | learningRate=0.15 | minDataInLeaf=50 | featureFraction=0.75 | baggingFraction=0.75 | lambdaL1=0.1 | lambdaL2=0.1</code>
      </div>
    </div>

    <!-- Footer -->
    <div style="text-align:center;padding:24px;color:#fff;font-size:0.9em">
      <p><strong>Day 3 V2 Auto-Tuning Report</strong> ‚Äî Generated on November 1, 2025</p>
      <p>Authors: V√µ Th·ªã Di·ªÖm Thanh (Model Training) ‚Ä¢ L√™ ƒêƒÉng Ho√†ng Tu·∫•n (Infrastructure)</p>
      <p style="margin-top:8px">
        <span class="badge badge-success">CV AUC-PR: 0.6417</span>
        <span class="badge badge-info">Val AUC-PR: 0.6315</span>
        <span class="badge badge-info">27 Training Runs</span>
        <span class="badge badge-info">10,017 Features</span>
      </p>
    </div>
  </div>
      </div>
      <a class="backtop" href="#top">‚Üë Back to top</a>
    </section>
    
    <section id="sec4" class="section">
      <h2>Section 4: Day 3 V2 - Visual Analysis & Final Report</h2>
      <div class="source-note">Source file: day3_v2_visual_analysis_report.html</div>
      <hr/>
      <div class="embedded-report">
        <div class="container">
    <div class="hero">
      <h1>üìä Day 3 V2 ‚Äî Visual Analysis & Final Report</h1>
      <p class="subtitle">V7 Baseline vs V7 Auto-tune ‚Äî Complete Statistical & Visual Comparison</p>
      <div style="margin-top:16px">
        <span class="badge badge-info">Date: November 1, 2025 @ 20:00</span>
        <span class="badge badge-success">Analysis Complete ‚úì</span>
        <span class="badge badge-info">Charts Generated ‚úì</span>
      </div>
    </div>

    <!-- Critical Alert -->
    <div class="card card-full">
      <h2>üö® CRITICAL: Duplicate Review IDs Warning</h2>
      <div class="danger-box">
        <h3 style="margin-top:0;color:#991b1b">‚ö†Ô∏è 83% Duplicate Review IDs in Both Submissions!</h3>
        
        <table>
          <thead>
            <tr><th>Metric</th><th>V7 Baseline</th><th>V7 Auto-tune</th><th>Status</th></tr>
          </thead>
          <tbody>
            <tr>
              <td><strong>Total Rows</strong></td>
              <td>1,735,280</td>
              <td>1,735,280</td>
              <td><span class="badge badge-info">Same</span></td>
            </tr>
            <tr>
              <td><strong>Unique IDs</strong></td>
              <td>294,010</td>
              <td>294,010</td>
              <td><span class="badge badge-info">Same</span></td>
            </tr>
            <tr>
              <td><strong>Duplicates</strong></td>
              <td>1,441,270</td>
              <td>1,441,270</td>
              <td><span class="badge badge-danger">83.06%</span></td>
            </tr>
            <tr>
              <td><strong>Avg Repeats/ID</strong></td>
              <td>~5.9 times</td>
              <td>~5.9 times</td>
              <td><span class="badge badge-warning">High</span></td>
            </tr>
          </tbody>
        </table>

        <h4>Root Cause:</h4>
        <ul>
          <li>Test data source (<code>features_test_v4</code> on HDFS) contains duplicate review_ids</li>
          <li>Prediction pipeline processes all rows ‚Üí generates duplicate predictions</li>
          <li>Same pattern in both models confirms it's a data issue, not model issue</li>
        </ul>

        <h4>‚ö° Action Required:</h4>
        <div class="warning-box">
          <strong>Before Submission:</strong><br>
          1. <strong>Check competition rules:</strong> Does it require 1 prediction per review_id?<br>
          2. <strong>If YES (unique required):</strong> Clean duplicates using <code>keep='first'</code> ‚Üí 1.73M ‚Üí 294K rows<br>
          3. <strong>If NO (duplicates OK):</strong> Submit as-is (1.73M rows)<br><br>
          
          <strong>Recommended cleaning script:</strong><br>
          <code>df.drop_duplicates(subset='review_id', keep='first').to_csv('submission_clean.csv', index=False)</code>
        </div>
      </div>
    </div>

    <!-- Executive Summary -->
    <div class="card card-full">
      <h2>üìà Executive Summary</h2>
      
      <div class="metric-grid">
        <div class="metric-box">
          <div class="label">Winner Model</div>
          <div class="value">V7 Baseline</div>
        </div>
        <div class="metric-box">
          <div class="label">Best AUC-PR</div>
          <div class="value">0.6327</div>
        </div>
        <div class="metric-box">
          <div class="label">Predictions Each</div>
          <div class="value">1.73M</div>
        </div>
        <div class="metric-box">
          <div class="label">Feature Dimension</div>
          <div class="value">10,017</div>
        </div>
      </div>

      <div class="success-box">
        <strong>‚úÖ Analysis Complete!</strong><br>
        - Both models trained successfully with 5M training samples<br>
        - Predictions generated for all 1.73M test rows<br>
        - Statistical analysis & visualization charts created<br>
        - <strong>Recommendation: Submit V7 Baseline FIRST</strong> (higher validation AUC-PR)
      </div>
    </div>

    <!-- Detailed Comparison -->
    <div class="card card-full">
      <h2>‚öñÔ∏è V7 Baseline vs V7 Auto-tune ‚Äî Detailed Comparison</h2>
      
      <div style="text-align:center;margin:20px 0">
        <span class="vs-badge">V7 BASELINE (WINNER) vs V7 AUTO-TUNE</span>
      </div>

      <h3>üèÜ Model Performance Metrics</h3>
      <table>
        <thead>
          <tr>
            <th>Metric</th>
            <th>V7 Baseline</th>
            <th>V7 Auto-tune</th>
            <th>Difference</th>
            <th>Winner</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>Validation AUC-PR</strong></td>
            <td><span class="badge badge-success">0.6327</span></td>
            <td><span class="badge badge-warning">0.6315</span></td>
            <td>+0.0012 (+0.19%)</td>
            <td>üèÜ Baseline</td>
          </tr>
          <tr>
            <td><strong>Validation AUC-ROC</strong></td>
            <td><span class="badge badge-success">0.8392</span></td>
            <td><span class="badge badge-warning">0.8376</span></td>
            <td>+0.0016 (+0.19%)</td>
            <td>üèÜ Baseline</td>
          </tr>
          <tr>
            <td><strong>Training Time</strong></td>
            <td><span class="badge badge-success">2.5 hours</span></td>
            <td><span class="badge badge-info">2.7 hours</span></td>
            <td>-0.2h faster</td>
            <td>üèÜ Baseline</td>
          </tr>
          <tr>
            <td><strong>numLeaves</strong></td>
            <td>120</td>
            <td>100</td>
            <td>+20 (higher capacity)</td>
            <td>‚Äî</td>
          </tr>
          <tr>
            <td><strong>learningRate</strong></td>
            <td>0.03</td>
            <td>0.15</td>
            <td>-0.12 (slower learning)</td>
            <td>‚Äî</td>
          </tr>
        </tbody>
      </table>

      <h3>üìä Prediction Statistics</h3>
      <table>
        <thead>
          <tr>
            <th>Statistic</th>
            <th>V7 Baseline</th>
            <th>V7 Auto-tune</th>
            <th>Observation</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>Mean Probability</strong></td>
            <td>0.5627</td>
            <td>0.5729</td>
            <td>Auto-tune predicts higher on average (+1.8%)</td>
          </tr>
          <tr>
            <td><strong>Median</strong></td>
            <td>0.5746</td>
            <td>0.5851</td>
            <td>Auto-tune more optimistic (+1.8%)</td>
          </tr>
          <tr>
            <td><strong>Std Dev</strong></td>
            <td>0.0670</td>
            <td>0.0773</td>
            <td>Auto-tune more spread out (+15.4%)</td>
          </tr>
          <tr>
            <td><strong>Min Probability</strong></td>
            <td>0.3669</td>
            <td>0.3467</td>
            <td>Auto-tune goes lower (-5.5%)</td>
          </tr>
          <tr>
            <td><strong>Max Probability</strong></td>
            <td>0.6543</td>
            <td>0.6792</td>
            <td>Auto-tune goes higher (+3.8%)</td>
          </tr>
          <tr>
            <td><strong>Range</strong></td>
            <td>0.2874</td>
            <td>0.3325</td>
            <td>Auto-tune wider range (+15.7%)</td>
          </tr>
          <tr>
            <td><strong>Q1 (25%)</strong></td>
            <td>0.5156</td>
            <td>0.5164</td>
            <td>Very similar (+0.2%)</td>
          </tr>
          <tr>
            <td><strong>Q3 (75%)</strong></td>
            <td>0.6192</td>
            <td>0.6405</td>
            <td>Auto-tune higher upper quartile (+3.4%)</td>
          </tr>
        </tbody>
      </table>

      <div class="info-box">
        <strong>üìä Key Observations:</strong><br>
        <ul>
          <li><strong>V7 Baseline:</strong> More conservative predictions (narrower range: 0.367-0.654), lower std (0.067)</li>
          <li><strong>V7 Auto-tune:</strong> More confident predictions (wider range: 0.347-0.679), higher std (0.077)</li>
          <li><strong>Correlation:</strong> Both models agree on relative ordering (high correlation expected)</li>
          <li><strong>Calibration:</strong> Baseline slightly under-predicts, Auto-tune slightly over-predicts</li>
        </ul>
      </div>
    </div>

    <!-- Visual Analysis Charts -->
    <div class="card card-full">
      <h2>üìä Visual Analysis ‚Äî Distribution Charts</h2>
      
      <div class="warning-box">
        <strong>‚ö†Ô∏è Note on Charts:</strong> Charts were generated sequentially, so the last generated charts (V7 Auto-tune) 
        are currently saved in <code>output_final/analysis/</code>. Both models have identical filenames, 
        so only V7 Auto-tune charts are preserved. To view V7 Baseline charts, re-run the analysis script 
        with V7 Baseline path.
      </div>

      <h3>üéØ V7 Auto-tune ‚Äî Distribution Analysis</h3>
      
      <div class="chart-container">
        <h4>4-Panel Distribution Chart</h4>
        <p><em>Histogram, Pie Chart, Boxplot, and Cumulative Distribution Function</em></p>
        <img src="../output_final/analysis/submission_distribution.png" alt="V7 Auto-tune Distribution" class="chart-img">
        
        <div class="info-box" style="text-align:left;margin-top:16px">
          <strong>Chart Insights:</strong><br>
          <ul>
            <li><strong>Histogram (Top-Left):</strong> Bell-shaped distribution centered around 0.57, slight left skew</li>
            <li><strong>Pie Chart (Top-Right):</strong> 4 bins showing probability ranges (Low/Med-Low/Med-High/High)</li>
            <li><strong>Boxplot (Bottom-Left):</strong> Shows median, quartiles, and outliers - very compact distribution</li>
            <li><strong>CDF (Bottom-Right):</strong> Cumulative distribution with key percentiles marked (25%, 50%, 75%, 90%)</li>
          </ul>
        </div>
      </div>

      <div class="chart-container">
        <h4>Simple Pie Chart ‚Äî Binary Classification</h4>
        <p><em>Not Helpful (prob < 0.5) vs Helpful (prob ‚â• 0.5)</em></p>
        <img src="../output_final/analysis/submission_pie_chart.png" alt="V7 Auto-tune Pie Chart" class="chart-img">
        
        <div class="info-box" style="text-align:left;margin-top:16px">
          <strong>Classification Split:</strong><br>
          Based on threshold 0.5, majority of predictions fall into "Helpful" category (prob ‚â• 0.5).
          This indicates model predicts most reviews as helpful, which aligns with the mean probability of ~0.57.
        </div>
      </div>

      <h3>üìÅ Generated Files</h3>
      <div class="code-block">
        <pre>output_final/analysis/
‚îú‚îÄ‚îÄ submission_distribution.png  (4-panel chart)
‚îú‚îÄ‚îÄ submission_pie_chart.png     (simple pie chart)
‚îî‚îÄ‚îÄ submission_statistics.json   (numeric stats)</pre>
      </div>

      <div class="success-box">
        <strong>‚úÖ Analysis Files Ready!</strong><br>
        All statistical analysis and visualization charts have been generated and saved to 
        <code>output_final/analysis/</code> directory.
      </div>
    </div>

    <!-- Model Selection Recommendation -->
    <div class="card card-full">
      <h2>üéØ Final Recommendation ‚Äî Which Model to Submit?</h2>
      
      <div class="compare-grid">
        <!-- V7 Baseline -->
        <div class="winner-card" style="padding:20px;border-radius:12px">
          <h3 style="color:#10b981;margin-top:0">üèÜ V7 Baseline (RECOMMENDED)</h3>
          
          <div class="metric-box" style="margin:12px 0">
            <div class="label">Validation AUC-PR</div>
            <div class="value">0.6327</div>
          </div>

          <h4>‚úÖ Advantages:</h4>
          <ul>
            <li><strong>Highest validation score</strong> (AUC-PR 0.6327, AUC-ROC 0.8392)</li>
            <li><strong>Faster training</strong> (2.5 hours vs 2.7 hours)</li>
            <li><strong>Manual tuning success</strong> (proven config from experience)</li>
            <li><strong>Conservative predictions</strong> (narrower range, less risky)</li>
            <li><strong>Lower std dev</strong> (0.067 vs 0.077, more stable)</li>
          </ul>

          <h4>üìÅ File to Submit:</h4>
          <div class="code-block">
            <pre>output_final/submission_v7.csv
Size: 53.77 MB
Rows: 1,735,280 (with duplicates)
Unique IDs: 294,010</pre>
          </div>

          <div class="success-box">
            <strong>üéØ Priority: SUBMIT FIRST</strong><br>
            V7 Baseline has the highest validation AUC-PR and most stable predictions.
          </div>
        </div>

        <!-- V7 Auto-tune -->
        <div class="runner-card" style="padding:20px;border-radius:12px">
          <h3 style="color:#3b82f6;margin-top:0">ü•à V7 Auto-tune (BACKUP)</h3>
          
          <div class="metric-box" style="margin:12px 0;background:linear-gradient(135deg,#3b82f6 0%,#1e40af 100%)">
            <div class="label">Validation AUC-PR</div>
            <div class="value">0.6315</div>
          </div>

          <h4>‚úÖ Advantages:</h4>
          <ul>
            <li><strong>More robust</strong> (from 27 CV runs, 3-fold √ó 9 configs)</li>
            <li><strong>Grid-searched params</strong> (systematic optimization)</li>
            <li><strong>Best CV mean</strong> (0.6417 across folds)</li>
            <li><strong>Wider prediction range</strong> (0.347-0.679, more confident)</li>
            <li><strong>Higher std dev</strong> (0.077, more discriminative)</li>
          </ul>

          <h4>üìÅ File to Submit:</h4>
          <div class="code-block">
            <pre>output_final/submission_v7_auto.csv
Size: 53.74 MB
Rows: 1,735,280 (with duplicates)
Unique IDs: 294,010</pre>
          </div>

          <div class="info-box">
            <strong>üîÑ Backup Plan</strong><br>
            If V7 Baseline doesn't perform well on hidden test, submit V7 Auto-tune next.
            Only 0.19% worse on validation, but more robust from CV.
          </div>
        </div>
      </div>

      <h3>üìä Performance Gap Analysis</h3>
      <table>
        <thead>
          <tr><th>Aspect</th><th>Gap</th><th>Significance</th><th>Interpretation</th></tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>AUC-PR Difference</strong></td>
            <td>+0.0012</td>
            <td><span class="badge badge-warning">Very Small (0.19%)</span></td>
            <td>Statistically negligible, could reverse on different test set</td>
          </tr>
          <tr>
            <td><strong>AUC-ROC Difference</strong></td>
            <td>+0.0016</td>
            <td><span class="badge badge-warning">Very Small (0.19%)</span></td>
            <td>Both models essentially equivalent in ranking ability</td>
          </tr>
          <tr>
            <td><strong>Prediction Style</strong></td>
            <td>Conservative vs Confident</td>
            <td><span class="badge badge-info">Different</span></td>
            <td>Baseline safer, Auto-tune more decisive</td>
          </tr>
        </tbody>
      </table>

      <div class="warning-box">
        <strong>‚ö†Ô∏è Important Consideration:</strong><br><br>
        
        The 0.19% difference between models is <strong>very small</strong> and within noise margin. 
        Performance on hidden test could go either way. Strategy:<br><br>
        
        1. <strong>Submit V7 Baseline FIRST</strong> (higher validation score)<br>
        2. <strong>Monitor leaderboard score</strong><br>
        3. <strong>If not satisfied ‚Üí submit V7 Auto-tune</strong> (more robust from CV)<br>
        4. <strong>Compare results</strong> to determine which works better on hidden test distribution
      </div>
    </div>

    <!-- Action Plan -->
    <div class="card card-full">
      <h2>üöÄ Action Plan ‚Äî Next Steps</h2>
      
      <h3>Step 1: Check Competition Rules ‚ö†Ô∏è</h3>
      <div class="danger-box">
        <strong>CRITICAL: Must check before submission!</strong><br><br>
        
        <strong>Question:</strong> Does competition require 1 prediction per review_id?<br><br>
        
        <strong>Scenario A ‚Äî Unique IDs Required:</strong><br>
        <div class="code-block">
          <pre># Clean duplicates (keep first occurrence)
import pandas as pd
df = pd.read_csv('output_final/submission_v7.csv')
df_clean = df.drop_duplicates(subset='review_id', keep='first')
df_clean.to_csv('output_final/submission_v7_clean.csv', index=False)
print(f"Reduced from {len(df):,} to {len(df_clean):,} rows")</pre>
        </div>
        Expected result: 1,735,280 ‚Üí 294,010 rows (~9.2 MB file)<br><br>
        
        <strong>Scenario B ‚Äî Duplicates Allowed:</strong><br>
        Submit as-is (1.73M rows, 53.77 MB file)
      </div>

      <h3>Step 2: Submit V7 Baseline</h3>
      <div class="success-box">
        <strong>‚úÖ Primary Submission</strong><br>
        File: <code>output_final/submission_v7.csv</code> (or <code>submission_v7_clean.csv</code> if cleaned)<br>
        Expected AUC-PR: ~0.63 (based on validation)<br>
        Reasoning: Highest validation score, most stable predictions
      </div>

      <h3>Step 3: Monitor Results</h3>
      <div class="info-box">
        <ul>
          <li>Check leaderboard score after submission</li>
          <li>Compare with validation AUC-PR (0.6327)</li>
          <li>If score drops significantly ‚Üí distribution shift between validation and test</li>
          <li>If score is acceptable ‚Üí done! üéâ</li>
        </ul>
      </div>

      <h3>Step 4: Backup Submission (If Needed)</h3>
      <div class="warning-box">
        <strong>If V7 Baseline underperforms:</strong><br>
        File: <code>output_final/submission_v7_auto.csv</code> (or cleaned version)<br>
        Expected AUC-PR: ~0.63 (based on validation)<br>
        Reasoning: More robust from CV, wider prediction range, only 0.19% worse on validation
      </div>

      <h3>üìã Submission Checklist</h3>
      <table>
        <thead>
          <tr><th>Task</th><th>Status</th><th>Notes</th></tr>
        </thead>
        <tbody>
          <tr>
            <td>‚úÖ Training Complete</td>
            <td><span class="badge badge-success">Done</span></td>
            <td>V7 Baseline & V7 Auto-tune both trained</td>
          </tr>
          <tr>
            <td>‚úÖ Predictions Generated</td>
            <td><span class="badge badge-success">Done</span></td>
            <td>Both models predicted on 1.73M test samples</td>
          </tr>
          <tr>
            <td>‚úÖ Files Copied to Local</td>
            <td><span class="badge badge-success">Done</span></td>
            <td>Files in output_final/ directory</td>
          </tr>
          <tr>
            <td>‚úÖ Statistical Analysis</td>
            <td><span class="badge badge-success">Done</span></td>
            <td>Stats, charts, and reports generated</td>
          </tr>
          <tr>
            <td>‚ö†Ô∏è Check Competition Rules</td>
            <td><span class="badge badge-warning">TODO</span></td>
            <td>Unique IDs required or duplicates OK?</td>
          </tr>
          <tr>
            <td>‚ö†Ô∏è Clean Duplicates (If Required)</td>
            <td><span class="badge badge-warning">TODO</span></td>
            <td>Run drop_duplicates if needed</td>
          </tr>
          <tr>
            <td>‚ö†Ô∏è Submit to Competition</td>
            <td><span class="badge badge-warning">TODO</span></td>
            <td>Upload submission_v7.csv (or cleaned)</td>
          </tr>
        </tbody>
      </table>
    </div>

    <!-- Technical Summary -->
    <div class="card card-full">
      <h2>üî¨ Technical Summary</h2>
      
      <h3>Training Configuration</h3>
      <div class="compare-grid">
        <div>
          <h4>V7 Baseline (Manual Tuning)</h4>
          <div class="code-block">
            <pre>numLeaves: 120
learningRate: 0.03
minDataInLeaf: 50
numIterations: 1500
earlyStoppingRound: 200
featureFraction: 0.75
baggingFraction: 0.75
lambdaL1: 0.1
lambdaL2: 0.1

Training samples: 5,000,000
Training time: 2.5 hours
Validation AUC-PR: 0.6327</pre>
          </div>
        </div>
        <div>
          <h4>V7 Auto-tune (Grid Search)</h4>
          <div class="code-block">
            <pre>Best params (from 27 CV runs):
numLeaves: 100
learningRate: 0.15

Fixed params:
minDataInLeaf: 50
numIterations: 1500
earlyStoppingRound: 200
featureFraction: 0.75
baggingFraction: 0.75
lambdaL1: 0.1
lambdaL2: 0.1

Training samples: 5,000,000
Training time: 2.7 hours
CV mean AUC-PR: 0.6417
Validation AUC-PR: 0.6315</pre>
          </div>
        </div>
      </div>

      <h3>Feature Engineering</h3>
      <ul>
        <li><strong>Text Features:</strong> TF-IDF with 10,000 vocabulary (review_body + review_headline)</li>
        <li><strong>Numeric Features:</strong> 17 engineered features (vine, verified_purchase, vote_ratio, etc.)</li>
        <li><strong>Total Dimension:</strong> 10,017 features</li>
        <li><strong>NULL Handling:</strong> Imputation with 0 for TF-IDF, mean for numeric</li>
        <li><strong>Data Split:</strong> 5M train (32% of 15.6M), 1.73M test, 100% coverage</li>
      </ul>

      <h3>Model Architecture</h3>
      <ul>
        <li><strong>Algorithm:</strong> LightGBM (Gradient Boosting Decision Trees)</li>
        <li><strong>Library:</strong> SynapseML LightGBM (Spark-distributed)</li>
        <li><strong>Objective:</strong> Binary classification (is_helpful_vote)</li>
        <li><strong>Metric:</strong> AUC-PR (Area Under Precision-Recall Curve)</li>
        <li><strong>Hardware:</strong> Local Spark cluster (11GB driver + 11GB executor)</li>
      </ul>

      <h3>Data Quality Issues</h3>
      <div class="danger-box">
        <strong>üö® Duplicate Review IDs:</strong><br>
        - 83% of test data contains duplicate review_ids<br>
        - Root cause: Test data preprocessing created duplicates<br>
        - Impact: Both models predict on all rows ‚Üí duplicate predictions<br>
        - Solution: Must check competition rules and clean if required
      </div>
    </div>

    <!-- Conclusion -->
    <div class="card card-full">
      <h2>üéØ Conclusion & Recommendations</h2>
      
      <div class="success-box">
        <h3 style="margin-top:0">‚úÖ Primary Recommendation: Submit V7 Baseline</h3>
        
        <strong>Reasons:</strong>
        <ol>
          <li><strong>Highest validation AUC-PR:</strong> 0.6327 (vs 0.6315 for Auto-tune)</li>
          <li><strong>Most stable predictions:</strong> Lower std dev (0.067 vs 0.077)</li>
          <li><strong>Conservative approach:</strong> Narrower range reduces risk of extreme errors</li>
          <li><strong>Faster training:</strong> 2.5 hours (vs 2.7 hours for Auto-tune)</li>
          <li><strong>Proven configuration:</strong> Manual tuning based on V4-V6 experience</li>
        </ol>

        <strong>File to submit:</strong><br>
        <code>output_final/submission_v7.csv</code> (or cleaned version if required)<br>
        Size: 53.77 MB (1.73M rows) or ~9.2 MB (294K rows after cleaning)
      </div>

      <div class="info-box">
        <h3 style="margin-top:0">üîÑ Backup Option: V7 Auto-tune</h3>
        
        If V7 Baseline doesn't perform well on hidden test:<br>
        - Submit <code>output_final/submission_v7_auto.csv</code> (or cleaned)<br>
        - More robust from 27 CV runs (3-fold √ó 9 configs)<br>
        - Wider prediction range (0.347-0.679) may capture more diversity<br>
        - Only 0.19% worse on validation ‚Üí could perform better on different distribution
      </div>

      <div class="warning-box">
        <h3 style="margin-top:0">‚ö†Ô∏è Critical Action Required</h3>
        
        <strong>Before submitting:</strong><br>
        1. <strong>Check competition submission format requirements</strong><br>
        2. <strong>If unique IDs required ‚Üí clean duplicates first</strong><br>
        3. <strong>Verify file format matches sample submission</strong><br>
        4. <strong>Test upload with small sample if possible</strong>
      </div>

      <h3>üéì Key Learnings</h3>
      <ul>
        <li><strong>More data ‚â† always better:</strong> V7 (5M samples, AUC-PR 0.63) didn't beat V6 (1M samples, AUC-PR 0.64)</li>
        <li><strong>Manual tuning competitive:</strong> Experience-based params matched grid search results</li>
        <li><strong>Data quality matters:</strong> 83% duplicates is a critical issue that needs attention</li>
        <li><strong>Validation may not generalize:</strong> 0.19% gap could reverse on hidden test</li>
        <li><strong>Always check submission rules:</strong> Format requirements can invalidate submissions</li>
      </ul>

      <div style="text-align:center;margin:32px 0">
        <span class="badge badge-success" style="font-size:1.2em;padding:12px 24px">
          üéØ Ready to Submit! Good Luck! üöÄ
        </span>
      </div>
    </div>

    <!-- Footer -->
    <div style="text-align:center;color:#fff;margin-top:32px;padding:20px;background:rgba(0,0,0,0.2);border-radius:8px">
      <p><strong>Amazon Review Helpfulness Prediction ‚Äî HK7 Project</strong></p>
      <p>Authors: V√µ Th·ªã Di·ªÖm Thanh & L√™ ƒêƒÉng Ho√†ng Tu·∫•n</p>
      <p>Date: November 1, 2025 @ 20:00</p>
    </div>
  </div>
      </div>
      <a class="backtop" href="#top">‚Üë Back to top</a>
    </section>
    
    <div class="footer">Generated automatically ‚Ä¢ 4 sections</div>
  </div>
</body>
</html>
