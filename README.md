# üß† D·ª∞ √ÅN BIG DATA ‚Äì D·ª∞ ƒêO√ÅN M·ª®C ƒê·ªò H·ªÆU √çCH C·ª¶A ƒê√ÅNH GI√Å TR√äN AMAZON

## üéØ Gi·ªõi thi·ªáu

ƒê·ªì √°n n√†y thu·ªôc m√¥n **Ph√¢n t√≠ch D·ªØ li·ªáu l·ªõn**, y√™u c·∫ßu thi·∫øt k·∫ø v√† tri·ªÉn khai m·ªôt **pipeline ph√¢n t√≠ch d·ªØ li·ªáu ho√†n ch·ªânh b·∫±ng Apache Spark ch·∫°y tr√™n HDFS**, x·ª≠ l√Ω d·ªØ li·ªáu quy m√¥ l·ªõn (ETL), tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng ph·ª©c t·∫°p v√† √°p d·ª•ng m√¥ h√¨nh h·ªçc m√°y (Machine Learning) ƒë·ªÉ gi·∫£i quy·∫øt b√†i to√°n c·ª• th·ªÉ.

Nh√≥m l·ª±a ch·ªçn **ƒê·ªÅ t√†i 3: D·ª± ƒëo√°n M·ª©c ƒë·ªô H·ªØu √≠ch c·ªßa ƒê√°nh gi√° tr√™n Amazon**, t·∫≠p trung v√†o vi·ªác **ph√¢n lo·∫°i b√†i ƒë√°nh gi√° l√† ‚Äúh·ªØu √≠ch‚Äù hay ‚Äúkh√¥ng h·ªØu √≠ch‚Äù** d·ª±a tr√™n vƒÉn b·∫£n (text) v√† metadata.

---

## üßæ Y√™u c·∫ßu chung

- **C√¥ng ngh·ªá s·ª≠ d·ª•ng:** Apache Spark (PySpark) ‚Äì x·ª≠ l√Ω d·ªØ li·ªáu tr√™n HDFS.  
- **Ph·∫°m vi:** L√†m vi·ªác theo nh√≥m, tri·ªÉn khai pipeline ƒë·∫ßy ƒë·ªß: ETL ‚Üí Feature Engineering ‚Üí Model Training ‚Üí Evaluation ‚Üí Submission.  
- **ƒê√°nh gi√°:** D·ª±a tr√™n b·ªô test ·∫©n (hidden test set).  
- **S·∫£n ph·∫©m n·ªôp:**  
  - File d·ª± ƒëo√°n `submission.csv`  
  - M√£ ngu·ªìn  
  - B√°o c√°o m√¥ t·∫£ ph∆∞∆°ng ph√°p, ƒë·∫∑c tr∆∞ng v√† ki·∫øn tr√∫c m√¥ h√¨nh  

---

## üì¶ B·ªô d·ªØ li·ªáu

- **T√™n:** Amazon Reviews 2023 ‚Äì *Movies_and_TV*  
- **Ngu·ªìn:** [Hugging Face Dataset](https://huggingface.co/datasets/McAuley-Lab/Amazon-Reviews-2023/tree/main/raw)  
- **Quy m√¥:** ~17.3 tri·ªáu ƒë√°nh gi√° (JSONL format)  
- **C√°c tr∆∞·ªùng d·ªØ li·ªáu:**  
  - `review_id`  
  - `review_text`  
  - `star_rating`  
  - `helpful_votes`  
  - `user_id`, `product_id`, `timestamp`

---

## üß© B√†i to√°n Machine Learning

- **D·∫°ng b√†i to√°n:** Ph√¢n lo·∫°i nh·ªã ph√¢n (binary classification).  
- **M·ª•c ti√™u:** D·ª± ƒëo√°n x√°c su·∫•t b√†i ƒë√°nh gi√° ƒë∆∞·ª£c xem l√† ‚Äúh·ªØu √≠ch‚Äù.  
- **Nh√£n m·ª•c ti√™u (target):**  
  - `is_helpful = 1` n·∫øu `helpful_votes > X` (v√≠ d·ª• X = 2)  
  - `is_helpful = 0` ng∆∞·ª£c l·∫°i  
- **Kh√≥ khƒÉn ch√≠nh:**  
  - M·∫•t c√¢n b·∫±ng d·ªØ li·ªáu nghi√™m tr·ªçng  
  - D·ªØ li·ªáu vƒÉn b·∫£n (text) l·ªõn v√† phi c·∫•u tr√∫c  
- **Ti√™u ch√≠ ƒë√°nh gi√°:**  
  - **AUC-PR (Area Under Precision-Recall Curve)**  
- **ƒê·∫ßu ra y√™u c·∫ßu:**  
  - File `submission.csv` g·ªìm hai c·ªôt:  
    - `review_id`  
    - `probability_helpful`

---

## üöÄ K·∫ø ho·∫°ch Th·ª±c hi·ªán (7 Ng√†y)

### üóìÔ∏è **Ng√†y 1 ‚Äì Kh√°m ph√° d·ªØ li·ªáu (EDA)**
- Thi·∫øt l·∫≠p m√¥i tr∆∞·ªùng Spark/Python v√† th∆∞ vi·ªán c·∫ßn thi·∫øt (`pandas`, `nltk`, `scikit-learn`, `lightgbm`).
- ƒê·ªçc d·ªØ li·ªáu theo `chunksize` (100k d√≤ng/l·∫ßn) do k√≠ch th∆∞·ªõc l·ªõn.
- Ph√¢n t√≠ch ph√¢n ph·ªëi `helpful_votes`, x√°c ƒë·ªãnh t·ª∑ l·ªá m·∫•t c√¢n b·∫±ng.
- ƒê·ªãnh nghƒ©a `is_helpful` v√† t·∫°o t·∫≠p m·∫´u (sample 1‚Äì2 tri·ªáu d√≤ng).

---

### üóìÔ∏è **Ng√†y 2 ‚Äì Ti·ªÅn x·ª≠ l√Ω & Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng (Baseline)**
- X·ª≠ l√Ω vƒÉn b·∫£n: lowercase, x√≥a d·∫•u c√¢u, stopwords.
- Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng:
  - **Metadata:** `rating`, `review_length`, `timestamp`.
  - **Text:** TF-IDF (`max_features=5000`).
- X√¢y d·ª±ng `Pipeline` (`ColumnTransformer`) k·∫øt h·ª£p text v√† metadata.

---

### üóìÔ∏è **Ng√†y 3 ‚Äì M√¥ h√¨nh c∆° s·ªü & X·ª≠ l√Ω m·∫•t c√¢n b·∫±ng**
- Chia d·ªØ li·ªáu: 80% train / 20% validation.
- Hu·∫•n luy·ªán baseline:
  - `LogisticRegression(solver='liblinear')`
- ƒê√°nh gi√° b·∫±ng **AUC-PR**.
- Th·ª≠ x·ª≠ l√Ω m·∫•t c√¢n b·∫±ng:
  - `class_weight='balanced'`  
  - Ho·∫∑c m√¥ h√¨nh **LightGBM** v·ªõi `is_unbalance=True`.

---

### üóìÔ∏è **Ng√†y 4 ‚Äì C·∫£i ti·∫øn ƒë·∫∑c tr∆∞ng & th·ª≠ nghi·ªám**
- TF-IDF m·ªü r·ªông `ngram_range=(1,2)`.
- Th√™m ƒë·∫∑c tr∆∞ng c·∫£m x√∫c (Sentiment) b·∫±ng `VADER`.
- Metadata n√¢ng cao: `user_review_count`, `product_avg_rating`.
- Hu·∫•n luy·ªán LightGBM, so s√°nh k·∫øt qu·∫£ v·ªõi baseline.

---

### üóìÔ∏è **Ng√†y 5 ‚Äì Tinh ch·ªânh m√¥ h√¨nh (Tuning)**
- D√πng `RandomizedSearchCV` ho·∫∑c `Optuna` ƒë·ªÉ t·ªëi ∆∞u si√™u tham s·ªë (`num_leaves`, `learning_rate`, `scale_pos_weight`, ‚Ä¶).
- L∆∞u m√¥ h√¨nh, vectorizer, scaler ƒë√£ fit.

---

### üóìÔ∏è **Ng√†y 6 ‚Äì Hu·∫•n luy·ªán cu·ªëi c√πng (Full Training)**
- Hu·∫•n luy·ªán m√¥ h√¨nh cu·ªëi c√πng tr√™n t·∫≠p l·ªõn h∆°n (10‚Äì17 tri·ªáu d√≤ng).
- N·∫øu kh√¥ng ƒë·ªß RAM: fit TF-IDF tr√™n 5 tri·ªáu d√≤ng r·ªìi transform ph·∫ßn c√≤n l·∫°i.
- Ho·∫∑c d√πng `pyspark.ml` (`HashingTF`, `IDF`, `LogisticRegression`, `GBTClassifier`) ƒë·ªÉ ch·∫°y tr·ª±c ti·∫øp tr√™n Spark.

---

### üóìÔ∏è **Ng√†y 7 ‚Äì D·ª± ƒëo√°n & T·∫°o Submission**
- ƒê·ªçc d·ªØ li·ªáu test theo kh·ªëi (chunk ho·∫∑c Spark DataFrame).
- √Åp d·ª•ng pipeline x·ª≠ l√Ω + m√¥ h√¨nh ƒë√£ l∆∞u.
- Ghi k·∫øt qu·∫£ ra `submission.csv` v·ªõi c·ªôt:  
  - `review_id`, `probability_helpful`.

---

## üß† Ki·∫øn tr√∫c D·ª± √°n

```
project/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/ (JSONL g·ªëc)
‚îÇ   ‚îú‚îÄ‚îÄ processed/
‚îú‚îÄ‚îÄ code/
‚îÇ   ‚îú‚îÄ‚îÄ etl/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ preprocess_spark.py
‚îÇ   ‚îú‚îÄ‚îÄ features/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ text_features.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metadata_features.py
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_lightgbm.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ predict_pipeline.py
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ evaluation.py
‚îú‚îÄ‚îÄ output/
‚îÇ   ‚îú‚îÄ‚îÄ submission.csv
‚îÇ   ‚îú‚îÄ‚îÄ metrics.json
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ requirements.txt
```

---

## ‚öôÔ∏è C√¥ng c·ª• & Th∆∞ vi·ªán ch√≠nh

- **Apache Spark / PySpark** (ETL & TF-IDF ph√¢n t√°n)  
- **scikit-learn**, **LightGBM**, **Optuna**  
- **NLTK**, **VADER Sentiment**  
- **HDFS**, **Docker**, **Jupyter Notebook**

---

## üìä ƒê√°nh gi√° & N·ªôp b√†i

- **Metric:** AUC-PR  
- **File n·ªôp:** `submission.csv`  
- **H·∫°n ch·∫ø:** B·ªô test ·∫©n ƒë∆∞·ª£c c√¥ng b·ªë 12 gi·ªù tr∆∞·ªõc deadline  
- **B√°o c√°o:** m√¥ t·∫£ pipeline, ƒë·∫∑c tr∆∞ng, m√¥ h√¨nh, v√† quy tr√¨nh hu·∫•n luy·ªán.

---


## üë• Ph√¢n c√¥ng c√¥ng vi·ªác chi ti·∫øt (2 th√†nh vi√™n)

> Nh√≥m g·ªìm **L√™ ƒêƒÉng Ho√†ng Tu·∫•n** v√† **V√µ Th·ªã Di·ªÖm Thanh**. B·∫£ng d∆∞·ªõi ƒë√¢y m√¥ t·∫£ ph·∫°m vi, ƒë·∫ßu ra (deliverables) v√† ti√™u ch√≠ ho√†n th√†nh cho t·ª´ng th√†nh vi√™n ‚Äî b√°m s√°t pipeline ETL ‚Üí Features ‚Üí Modeling ‚Üí Evaluation ‚Üí Submission.

### 1) L√™ ƒêƒÉng Ho√†ng Tu·∫•n ‚Äî H·∫° t·∫ßng d·ªØ li·ªáu & Pipeline suy lu·∫≠n
- **H·∫° t·∫ßng & ETL (Spark + HDFS)**
  - D·ª±ng c·ª•m HDFS/Spark (Docker Compose), c·∫•u h√¨nh IO, ph√¢n quy·ªÅn HDFS.
  - Vi·∫øt **ETL Spark** ƒë·ªçc JSONL th√¥ ‚Üí chu·∫©n ho√° schema ‚Üí ghi **Parquet** ph√¢n v√πng theo th·ªùi gian (n·∫øu c√≥ `timestamp`).
  - Ki·ªÉm tra ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu (null %, ki·ªÉu d·ªØ li·ªáu, gi√° tr·ªã ngo·∫°i lai) v√† log k·∫øt qu·∫£.
- **Feature Store & Reproducibility**
  - Thi·∫øt l·∫≠p **feature store** c∆° b·∫£n cho metadata (`star_rating`, `review_length`, time-features).
  - ƒê·∫£m b·∫£o **kh√¥ng r√≤ r·ªâ d·ªØ li·ªáu**: fit transformer tr√™n **train-only**, persist artifact.
- **Baseline & Inference Pipeline**
  - Hu·∫•n luy·ªán **Logistic Regression** (TF‚ÄëIDF + metadata) v·ªõi `class_weight='balanced'`, b√°o c√°o **AUC‚ÄëPR** baseline.
  - X√¢y d·ª±ng **pipeline d·ª± ƒëo√°n theo chunk** tr√™n HDFS (ƒë·ªçc ‚Üí transform ‚Üí predict ‚Üí ghi `submission.csv`), ki·ªÉm so√°t b·ªô nh·ªõ.
- **T·ªëi ∆∞u t√†i nguy√™n**
  - ƒêi·ªÅu ch·ªânh `max_features`, `min_df`, batch-size transform ƒë·ªÉ t·ªëi ∆∞u RAM/ƒë·ªô tr·ªÖ.
- **Deliverables**
  - M√£: `code/etl/preprocess_spark.py`, `code/models/train_logreg.py`, `code/models/predict_pipeline.py`
  - Artifact: `vectorizer.joblib`, `scaler.joblib`, `model_logreg.joblib`, `meta.json`
  - **Output:** `output/submission.csv`
  - **B√°o c√°o:** Ki·∫øn tr√∫c HDFS/Spark, ETL, t·ªëi ∆∞u I/O, baseline.
- **Ti√™u ch√≠ ho√†n th√†nh (DoD)**
  - ETL ch·∫°y end‚Äëto‚Äëend tr√™n HDFS, file Parquet ph√¢n v√πng.
  - Pipeline suy lu·∫≠n sinh `submission.csv` ƒë√∫ng ƒë·ªãnh d·∫°ng (ƒë·ªß s·ªë d√≤ng, kh√¥ng NaN).
  - Baseline **AUC‚ÄëPR > 0** v√† log ƒë·∫ßy ƒë·ªß: pos/neg ratio, th·ªùi gian ch·∫°y, t√†i nguy√™n.

### 2) V√µ Th·ªã Di·ªÖm Thanh ‚Äî ƒê·∫∑c tr∆∞ng NLP & M√¥ h√¨nh n√¢ng cao
- **Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n**
  - Chu·∫©n ho√°: lowercase, b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát, stopwords, t√≠nh `review_length`.
  - (Tu·ª≥ ch·ªçn) Lemmatization/Stemming n·∫øu chi ph√≠ ch·∫•p nh·∫≠n ƒë∆∞·ª£c.
- **Feature Engineering**
  - **TF‚ÄëIDF** `ngram_range=(1,2)`, `min_df>=5`, ki·ªÉm so√°t `max_features` (10k‚Äì50k).
  - **Sentiment (VADER)**: th√™m `sentiment_compound` v√†o metadata.
  - (N√¢ng cao) Aggregates: `user_review_count`, `product_avg_rating` (t√≠nh tr√™n **train**, map sang val/test).
- **Modeling & Imbalance Handling**
  - **LightGBM** v·ªõi `is_unbalance=True` **ho·∫∑c** `scale_pos_weight = n_neg/n_pos`.
  - So s√°nh v·ªõi baseline b·∫±ng **AUC‚ÄëPR**, th√™m PR curve & ƒëi·ªÉm F1 t·ªëi ∆∞u.
  - Tuning nh·∫π (RandomizedSearch/Optuna) cho `num_leaves`, `n_estimators`, `learning_rate`.
- **Deliverables**
  - M√£: `code/features/text_features.py`, `code/features/metadata_features.py`, `code/models/train_lightgbm.py`
  - Artifact: `model_lgbm.joblib`, tham s·ªë/b√°o c√°o tuning.
  - **B√°o c√°o:** M√¥ t·∫£ ƒë·∫∑c tr∆∞ng, m√¥ h√¨nh, ph√¢n t√≠ch m·∫•t c√¢n b·∫±ng & k·∫øt qu·∫£.
- **Ti√™u ch√≠ ho√†n th√†nh (DoD)**
  - **AUC‚ÄëPR ‚â• baseline** v√† c·∫£i thi·ªán √Ω nghƒ©a (k√®m PR curve).
  - Artifacts t√°i s·ª≠ d·ª•ng ƒë∆∞·ª£c (fit tr√™n train, transform tr√™n val/test).
  - B·∫£ng so s√°nh: baseline vs LGBM (AP, th·ªùi gian train/infer, k√≠ch th∆∞·ªõc model).

### B·∫£ng t√≥m t·∫Øt (SV ‚Äì C√¥ng vi·ªác ‚Äì T·ª∑ l·ªá ho√†n th√†nh)
| SV | C√¥ng vi·ªác ƒë∆∞·ª£c giao | T·ª∑ l·ªá ho√†n th√†nh |
|---|---|---|
| **L√™ ƒêƒÉng Ho√†ng Tu·∫•n** | ETL Spark/HDFS; Feature store metadata; Baseline LogReg (balanced); Pipeline d·ª± ƒëo√°n theo chunk & `submission.csv`; B√°o c√°o ph·∫ßn h·ªá th·ªëng | ‚Ä¶% |
| **V√µ Th·ªã Di·ªÖm Thanh** | Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n; TF‚ÄëIDF + Sentiment + metadata n√¢ng cao; LightGBM + handling imbalance + tuning; B√°o c√°o ph·∫ßn NLP/M√¥ h√¨nh | ‚Ä¶% |

> **Chung:** m·ªçi th√≠ nghi·ªám c·∫ßn log **AUC‚ÄëPR**, pos/neg ratio, `scale_pos_weight`, `max_features`, th·ªùi gian ch·∫°y; l∆∞u `output/metrics.json`. Slide 8‚Äì12 trang (b√†i to√°n ‚Üí data ‚Üí pipeline ‚Üí k·∫øt qu·∫£ ‚Üí demo).


## üß© K·∫øt lu·∫≠n

D·ª± √°n n√†y minh h·ªça quy tr√¨nh ƒë·∫ßy ƒë·ªß c·ªßa m·ªôt h·ªá th·ªëng **ph√¢n t√≠ch d·ªØ li·ªáu l·ªõn k·∫øt h·ª£p NLP + ML** tr√™n t·∫≠p d·ªØ li·ªáu Amazon Reviews quy m√¥ h√†ng ch·ª•c tri·ªáu b·∫£n ghi, gi√∫p m√¥ ph·ªèng quy tr√¨nh tri·ªÉn khai th·ª±c t·∫ø c·ªßa c√°c h·ªá th·ªëng ƒë√°nh gi√° s·∫£n ph·∫©m trong th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠.

---

## üìÅ File n·ªôp cu·ªëi c√πng

```
submission.csv
‚îú‚îÄ‚îÄ review_id
‚îî‚îÄ‚îÄ probability_helpful
```
---

> **Gi·∫£ng vi√™n h∆∞·ªõng d·∫´n:**  
> M√¥n Ph√¢n t√≠ch D·ªØ li·ªáu L·ªõn ‚Äì HUIT  
> ƒê·ªÅ t√†i s·ªë 3 ‚Äì D·ª± ƒëo√°n M·ª©c ƒë·ªô H·ªØu √≠ch c·ªßa ƒê√°nh gi√° tr√™n Amazon  
> C√¥ng c·ª• ch√≠nh: Apache Spark + PySpark + LightGBM
